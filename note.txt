책 제목 후보
----------------------
```
안녕, ~
한국어 NLP
```


환경
----------------------
- CUDA 버전: 10.1
```
# requirements.txt
torch==1.7.0+cu101
transformers==4.1.0.dev0
```


목차
----------------------
1. 우리가 만나는 NLP
	1.1. 일상 속의 NLP 프로젝트
        1.1.1. 문장 분류
		1.1.2. 개체명 인식
		1.1.3. 기계 독해
		1.1.4. 질의 응답
        1.1.5. 문장 요약
        1.1.6. 자동 완성
		1.1.7. 이미지 캡셔닝

	1.2. 나도 한번 만들어보기
        1.2.1. 문장 분류
		1.2.2. 개체명 인식
		1.2.3. 기계 독해
		1.2.4. 질의 응답
        1.2.5. 문장 요약
        1.2.6. 자동 완성
		1.2.7. 이미지 캡셔닝
		
	1.3. 회고해보기
		1.3.1. 성능 비교
		1.3.1. 어려웠던 점들
		
	1.4. 해결해보기
		1.4.1. ULM-FiT
	
2. 언어 모델 들어가기
	2.1. 언어 모델이란 무엇인가?
	2.2. N-Gram 언어 모델
		2.2.1. N-Gram 언어 모델 이해하기
		2.2.2. 만들어보기
		2.2.3. 활용하기
		2.2.3. Interpolation과 Backoff
		
	2.3. 확률 기반의 언어 모델
		2.3.1. 확률 기반의 언어 모델 이해하기
		2.3.2. 만들어보기
		2.3.3. 활용하기

	2.4. Word2Vec
		2.4.1. Word2Vec 언어 모델 이해하기
		2.4.2. 만들어보기
		2.4.3. 활용하기

	2.5. 언어 모델 비교하기
		2.5.1. GLUE 데이터셋
		2.5.2. SQuAD 데이터셋
		2.5.3. 성능 비교
		2.5.4. 어려웠던 점들

3. 언어 모델 발전시키기
	3.1. LSTM 기반의 언어 모델
		3.1.1. Why LSTM?
		3.1.2. LSTM 언어 모델 이해하기
		3.1.3. 만들어보기
		3.1.4. 활용하기
		
	3.2. 언어 모델 비교하기
		3.2.1. 성능 비교
		3.2.2. 어려웠던 점들
		
4. 트랜스포머 탄생
	4.1. Attention의 출현
		4.1.1. Why Attention?
		4.1.2. Attention 이해하기
		4.1.3. 만들어보기
		4.1.4. 활용하기
		4.1.5. 파해치기
		
	4.2. Attention is all you need
		4.2.1. What's new in 트랜스포머
		4.2.2. 구조 알아보기
		4.2.3. 만들어보기
		4.2.4. 파해치기
	
	4.3. BERT
		4.3.1. 구조 알아보기
		4.3.2. 만들어보기
		4.3.3. 파해치기

5. BERT 이후의 언어 모델
	5.1. RoBERTa
	5.2. GPT2
	5.3. ALBERT
	5.4. T5
	5.5. ELECTRA
	5.6. 

6. 다시 만들어보는 일상속의 NLP
	6.1. 다시 만들어보기
        6.1.1. 문장 분류
		6.1.2. 개체명 인식
		6.1.3. 기계 독해
		6.1.4. 질의 응답
        6.1.5. 문장 요약
        6.1.6. 자동 완성
		6.1.7. 이미지 캡셔닝
	
	6.2. 성능 비교하기
		6.2.1. 이미지 캡셔닝
		6.2.2. 번역 시스템
		...
		6.2.N.

# Reference
- http://intelligence.korea.ac.kr/members/wschoi//nlp/deeplearning/paperreview/Paper-Review-A-Neural-Probabilistic-Language-Model/: 확률 기반의 언어모델에 대한 리뷰
- https://wikidocs.net/46496: 딥 러닝을 이용한 자연어 처리 입문
- https://towardsdatascience.com/evolution-of-language-models-n-grams-word-embeddings-attention-transformers-a688151825d2: LM history
- https://huffon.github.io/2019/11/16/glue/: glue dataset 설명
- https://www.edwith.org/deepnlp: 조경현 강의
- https://arxiv.org/pdf/1409.0473.pdf: Attention 논문



# Note
- BPE 추가하기
- SQuAD 추가하기









