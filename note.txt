안녕, ~



1. 우리가 만나는 NLP
	1.1. 일상 속의 NLP 프로젝트
		1.1.1. 이미지 캡셔닝
		1.1.2. 번역 시스템
		...
		1.1.N.

	1.2. 나도 한번 만들어보기
		1.2.1. 이미지 캡셔닝
		1.2.2. 번역 시스템
		...
		1.2.N.
		
	1.3. 회고해보기
		1.3.1. 성능 비교
		1.3.1. 어려웠던 점들
		
	1.4. 해결해보기
		1.4.1. ULM-FiT
	
2. 언어 모델 들어가기
	2.1. 언어모델이란 무엇인가?
	2.2. N-Gram 언어모델
		2.2.1. N-Gram 언어모델 이해하기
		2.2.2. 만들어보기
		2.2.3. 활용하기
		2.2.3. Interpolation과 Backoff
		
	2.3. 확률 기반의 언어모델
		2.3.1. 확률 기반의 언어 모델 이해하기
		2.3.2. 만들어보기
		2.3.3. 활용하기

	2.4. Word2Vec
		2.4.1. Word2Vec 언어 모델 이해하기
		2.4.2. 만들어보기
		2.4.3. 활용하기

3. 언어 모델 발전시키기
	3.1. LSTM 기반의 언어모델
		3.1.1. Why LSTM?
		3.1.2. LSTM 언어 모델 이해하기
		3.1.3. 만들어보기
		3.1.4. 활용하기
		
	3.2. 언어 모델 비교하기
		3.2.1. 성능 비교
		3.2.2. 어려웠던 점들
		
4. Transformer 탄생
	4.1. Attention의 출현
		4.1.1. Why Attention?
		4.1.2. Attention 이해하기
		4.1.3. 만들어보기
		4.1.4. 활용하기
		4.1.5. 파해치기
		
	4.2. Attention is all you need
		4.2.1. What's new in Transformer
		4.2.2. 구조 알아보기
		4.2.3. 만들어보기
		4.2.4. 파해치기
	
	4.3. BERT
		4.3.1. 구조 알아보기
		4.3.2. 만들어보기
		4.3.3. 파해치기

5. BERT 이후의 언어 모델
	5.1. RoBERTa
	5.2. GPT2
	5.3. ALBERT
	5.4. T5
	5.5. ELECTRA
	5.6. 

6. 다시 만들어보는 일상속의 NLP
	6.1. 다시 만들어보기
		6.1.1. 이미지 캡셔닝
		6.1.2. 번역 시스템
		...
		6.1.N.
	
	6.2. 성능 비교하기
		6.2.1. 이미지 캡셔닝
		6.2.2. 번역 시스템
		...
		6.2.N.

# Reference
- http://intelligence.korea.ac.kr/members/wschoi//nlp/deeplearning/paperreview/Paper-Review-A-Neural-Probabilistic-Language-Model/: 확률 기반의 언어모델에 대한 리뷰
- https://wikidocs.net/46496: 딥 러닝을 이용한 자연어 처리 입문
- https://towardsdatascience.com/evolution-of-language-models-n-grams-word-embeddings-attention-transformers-a688151825d2: LM history
- 













