# 부록
https://openreview.net/pdf?id=rJ4km2R5t7
Sparse Attention -> longformer, bigbird

Performer?

3. GLUE 데이터셋
NLP 모델을 평가하기 위한 데이터셋 중에서 가장 많이 사용되는 데이터셋으로 GLUE를 들수 있다. GLUE는 General Language Understanding Evaluation의 약자이다. 언어 능력을 평가할때 글의 핵심을 파악하는 능력, 문맥의 순서를 파악하는 능력 등등 여러가지 평가 기준이 있다. GLUE는 9개의 서브테스크로 언어 모델의 성능을 평가한다. 9개의 서브테스크는 CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI이며, 이 서브테스크들은 단일 문장 또는 문장 쌍들로 구성돼 있다. 이 장에서는 각각의 데이터셋이 어떤 문제들로 구성돼 있는지에 대해서 알아보자.

3.1. CoLA 데이터셋
Corpus of Linguistic Acceptability의 약자로, 영어 문장의 문법적인 수용성을 평가하는 데이터셋이다. 0은 문법적으로 수용되지 않는 문장을, 1은 문법적으로 수용되는 문장을 의미한다. CoLA 데이터셋의 레이블은 균형잡혀 있지 않는 unbalanced classification data이다. 평가할 때 사용되는 스코어는 Matthew correlation coefficient를 사용한다.

3.2. SST-2 데이터셋
Stanford Sentiment Treebank 데이터셋이다. 영화 리뷰에 대한 사람의 감정을 positive/negative로 나눈 데이터셋이다. SST-2는 accuracy를 통해 평가된다.

3.3. MRPC
MRPC(Microsoft Research Paraphrase Corpus)는 마이크로소프트에서 공개한 문장 쌍 데이터셋이고 온라인 뉴스로부터 자동으로 수집된 데이터셋이다. 이 데이터셋은 문장 쌍으로 되어 있고, 두 문장의 의미가 같은지 다른지를 0과 1로 판단하는 데이터셋이다. 이 데이터셋에 대한 평가는 accuracy와 F1 스코어로 측정한다.

3.4. QQP
QQP는 Quora로부터 확보한 질문을 쌍으로 연결해둔 데이터셋이다. Quora는 네이버 지식인과 비슷한 질의응답 웹사이트이다. Quora에는 중복된 질문이 많이 올라온다. 질문을 쌍으로 연결해서 중복된 질문인지 그렇지 않은지를 0과 1로 구분한 데이터셋이다. 이 데이터셋에 대한 평가는 accuracy와 F1 스코어로 측정한다.

3.5. STS-B
STS-B는 Semantic Textual Similarity Benchmark의 약자로 뉴스, 비디오, 이미지 캡션 등으로부터 추출한 문장 쌍 데이터셋이다. 각 데이터셋은 유사도 스코어를 1부터 5로 구분한 문장 쌍들로 구성돼 있다. 이 데이터셋에 대한 평가는 Pearson and Spearman correlation coefficient로 측정한다.

3.6. MNLI
MNLI 데이터셋은 Multi-Genre Natural Language inference의 약자이다. 이 데이터셋은 가설(hypothesis)이 전제(premise)를 수반(entailment)하는지, 모순(contradiction)하는지 또는 중립(neutral)적인지를 나타내는 데이터셋이다. 전제 데이터셋은 번역된 연설문이나 픽션 또는 정부의 리포트로부터 추출했다. 평가할 때는 accuracy로 측정한다. MNLI는 matched와 mismatched 데이터셋으로 나뉜다. matched 데이터셋은 in-domain 데이터셋으로 같은 도메인 상에서 생성된 가설/전제 데이터셋이고, mismatched 데이터셋은 cross-domain 데이터셋으로 서로 다른 도메인의 데이터로부터 가설/전제 데이터셋을 만든 것이다.


3.7. QNLI
QNLI는 스탠포드 질문/응답 데이터셋으로부터 생성한 데이터셋이다. 문장과 질문을 쌍으로 두고 질문에 대한 응답이 문장 내에 있는지 없는지를 판단하는 데이터셋이다. 이 데이터셋의 평가는 accuracy를 이용한다.


3.8. RTE
RTE는 Recognizaing Textual Entailment의 약자이다. 이 데이터셋은 RTE1, RTE2, RTE3, RTE5를 합친 데이터셋이고 뉴스와 위키피디아 텍스트로 이루어져 있다. 두 문장을 쌍으로 만들어서 그 두 문장이 서로 수반되는 문장인지 그렇지 않은지를 분류한 데이터셋이다. 레이블이 neutral이거나 또는 not_entailment일 경우에는 not_entailment로 통일했다. 데이터셋의 평가는 accuracy를 이용한다.

3.9. WNLI
Winograd Schema Challenge라는 독해능력 테스크로부터 생성한 데이터셋이다. Winograd Schema Challenge는 문장에 있는 특정 단어를 가르키는 레퍼런스를 선택지로부터 찾는 문제이다. 예를 들어서 [블록1]을 보자.

<블록 시작>
블록1: Winograd Schema Challenge 예제 데이터
문장: The city councilmen refused the demonstrators a permit because they [feared/advocated] violence.
단어: The city councilmen
선택지: [feared/advocated]
<블록 끝>

they가 the city councilmen을 가르키게 하려면 선택되야 하는 동사는 "feared"이다. WNLI 데이터셋은 선택지 중에서 feared가 선택된다면 entailment로 레이블링하고 advocated로 선택되면 not_entailment로 레이블링해서 만들어졌다. 이 데이터셋의 평가는 accuracy를 이용한다. 이 데이터셋은 레이블 상에 문제가 있어서 BERT에서는 평가시 제외했다.

1. Sparse Attention
이 책의 2장에서 어텐션에 대해서 자세하게 설명했다. 그리고 이 책의 3장에서 셀프 어텐션을 이용하는 트랜스포머의 핵심 구조를 이해해봤다. 그런데 셀프 어텐션에도 단점이 있다. 바로 연산량이다. 시간 복잡도를 따져보면 O(n^2)이 된다. 여기에서 n은 시퀀스의 길이인데 시퀀스의 길이가 512가 넘는 긴 문장의 경우 시간 복잡도가 기하급수적으로 늘어나서 시간과 공간 복잡도 등에 영향을 준다. BERT의 연산량에 큰 영향을 미치는 것이 어텐션이다. 따라서 이 어텐션의 연산량을 줄이기 위한 많은 연구가 있었다. 그 중에서 Sparse 어탠션에 대해서 이야기해보고 빅버드(Big Bird)라고 하는 긴 시퀀스를 위한 트랜스포머에 대해서도 알아보자.

1.1. 가까운 단어에만 집중하기, 로컬 어텐션
셀프 어텐션에서는 토큰 하나가 쿼리로 주어지면 그것에 대한 키가 나머지 다른 모든 토큰이 된다. 즉 하나의 토큰에 대해서 나머지 토큰과 모두 비교하는 연산을 거치게 된다. 그런데 언어에서 "가까움"은 무시할 수 없다. 가까이 있는 단어들에 더 많은 집중을 해야하는 것이 일반적이다. 그렇기 때문에 로컬 어텐션은 하나의 토큰에 대한 어텐션을 계산할 때 그 토큰과 가까이 있는 토큰들만 집중적으로 어탠션을 계산하는 것이다.

<블록 시작>
문장: She is going to run soon
토큰: She, is, going, to, run, soon
어텐션(쿼리 → 키):
- she → is
- is → She, is, going
- going → is, going, to
- to → going, to, run
- run → to, run, soon
- soon → run, soon
블록1: 로컬 어텐션 예시
<블록 끝>

[블록1]에서 사용한 문장 "She is going to run soon"에 대해서 로컬 어텐션을 적용할 경우, 토큰을 세 개씩 묶어서 그 세 개의 토큰에 대해서 어텐션을 계산한다. 물론 꼭 세 개가 아니여도 상관없다. 하나의 토큰에 대해서 앞 뒤로 몇 개의 토큰에 집중해서 어탠션을 계산하는 것이 로컬 어탠션이다. 토큰간의 어텐션을 계산하는 관계를 노드와 엣지의 그래프로 그리면 로컬 어텐션의 경우 [그림1]과 같이 그릴 수 있다.

<그림 시작>
그림1: 로컬 어텐션에 대한 그래프 예시
<그림 끝>

1.2. 가까운




