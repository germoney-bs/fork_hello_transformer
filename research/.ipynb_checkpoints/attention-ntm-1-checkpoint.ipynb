{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference: https://towardsdatascience.com/attention-seq2seq-with-pytorch-learning-to-invert-a-sequence-34faf4133e53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0+cu101'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_map = {\n",
    "    'a':'z',\n",
    "    'b':'y',\n",
    "    'c':'x',\n",
    "    'd':'w',\n",
    "    'e':'v',\n",
    "    'f':'u',\n",
    "    'g':'t',\n",
    "    'h':'s',\n",
    "    'i':'r',\n",
    "    'j':'q',\n",
    "    'k':'p',\n",
    "    'l':'o',\n",
    "    'm':'n',\n",
    "    'n':'m',\n",
    "    'o':'l',\n",
    "    'p':'k',\n",
    "    'q':'j',\n",
    "    'r':'i',\n",
    "    's':'h',\n",
    "    't':'g',\n",
    "    'u':'f',\n",
    "    'v':'e',\n",
    "    'w':'d',\n",
    "    'x':'c',\n",
    "    'y':'b',\n",
    "    'z':'a'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2i = {\n",
    "    '<s>':0,\n",
    "    '</s>':1,\n",
    "    '<pad>':2,\n",
    "    'a':3,\n",
    "    'b':4,\n",
    "    'c':5,\n",
    "    'd':6,\n",
    "    'e':7,\n",
    "    'f':8,\n",
    "    'g':9,\n",
    "    'h':10,\n",
    "    'i':11,\n",
    "    'j':12,\n",
    "    'k':13,\n",
    "    'l':14,\n",
    "    'm':15,\n",
    "    'n':16,\n",
    "    'o':17,\n",
    "    'p':18,\n",
    "    'q':19,\n",
    "    'r':20,\n",
    "    's':21,\n",
    "    't':22,\n",
    "    'u':23,\n",
    "    'v':24,\n",
    "    'w':25,\n",
    "    'x':26,\n",
    "    'y':27,\n",
    "    'z':28,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2a = {v:k for k, v in a2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<s>',\n",
       " 1: '</s>',\n",
       " 2: '<pad>',\n",
       " 3: 'a',\n",
       " 4: 'b',\n",
       " 5: 'c',\n",
       " 6: 'd',\n",
       " 7: 'e',\n",
       " 8: 'f',\n",
       " 9: 'g',\n",
       " 10: 'h',\n",
       " 11: 'i',\n",
       " 12: 'j',\n",
       " 13: 'k',\n",
       " 14: 'l',\n",
       " 15: 'm',\n",
       " 16: 'n',\n",
       " 17: 'o',\n",
       " 18: 'p',\n",
       " 19: 'q',\n",
       " 20: 'r',\n",
       " 21: 's',\n",
       " 22: 't',\n",
       " 23: 'u',\n",
       " 24: 'v',\n",
       " 25: 'w',\n",
       " 26: 'x',\n",
       " 27: 'y',\n",
       " 28: 'z'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_alphabet_index():\n",
    "    random_length = np.random.randint(5, MAX_LENGTH-2)    # -2 because of <s> and </s>\n",
    "    #random_length = 14\n",
    "    random_alphabet_index = np.random.randint(0, 26, random_length) + 3\n",
    "    return random_alphabet_index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetToyDataset(Dataset):\n",
    "    def __init__(self, n_dataset=1000):\n",
    "        bos = 0\n",
    "        eos = 1\n",
    "        pad = 2\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "        for _ in range(n_dataset):\n",
    "            # make input example\n",
    "            aindex = generate_random_alphabet_index()\n",
    "            \n",
    "            # index to alphabet\n",
    "            alphabet = list(map(lambda a: i2a[a], aindex))\n",
    "            \n",
    "            # inversing\n",
    "            inversed_alphabet = list(map(lambda a: inverse_map[a], alphabet))\n",
    "            \n",
    "            # alphabet to index\n",
    "            iindex = list(map(lambda ia: a2i[ia], inversed_alphabet))\n",
    "            \n",
    "            # add bos, eos and pad\n",
    "            n_pad = MAX_LENGTH - len(aindex) - 2\n",
    "            #aindex = [bos] + aindex + [eos] + [pad]*n_pad\n",
    "            aindex = aindex + [eos] + [pad]*n_pad\n",
    "            #iindex = [bos] + iindex + [eos] + [pad]*n_pad\n",
    "            iindex = iindex + [eos] + [pad]*n_pad\n",
    "            \n",
    "            # add to examples\n",
    "            self.inputs.append(aindex)\n",
    "            self.labels.append(iindex)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return [\n",
    "            torch.tensor(self.inputs[index], dtype=torch.long),\n",
    "            torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AlphabetToyDataset()\n",
    "valid_dataset = AlphabetToyDataset(n_dataset=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_index_to_alphabet(index):\n",
    "    alphabet = list(map(lambda i: i2a[i], index))\n",
    "    return ' '.join(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aindex_14: f h c l x x </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "iindex_14: u s x o c c </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "** aindex_14: tensor([ 8, 10,  5, 14, 26, 26,  1,  2,  2,  2,  2,  2,  2,  2])\n",
      "** iindex_14: tensor([23, 21, 26, 17,  5,  5,  1,  2,  2,  2,  2,  2,  2,  2])\n",
      "------------\n",
      "aindex_14: x e w a g p m </s> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "iindex_14: c v d z t k n </s> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "** aindex_14: tensor([26,  7, 25,  3,  9, 18, 15,  1,  2,  2,  2,  2,  2,  2])\n",
      "** iindex_14: tensor([ 5, 24,  6, 28, 22, 13, 16,  1,  2,  2,  2,  2,  2,  2])\n",
      "------------\n",
      "aindex_14: f w y s r </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "iindex_14: u d b h i </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "** aindex_14: tensor([ 8, 25, 27, 21, 20,  1,  2,  2,  2,  2,  2,  2,  2,  2])\n",
      "** iindex_14: tensor([23,  6,  4, 10, 11,  1,  2,  2,  2,  2,  2,  2,  2,  2])\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    ex = train_dataset[i]\n",
    "    aindex, iindex = ex\n",
    "    \n",
    "    print('aindex_{}: {}'.format(len(aindex), convert_index_to_alphabet(aindex.numpy())))\n",
    "    print('iindex_{}: {}'.format(len(iindex), convert_index_to_alphabet(iindex.numpy())))\n",
    "    print('** aindex_{}: {}'.format(len(aindex), aindex))\n",
    "    print('** iindex_{}: {}'.format(len(iindex), iindex))\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AlphabetEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.embedding(inputs)\n",
    "        #print('** embedding: {}'.format(self.embedding(input).shape))\n",
    "        #print('** embedded: {}'.format(embedded.shape))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(AlphabetDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = F.relu(embedded)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetAttentionDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length):\n",
    "        super(AlphabetAttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden, encoder_outputs):\n",
    "        #print('AlphabetAttentionDecoder_INPUT: inputs: {}'.format(inputs.shape))\n",
    "        #print('AlphabetAttentionDecoder_INPUT: hidden: {}'.format(hidden.shape))\n",
    "        #print('AlphabetAttentionDecoder_INPUT: encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = F.relu(embedded)\n",
    "        #print('** embedded: {}'.format(embedded.shape))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        \n",
    "        # add attention\n",
    "        scores = torch.cat((embedded[0], hidden[0]), dim=1)\n",
    "        #print('** scores: {}'.format(scores.shape))\n",
    "        attn_weights = F.softmax(self.attn(scores), dim=1)\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs.transpose(1, 0))\n",
    "        #print('** attn_weights: {}'.format(attn_weights.shape))\n",
    "        #print('** attn_weights sum: {}'.format(attn_weights.sum()))\n",
    "        #print('** encoder_outputs transposed: {}'.format(encoder_outputs.transpose(1, 0).shape))\n",
    "        #print('** attn_applied: {}'.format(attn_applied.shape))\n",
    "        \n",
    "        # make output\n",
    "        output = torch.cat((attn_applied.transpose(1, 0), embedded), 2)\n",
    "        output = self.attn_combine(output)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        #print('** gru-output: {}'.format(output.shape))\n",
    "        #print('** gru-hidden: {}'.format(hidden.shape))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #print('** output[0]: {}'.format(output[0].shape))\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        \n",
    "        #print('AlphabetAttentionDecoder_OUTPUT: output: {}'.format(output.shape))\n",
    "        #print('------------------------------------------')\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetInversionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        hidden_size = 256\n",
    "        encoder = AlphabetEncoder(26+3, hidden_size).to(device)\n",
    "        decoder = AlphabetDecoder(hidden_size, 26+3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_attention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder = AlphabetEncoder(26+3, hidden_size).to(device)\n",
    "decoder = AlphabetDecoder(hidden_size, 26+3).to(device)\n",
    "#decoder = AlphabetAttentionDecoder(hidden_size, 26+3, MAX_LENGTH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_t = inputs.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# encoder_hidden = encoder.initHidden(batch_size)\n",
    "# encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = 0\n",
    "eos = 1\n",
    "pad = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=5):\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    # zero_grad for encoder/decoder optimizer\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # zero_grad for encoder/decoder optimizer\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        tbar = tqdm(enumerate(dataloader), desc='training {}th epoch'.format(epoch))\n",
    "        ####################################\n",
    "        for i, batch in tbar:\n",
    "\n",
    "            # get inputs and labels\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #print('** inputs[0]: {}'.format(inputs[0]))\n",
    "            #print('** labels[0]: {}'.format(labels[0]))\n",
    "            \n",
    "            # transpose inputs and labels\n",
    "            inputs = inputs.transpose(1, 0)\n",
    "            labels = labels.transpose(1, 0)\n",
    "\n",
    "            # initialize hidden for encoder\n",
    "            batch_size = inputs.size()[1]\n",
    "            max_length = inputs.size()[0]\n",
    "            encoder_hidden = encoder.initHidden(batch_size)\n",
    "            encoder_outputs = torch.zeros(max_length, batch_size, hidden_size, device=device)\n",
    "\n",
    "            # encoding\n",
    "            for j, inp in enumerate(inputs):\n",
    "                inp = inp.unsqueeze(0)\n",
    "                encoder_output, encoder_hidden = encoder(inp, encoder_hidden)\n",
    "                #print('** encoder_output_{}: {}'.format(i, encoder_output.shape))\n",
    "                encoder_outputs[j] += encoder_output[:,0]\n",
    "\n",
    "            # initialize hidden for decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            loss = 0\n",
    "\n",
    "            decoder_inputs = torch.tensor([[bos]*batch_size], device=device)\n",
    "            #print('** decoder_inputs: {}'.format(decoder_inputs.shape))\n",
    "\n",
    "            # decoding\n",
    "            for inp in labels:\n",
    "                #inp = inp.unsqueeze(0)\n",
    "                #print('** inp: {}'.format(inp))\n",
    "                #print('** inp shape: {}'.format(inp.shape))\n",
    "                #return\n",
    "\n",
    "                #print('** decoder_inputs: {}'.format(decoder_inputs.shape))\n",
    "                #print('** decoder_inputs[:,0]: {}'.format(decoder_inputs[:,0]))\n",
    "                if use_attention:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_inputs, decoder_hidden, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_inputs, decoder_hidden)\n",
    "\n",
    "                #print('** inp shape VS decoder_output shape: {} VS {}'.format(inp.shape, decoder_output.shape))\n",
    "                #print('** inp[0]: {}'.format(inp[0]))\n",
    "                #print('** decoder_output[0]: {}'.format(decoder_output[0]))\n",
    "                #print('** -----')\n",
    "                loss_it = criterion(decoder_output, inp)\n",
    "                loss += loss_it\n",
    "\n",
    "                decoder_inputs = inp.unsqueeze(0)\n",
    "                #print('** label vs pred: {} vs {} → {:.4f}'.format(inp.shape, decoder_output.shape, loss_it))\n",
    "\n",
    "            #return\n",
    "            #print('total loss before backward: {:.4f}'.format(loss))\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "            decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "            #print('total loss after backward: {:.4f}'.format(loss))\n",
    "            #print('encoder, decoder: {:.4f} {:.4f}'.format(encoder_sum, decoder_sum))\n",
    "\n",
    "            # update encoder/decoder\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "            decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "            #print('encoder, decoder: {:.4f} {:.4f}'.format(encoder_sum, decoder_sum))\n",
    "            #print('total loss after step: {:.4f}'.format(loss))\n",
    "            #print('{}-{}th iteration → total loss after update: {:.4f}'.format(epoch, i, loss))\n",
    "            \n",
    "            tbar.set_postfix(loss=loss.data.item())\n",
    "            #return\n",
    "            #break\n",
    "        ####################################\n",
    "        #return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 1000it [00:46, 21.40it/s, loss=3.47e+4]\n",
      "training 1th epoch: 1000it [00:43, 22.92it/s, loss=4.62e+4]\n",
      "training 2th epoch: 1000it [00:45, 21.89it/s, loss=4.95e+4]\n",
      "training 3th epoch: 1000it [00:48, 20.74it/s, loss=7.37e+4]\n",
      "training 4th epoch: 117it [00:05, 20.90it/s, loss=3.95e+4]"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:01, 19.34it/s, loss=0.731] \n",
      "training 1th epoch: 32it [00:01, 24.39it/s, loss=0.701] \n",
      "training 2th epoch: 32it [00:01, 24.50it/s, loss=0.674] \n",
      "training 3th epoch: 32it [00:01, 24.39it/s, loss=0.65]  \n",
      "training 4th epoch: 32it [00:01, 24.23it/s, loss=0.627] \n",
      "training 5th epoch: 32it [00:01, 20.64it/s, loss=0.606] \n",
      "training 6th epoch: 32it [00:01, 20.59it/s, loss=0.586] \n",
      "training 7th epoch: 32it [00:01, 20.65it/s, loss=0.568] \n",
      "training 8th epoch: 32it [00:01, 20.56it/s, loss=0.551] \n",
      "training 9th epoch: 32it [00:01, 20.37it/s, loss=0.535] \n",
      "training 10th epoch: 32it [00:01, 19.61it/s, loss=0.52]  \n",
      "training 11th epoch: 32it [00:01, 17.94it/s, loss=0.505] \n",
      "training 12th epoch: 32it [00:01, 18.60it/s, loss=0.492] \n",
      "training 13th epoch: 32it [00:01, 18.15it/s, loss=0.479] \n",
      "training 14th epoch: 32it [00:01, 17.93it/s, loss=0.467] \n",
      "training 15th epoch: 32it [00:01, 17.15it/s, loss=0.456] \n",
      "training 16th epoch: 32it [00:01, 17.86it/s, loss=0.445] \n",
      "training 17th epoch: 32it [00:01, 18.24it/s, loss=0.434] \n",
      "training 18th epoch: 32it [00:01, 18.40it/s, loss=0.424] \n",
      "training 19th epoch: 32it [00:01, 18.17it/s, loss=0.415] \n",
      "training 20th epoch: 32it [00:01, 18.34it/s, loss=0.406] \n",
      "training 21th epoch: 32it [00:01, 18.16it/s, loss=0.397] \n",
      "training 22th epoch: 32it [00:01, 18.19it/s, loss=0.389] \n",
      "training 23th epoch: 32it [00:01, 18.04it/s, loss=0.381] \n",
      "training 24th epoch: 32it [00:01, 18.35it/s, loss=0.373] \n",
      "training 25th epoch: 32it [00:01, 18.25it/s, loss=0.366] \n",
      "training 26th epoch: 32it [00:01, 18.27it/s, loss=0.359] \n",
      "training 27th epoch: 32it [00:01, 18.43it/s, loss=0.352] \n",
      "training 28th epoch: 32it [00:01, 18.29it/s, loss=0.346] \n",
      "training 29th epoch: 32it [00:01, 18.11it/s, loss=0.34]  \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:01, 18.09it/s, loss=0.333] \n",
      "training 1th epoch: 32it [00:01, 19.98it/s, loss=0.328] \n",
      "training 2th epoch: 32it [00:01, 20.25it/s, loss=0.322] \n",
      "training 3th epoch: 32it [00:01, 20.15it/s, loss=0.316] \n",
      "training 4th epoch: 32it [00:01, 20.59it/s, loss=0.311] \n",
      "training 5th epoch: 32it [00:01, 20.78it/s, loss=0.306] \n",
      "training 6th epoch: 32it [00:01, 20.38it/s, loss=0.301] \n",
      "training 7th epoch: 32it [00:01, 20.51it/s, loss=0.296] \n",
      "training 8th epoch: 32it [00:01, 20.24it/s, loss=0.292] \n",
      "training 9th epoch: 32it [00:01, 20.58it/s, loss=0.287] \n",
      "training 10th epoch: 32it [00:01, 20.31it/s, loss=0.283] \n",
      "training 11th epoch: 32it [00:01, 20.66it/s, loss=0.279] \n",
      "training 12th epoch: 32it [00:01, 20.48it/s, loss=0.275] \n",
      "training 13th epoch: 32it [00:01, 19.97it/s, loss=0.271] \n",
      "training 14th epoch: 32it [00:01, 19.20it/s, loss=0.267] \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:02, 15.73it/s, loss=0.484] \n",
      "training 1th epoch: 32it [00:01, 17.67it/s, loss=0.474] \n",
      "training 2th epoch: 32it [00:02, 15.08it/s, loss=0.464] \n",
      "training 3th epoch: 32it [00:02, 15.08it/s, loss=0.454] \n",
      "training 4th epoch: 32it [00:02, 14.94it/s, loss=0.444] \n",
      "training 5th epoch: 32it [00:02, 14.74it/s, loss=0.435] \n",
      "training 6th epoch: 32it [00:02, 14.57it/s, loss=0.426] \n",
      "training 7th epoch: 32it [00:02, 14.56it/s, loss=0.418] \n",
      "training 8th epoch: 32it [00:02, 14.64it/s, loss=0.41]  \n",
      "training 9th epoch: 32it [00:02, 14.62it/s, loss=0.402] \n",
      "training 10th epoch: 32it [00:02, 13.98it/s, loss=0.394] \n",
      "training 11th epoch: 32it [00:02, 14.65it/s, loss=0.387] \n",
      "training 12th epoch: 32it [00:02, 14.71it/s, loss=0.38]  \n",
      "training 13th epoch: 32it [00:02, 14.66it/s, loss=0.373] \n",
      "training 14th epoch: 32it [00:02, 14.67it/s, loss=0.366] \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:02, 14.56it/s, loss=0.36]  \n",
      "training 1th epoch: 32it [00:02, 14.58it/s, loss=0.354] \n",
      "training 2th epoch: 32it [00:02, 14.68it/s, loss=0.348] \n",
      "training 3th epoch: 32it [00:02, 14.74it/s, loss=0.342] \n",
      "training 4th epoch: 32it [00:02, 14.80it/s, loss=0.336] \n",
      "training 5th epoch: 32it [00:02, 14.75it/s, loss=0.331] \n",
      "training 6th epoch: 32it [00:02, 14.78it/s, loss=0.325] \n",
      "training 7th epoch: 32it [00:02, 14.84it/s, loss=0.32]  \n",
      "training 8th epoch: 32it [00:02, 14.73it/s, loss=0.315] \n",
      "training 9th epoch: 32it [00:02, 14.80it/s, loss=0.31]  \n",
      "training 10th epoch: 32it [00:02, 14.54it/s, loss=0.305] \n",
      "training 11th epoch: 32it [00:02, 14.74it/s, loss=0.301] \n",
      "training 12th epoch: 32it [00:02, 14.75it/s, loss=0.296] \n",
      "training 13th epoch: 32it [00:02, 14.68it/s, loss=0.292] \n",
      "training 14th epoch: 32it [00:02, 14.63it/s, loss=0.287] \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:01, 22.82it/s, loss=0.263] \n",
      "training 1th epoch: 32it [00:01, 24.46it/s, loss=0.259] \n",
      "training 2th epoch: 32it [00:01, 24.31it/s, loss=0.256] \n",
      "training 3th epoch: 32it [00:01, 24.25it/s, loss=0.252] \n",
      "training 4th epoch: 32it [00:01, 24.69it/s, loss=0.249] \n",
      "training 5th epoch: 32it [00:01, 24.34it/s, loss=0.245] \n",
      "training 6th epoch: 32it [00:01, 24.31it/s, loss=0.242] \n",
      "training 7th epoch: 32it [00:01, 21.87it/s, loss=0.239] \n",
      "training 8th epoch: 32it [00:01, 20.72it/s, loss=0.236] \n",
      "training 9th epoch: 32it [00:01, 20.59it/s, loss=0.233] \n",
      "training 10th epoch: 32it [00:01, 20.71it/s, loss=0.23]  \n",
      "training 11th epoch: 32it [00:01, 20.57it/s, loss=0.227] \n",
      "training 12th epoch: 32it [00:01, 20.69it/s, loss=0.225] \n",
      "training 13th epoch: 32it [00:01, 20.70it/s, loss=0.222] \n",
      "training 14th epoch: 32it [00:01, 20.51it/s, loss=0.219] \n",
      "training 15th epoch: 32it [00:01, 20.65it/s, loss=0.217] \n",
      "training 16th epoch: 32it [00:01, 20.51it/s, loss=0.214] \n",
      "training 17th epoch: 32it [00:01, 20.31it/s, loss=0.212] \n",
      "training 18th epoch: 32it [00:01, 20.19it/s, loss=0.209] \n",
      "training 19th epoch: 32it [00:01, 20.80it/s, loss=0.207] \n",
      "training 20th epoch: 32it [00:01, 20.41it/s, loss=0.205] \n",
      "training 21th epoch: 32it [00:01, 20.08it/s, loss=0.202] \n",
      "training 22th epoch: 32it [00:01, 20.74it/s, loss=0.2]   \n",
      "training 23th epoch: 32it [00:01, 20.72it/s, loss=0.198] \n",
      "training 24th epoch: 32it [00:01, 20.23it/s, loss=0.196] \n",
      "training 25th epoch: 32it [00:01, 19.92it/s, loss=0.194] \n",
      "training 26th epoch: 32it [00:01, 20.41it/s, loss=0.192] \n",
      "training 27th epoch: 32it [00:01, 20.56it/s, loss=0.19]  \n",
      "training 28th epoch: 32it [00:01, 20.72it/s, loss=0.188] \n",
      "training 29th epoch: 32it [00:01, 18.82it/s, loss=0.186] \n",
      "training 30th epoch: 32it [00:01, 18.78it/s, loss=0.184] \n",
      "training 31th epoch: 32it [00:01, 18.58it/s, loss=0.182] \n",
      "training 32th epoch: 32it [00:01, 18.65it/s, loss=0.18]  \n",
      "training 33th epoch: 32it [00:01, 18.70it/s, loss=0.178] \n",
      "training 34th epoch: 32it [00:01, 18.78it/s, loss=0.177] \n",
      "training 35th epoch: 32it [00:01, 18.66it/s, loss=0.175] \n",
      "training 36th epoch: 32it [00:01, 18.36it/s, loss=0.173] \n",
      "training 37th epoch: 32it [00:01, 18.99it/s, loss=0.172] \n",
      "training 38th epoch: 32it [00:01, 18.85it/s, loss=0.17]  \n",
      "training 39th epoch: 32it [00:01, 18.86it/s, loss=0.168] \n",
      "training 40th epoch: 32it [00:01, 18.71it/s, loss=0.167] \n",
      "training 41th epoch: 32it [00:01, 18.91it/s, loss=0.165] \n",
      "training 42th epoch: 32it [00:01, 18.99it/s, loss=0.164] \n",
      "training 43th epoch: 32it [00:01, 18.88it/s, loss=0.162] \n",
      "training 44th epoch: 32it [00:01, 18.87it/s, loss=0.161] \n",
      "training 45th epoch: 32it [00:01, 19.05it/s, loss=0.159] \n",
      "training 46th epoch: 32it [00:01, 18.90it/s, loss=0.158] \n",
      "training 47th epoch: 32it [00:01, 18.47it/s, loss=0.157] \n",
      "training 48th epoch: 32it [00:01, 18.74it/s, loss=0.155] \n",
      "training 49th epoch: 32it [00:01, 18.94it/s, loss=0.154] \n",
      "training 50th epoch: 32it [00:01, 18.93it/s, loss=0.153] \n",
      "training 51th epoch: 32it [00:01, 18.82it/s, loss=0.151] \n",
      "training 52th epoch: 32it [00:01, 18.96it/s, loss=0.15]  \n",
      "training 53th epoch: 32it [00:01, 19.63it/s, loss=0.149] \n",
      "training 54th epoch: 32it [00:01, 20.40it/s, loss=0.148] \n",
      "training 55th epoch: 32it [00:01, 20.43it/s, loss=0.146] \n",
      "training 56th epoch: 32it [00:01, 20.64it/s, loss=0.145] \n",
      "training 57th epoch: 32it [00:01, 20.77it/s, loss=0.144] \n",
      "training 58th epoch: 32it [00:01, 20.49it/s, loss=0.143] \n",
      "training 59th epoch: 32it [00:01, 20.81it/s, loss=0.142] \n",
      "training 60th epoch: 32it [00:01, 20.65it/s, loss=0.141] \n",
      "training 61th epoch: 32it [00:01, 20.84it/s, loss=0.139] \n",
      "training 62th epoch: 32it [00:01, 20.60it/s, loss=0.138] \n",
      "training 63th epoch: 32it [00:01, 20.67it/s, loss=0.137] \n",
      "training 64th epoch: 32it [00:01, 20.64it/s, loss=0.136] \n",
      "training 65th epoch: 32it [00:01, 20.44it/s, loss=0.135] \n",
      "training 66th epoch: 32it [00:01, 20.81it/s, loss=0.134] \n",
      "training 67th epoch: 32it [00:01, 20.50it/s, loss=0.133] \n",
      "training 68th epoch: 32it [00:01, 20.75it/s, loss=0.132] \n",
      "training 69th epoch: 32it [00:01, 20.56it/s, loss=0.131] \n",
      "training 70th epoch: 32it [00:01, 20.83it/s, loss=0.13]  \n",
      "training 71th epoch: 32it [00:01, 20.49it/s, loss=0.129] \n",
      "training 72th epoch: 32it [00:01, 20.74it/s, loss=0.128] \n",
      "training 73th epoch: 32it [00:01, 20.50it/s, loss=0.127] \n",
      "training 74th epoch: 32it [00:01, 20.72it/s, loss=0.126] \n",
      "training 75th epoch: 32it [00:01, 20.68it/s, loss=0.126] \n",
      "training 76th epoch: 32it [00:01, 20.61it/s, loss=0.125] \n",
      "training 77th epoch: 32it [00:01, 20.12it/s, loss=0.124] \n",
      "training 78th epoch: 32it [00:01, 20.41it/s, loss=0.123] \n",
      "training 79th epoch: 32it [00:01, 20.28it/s, loss=0.122] \n",
      "training 80th epoch: 32it [00:01, 20.30it/s, loss=0.121] \n",
      "training 81th epoch: 32it [00:01, 19.82it/s, loss=0.12]  \n",
      "training 82th epoch: 32it [00:01, 20.17it/s, loss=0.12]  \n",
      "training 83th epoch: 32it [00:01, 20.38it/s, loss=0.119] \n",
      "training 84th epoch: 32it [00:01, 19.27it/s, loss=0.118] \n",
      "training 85th epoch: 32it [00:01, 20.08it/s, loss=0.117] \n",
      "training 86th epoch: 32it [00:01, 20.63it/s, loss=0.116] \n",
      "training 87th epoch: 32it [00:01, 20.06it/s, loss=0.116] \n",
      "training 88th epoch: 32it [00:01, 20.39it/s, loss=0.115] \n",
      "training 89th epoch: 32it [00:01, 20.31it/s, loss=0.114] \n",
      "training 90th epoch: 32it [00:01, 20.73it/s, loss=0.113] \n",
      "training 91th epoch: 32it [00:01, 20.56it/s, loss=0.113] \n",
      "training 92th epoch: 32it [00:01, 20.62it/s, loss=0.112] \n",
      "training 93th epoch: 32it [00:01, 20.36it/s, loss=0.111] \n",
      "training 94th epoch: 32it [00:01, 20.56it/s, loss=0.11]  \n",
      "training 95th epoch: 32it [00:01, 20.65it/s, loss=0.11]  \n",
      "training 96th epoch: 32it [00:01, 20.65it/s, loss=0.109] \n",
      "training 97th epoch: 32it [00:01, 20.43it/s, loss=0.108]  \n",
      "training 98th epoch: 32it [00:01, 20.45it/s, loss=0.108]  \n",
      "training 99th epoch: 32it [00:01, 20.81it/s, loss=0.107] \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:01, 20.43it/s, loss=0.106] \n",
      "training 1th epoch: 32it [00:01, 20.16it/s, loss=0.106]  \n",
      "training 2th epoch: 32it [00:01, 20.72it/s, loss=0.105] \n",
      "training 3th epoch: 32it [00:01, 20.42it/s, loss=0.104] \n",
      "training 4th epoch: 32it [00:01, 19.83it/s, loss=0.104]  \n",
      "training 5th epoch: 32it [00:01, 18.51it/s, loss=0.103]  \n",
      "training 6th epoch: 32it [00:01, 18.71it/s, loss=0.102]  \n",
      "training 7th epoch: 32it [00:01, 18.63it/s, loss=0.102]  \n",
      "training 8th epoch: 32it [00:01, 18.68it/s, loss=0.101]  \n",
      "training 9th epoch: 32it [00:01, 18.58it/s, loss=0.101]  \n",
      "training 10th epoch: 32it [00:01, 18.70it/s, loss=0.1]    \n",
      "training 11th epoch: 32it [00:01, 18.68it/s, loss=0.0995] \n",
      "training 12th epoch: 32it [00:01, 18.68it/s, loss=0.0989] \n",
      "training 13th epoch: 32it [00:01, 18.61it/s, loss=0.0984] \n",
      "training 14th epoch: 32it [00:01, 18.40it/s, loss=0.0978] \n",
      "training 15th epoch: 32it [00:01, 18.51it/s, loss=0.0972] \n",
      "training 16th epoch: 32it [00:01, 18.50it/s, loss=0.0967] \n",
      "training 17th epoch: 32it [00:01, 18.92it/s, loss=0.0961] \n",
      "training 18th epoch: 32it [00:01, 19.63it/s, loss=0.0956] \n",
      "training 19th epoch: 32it [00:01, 19.88it/s, loss=0.0951] \n",
      "training 20th epoch: 32it [00:01, 20.61it/s, loss=0.0945] \n",
      "training 21th epoch: 32it [00:01, 20.65it/s, loss=0.094]  \n",
      "training 22th epoch: 32it [00:01, 20.53it/s, loss=0.0935] \n",
      "training 23th epoch: 32it [00:01, 20.61it/s, loss=0.093]  \n",
      "training 24th epoch: 32it [00:01, 20.54it/s, loss=0.0925] \n",
      "training 25th epoch: 32it [00:01, 20.28it/s, loss=0.092]  \n",
      "training 26th epoch: 32it [00:01, 20.42it/s, loss=0.0915] \n",
      "training 27th epoch: 32it [00:01, 20.67it/s, loss=0.091]  \n",
      "training 28th epoch: 32it [00:01, 20.66it/s, loss=0.0905] \n",
      "training 29th epoch: 32it [00:01, 20.02it/s, loss=0.09]   \n",
      "training 30th epoch: 32it [00:01, 20.74it/s, loss=0.0895] \n",
      "training 31th epoch: 32it [00:01, 20.81it/s, loss=0.0891] \n",
      "training 32th epoch: 32it [00:01, 20.50it/s, loss=0.0886] \n",
      "training 33th epoch: 32it [00:01, 20.72it/s, loss=0.0881] \n",
      "training 34th epoch: 32it [00:01, 20.84it/s, loss=0.0877] \n",
      "training 35th epoch: 32it [00:01, 20.68it/s, loss=0.0872]\n",
      "training 36th epoch: 32it [00:01, 20.44it/s, loss=0.0868] \n",
      "training 37th epoch: 32it [00:01, 20.78it/s, loss=0.0863] \n",
      "training 38th epoch: 32it [00:01, 20.88it/s, loss=0.0859] \n",
      "training 39th epoch: 32it [00:01, 22.34it/s, loss=0.0854] \n",
      "training 40th epoch: 32it [00:01, 22.19it/s, loss=0.085]  \n",
      "training 41th epoch: 32it [00:01, 20.65it/s, loss=0.0846] \n",
      "training 42th epoch: 32it [00:01, 20.55it/s, loss=0.0841] \n",
      "training 43th epoch: 32it [00:01, 20.62it/s, loss=0.0837] \n",
      "training 44th epoch: 32it [00:01, 20.89it/s, loss=0.0833]\n",
      "training 45th epoch: 32it [00:01, 20.75it/s, loss=0.0829] \n",
      "training 46th epoch: 32it [00:01, 20.84it/s, loss=0.0825] \n",
      "training 47th epoch: 32it [00:01, 20.84it/s, loss=0.0821] \n",
      "training 48th epoch: 32it [00:01, 20.66it/s, loss=0.0817] \n",
      "training 49th epoch: 32it [00:01, 20.67it/s, loss=0.0813] \n",
      "training 50th epoch: 32it [00:01, 20.44it/s, loss=0.0809] \n",
      "training 51th epoch: 32it [00:01, 20.74it/s, loss=0.0805] \n",
      "training 52th epoch: 32it [00:01, 20.86it/s, loss=0.0801] \n",
      "training 53th epoch: 32it [00:01, 20.56it/s, loss=0.0797] \n",
      "training 54th epoch: 32it [00:01, 20.65it/s, loss=0.0793] \n",
      "training 55th epoch: 32it [00:01, 20.72it/s, loss=0.079]  \n",
      "training 56th epoch: 32it [00:01, 20.82it/s, loss=0.0786] \n",
      "training 57th epoch: 32it [00:01, 19.99it/s, loss=0.0782] \n",
      "training 58th epoch: 32it [00:01, 18.06it/s, loss=0.0779] \n",
      "training 59th epoch: 32it [00:01, 18.78it/s, loss=0.0775] \n",
      "training 60th epoch: 32it [00:01, 18.39it/s, loss=0.0771] \n",
      "training 61th epoch: 32it [00:01, 18.19it/s, loss=0.0768] \n",
      "training 62th epoch: 32it [00:01, 18.15it/s, loss=0.0764] \n",
      "training 63th epoch: 32it [00:01, 18.20it/s, loss=0.0761] \n",
      "training 64th epoch: 32it [00:01, 18.35it/s, loss=0.0757] \n",
      "training 65th epoch: 32it [00:01, 18.30it/s, loss=0.0754] \n",
      "training 66th epoch: 32it [00:01, 18.37it/s, loss=0.075]  \n",
      "training 67th epoch: 32it [00:01, 18.30it/s, loss=0.0747] \n",
      "training 68th epoch: 32it [00:01, 18.55it/s, loss=0.0744] \n",
      "training 69th epoch: 32it [00:01, 18.22it/s, loss=0.074]  \n",
      "training 70th epoch: 32it [00:01, 18.65it/s, loss=0.0737] \n",
      "training 71th epoch: 32it [00:01, 18.50it/s, loss=0.0734] \n",
      "training 72th epoch: 32it [00:01, 18.21it/s, loss=0.073]  \n",
      "training 73th epoch: 32it [00:01, 18.44it/s, loss=0.0727] \n",
      "training 74th epoch: 32it [00:01, 18.12it/s, loss=0.0724] \n",
      "training 75th epoch: 32it [00:01, 18.39it/s, loss=0.0721] \n",
      "training 76th epoch: 32it [00:01, 18.18it/s, loss=0.0718] \n",
      "training 77th epoch: 32it [00:01, 18.19it/s, loss=0.0714] \n",
      "training 78th epoch: 32it [00:01, 18.48it/s, loss=0.0711] \n",
      "training 79th epoch: 32it [00:01, 18.48it/s, loss=0.0708] \n",
      "training 80th epoch: 32it [00:01, 18.15it/s, loss=0.0705] \n",
      "training 81th epoch: 32it [00:01, 18.15it/s, loss=0.0702] \n",
      "training 82th epoch: 32it [00:01, 17.69it/s, loss=0.0699] \n",
      "training 83th epoch: 32it [00:01, 19.34it/s, loss=0.0696] \n",
      "training 84th epoch: 32it [00:01, 20.48it/s, loss=0.0693] \n",
      "training 85th epoch: 32it [00:01, 20.12it/s, loss=0.069]  \n",
      "training 86th epoch: 32it [00:01, 20.50it/s, loss=0.0688] \n",
      "training 87th epoch: 32it [00:01, 20.50it/s, loss=0.0685] \n",
      "training 88th epoch: 32it [00:01, 20.78it/s, loss=0.0682] \n",
      "training 89th epoch: 32it [00:01, 20.77it/s, loss=0.0679] \n",
      "training 90th epoch: 32it [00:01, 19.51it/s, loss=0.0676] \n",
      "training 91th epoch: 32it [00:01, 19.41it/s, loss=0.0673] \n",
      "training 92th epoch: 32it [00:01, 20.13it/s, loss=0.0671] \n",
      "training 93th epoch: 32it [00:01, 20.33it/s, loss=0.0668] \n",
      "training 94th epoch: 32it [00:01, 20.70it/s, loss=0.0665] \n",
      "training 95th epoch: 32it [00:01, 20.53it/s, loss=0.0662] \n",
      "training 96th epoch: 32it [00:01, 20.54it/s, loss=0.066]  \n",
      "training 97th epoch: 32it [00:01, 20.58it/s, loss=0.0657] \n",
      "training 98th epoch: 32it [00:01, 20.46it/s, loss=0.0654] \n",
      "training 99th epoch: 32it [00:01, 18.47it/s, loss=0.0652] \n",
      "training 100th epoch: 32it [00:01, 18.27it/s, loss=0.0649] \n",
      "training 101th epoch: 32it [00:01, 18.61it/s, loss=0.0647] \n",
      "training 102th epoch: 32it [00:01, 18.83it/s, loss=0.0644] \n",
      "training 103th epoch: 32it [00:01, 19.01it/s, loss=0.0641] \n",
      "training 104th epoch: 32it [00:01, 18.65it/s, loss=0.0639] \n",
      "training 105th epoch: 32it [00:01, 18.55it/s, loss=0.0636] \n",
      "training 106th epoch: 32it [00:01, 18.54it/s, loss=0.0634] \n",
      "training 107th epoch: 32it [00:01, 18.84it/s, loss=0.0631] \n",
      "training 108th epoch: 32it [00:01, 18.78it/s, loss=0.0629] \n",
      "training 109th epoch: 32it [00:01, 18.77it/s, loss=0.0627] \n",
      "training 110th epoch: 32it [00:01, 18.74it/s, loss=0.0624] \n",
      "training 111th epoch: 32it [00:01, 18.95it/s, loss=0.0622] \n",
      "training 112th epoch: 32it [00:01, 21.15it/s, loss=0.0619] \n",
      "training 113th epoch: 32it [00:01, 21.75it/s, loss=0.0617] \n",
      "training 114th epoch: 32it [00:01, 20.71it/s, loss=0.0615] \n",
      "training 115th epoch: 32it [00:01, 21.02it/s, loss=0.0612] \n",
      "training 116th epoch: 32it [00:01, 20.89it/s, loss=0.061]  \n",
      "training 117th epoch: 32it [00:01, 20.79it/s, loss=0.0608] \n",
      "training 118th epoch: 32it [00:01, 20.76it/s, loss=0.0605] \n",
      "training 119th epoch: 32it [00:01, 20.77it/s, loss=0.0603] \n",
      "training 120th epoch: 32it [00:01, 20.68it/s, loss=0.0601] \n",
      "training 121th epoch: 32it [00:01, 20.72it/s, loss=0.0599] \n",
      "training 122th epoch: 32it [00:01, 20.36it/s, loss=0.0596] \n",
      "training 123th epoch: 32it [00:01, 20.23it/s, loss=0.0594] \n",
      "training 124th epoch: 32it [00:01, 20.83it/s, loss=0.0592] \n",
      "training 125th epoch: 32it [00:01, 20.85it/s, loss=0.059]  \n",
      "training 126th epoch: 32it [00:01, 20.67it/s, loss=0.0588] \n",
      "training 127th epoch: 32it [00:01, 19.89it/s, loss=0.0586] \n",
      "training 128th epoch: 32it [00:01, 20.25it/s, loss=0.0583] \n",
      "training 129th epoch: 32it [00:01, 20.71it/s, loss=0.0581] \n",
      "training 130th epoch: 32it [00:01, 24.27it/s, loss=0.0579] \n",
      "training 131th epoch: 32it [00:01, 24.55it/s, loss=0.0577] \n",
      "training 132th epoch: 32it [00:01, 23.24it/s, loss=0.0575] \n",
      "training 133th epoch: 32it [00:01, 23.46it/s, loss=0.0573] \n",
      "training 134th epoch: 32it [00:01, 22.35it/s, loss=0.0571] \n",
      "training 135th epoch: 32it [00:01, 20.91it/s, loss=0.0569] \n",
      "training 136th epoch: 32it [00:01, 20.74it/s, loss=0.0567] \n",
      "training 137th epoch: 32it [00:01, 20.58it/s, loss=0.0565] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 138th epoch: 32it [00:01, 20.39it/s, loss=0.0563] \n",
      "training 139th epoch: 32it [00:01, 20.54it/s, loss=0.0561] \n",
      "training 140th epoch: 32it [00:01, 20.54it/s, loss=0.0559] \n",
      "training 141th epoch: 32it [00:01, 20.26it/s, loss=0.0557] \n",
      "training 142th epoch: 32it [00:01, 20.75it/s, loss=0.0555] \n",
      "training 143th epoch: 32it [00:01, 20.59it/s, loss=0.0553] \n",
      "training 144th epoch: 32it [00:01, 20.48it/s, loss=0.0551] \n",
      "training 145th epoch: 32it [00:01, 20.60it/s, loss=0.0549] \n",
      "training 146th epoch: 32it [00:01, 20.59it/s, loss=0.0547] \n",
      "training 147th epoch: 32it [00:01, 20.55it/s, loss=0.0546] \n",
      "training 148th epoch: 32it [00:01, 21.37it/s, loss=0.0544] \n",
      "training 149th epoch: 32it [00:01, 21.32it/s, loss=0.0542] \n",
      "training 150th epoch: 32it [00:01, 20.46it/s, loss=0.054]  \n",
      "training 151th epoch: 32it [00:01, 20.66it/s, loss=0.0538] \n",
      "training 152th epoch: 32it [00:01, 20.80it/s, loss=0.0536] \n",
      "training 153th epoch: 32it [00:01, 20.84it/s, loss=0.0535] \n",
      "training 154th epoch: 32it [00:01, 20.89it/s, loss=0.0533] \n",
      "training 155th epoch: 32it [00:01, 20.48it/s, loss=0.0531] \n",
      "training 156th epoch: 32it [00:01, 20.53it/s, loss=0.0529] \n",
      "training 157th epoch: 32it [00:01, 22.07it/s, loss=0.0527] \n",
      "training 158th epoch: 32it [00:01, 24.06it/s, loss=0.0526] \n",
      "training 159th epoch: 32it [00:01, 24.21it/s, loss=0.0524] \n",
      "training 160th epoch: 32it [00:01, 24.24it/s, loss=0.0522] \n",
      "training 161th epoch: 32it [00:01, 24.68it/s, loss=0.0521] \n",
      "training 162th epoch: 32it [00:01, 24.37it/s, loss=0.0519] \n",
      "training 163th epoch: 32it [00:01, 24.32it/s, loss=0.0517] \n",
      "training 164th epoch: 32it [00:01, 23.50it/s, loss=0.0515] \n",
      "training 165th epoch: 32it [00:01, 20.82it/s, loss=0.0514] \n",
      "training 166th epoch: 32it [00:01, 20.81it/s, loss=0.0512] \n",
      "training 167th epoch: 32it [00:01, 19.12it/s, loss=0.051]  \n",
      "training 168th epoch: 32it [00:01, 20.40it/s, loss=0.0509] \n",
      "training 169th epoch: 32it [00:01, 20.67it/s, loss=0.0507] \n",
      "training 170th epoch: 32it [00:01, 20.13it/s, loss=0.0506] \n",
      "training 171th epoch: 32it [00:01, 20.07it/s, loss=0.0504] \n",
      "training 172th epoch: 32it [00:01, 20.36it/s, loss=0.0502] \n",
      "training 173th epoch: 32it [00:01, 20.59it/s, loss=0.0501] \n",
      "training 174th epoch: 32it [00:01, 20.54it/s, loss=0.0499] \n",
      "training 175th epoch: 32it [00:01, 20.49it/s, loss=0.0498] \n",
      "training 176th epoch: 32it [00:01, 20.71it/s, loss=0.0496] \n",
      "training 177th epoch: 32it [00:01, 20.83it/s, loss=0.0494] \n",
      "training 178th epoch: 32it [00:01, 20.47it/s, loss=0.0493] \n",
      "training 179th epoch: 32it [00:01, 20.61it/s, loss=0.0491] \n",
      "training 180th epoch: 32it [00:01, 20.74it/s, loss=0.049]  \n",
      "training 181th epoch: 32it [00:01, 20.39it/s, loss=0.0488] \n",
      "training 182th epoch: 32it [00:01, 22.67it/s, loss=0.0487] \n",
      "training 183th epoch: 32it [00:01, 24.26it/s, loss=0.0485] \n",
      "training 184th epoch: 32it [00:01, 22.64it/s, loss=0.0484] \n",
      "training 185th epoch: 32it [00:01, 20.34it/s, loss=0.0482] \n",
      "training 186th epoch: 32it [00:01, 19.84it/s, loss=0.0481] \n",
      "training 187th epoch: 32it [00:01, 20.54it/s, loss=0.0479] \n",
      "training 188th epoch: 32it [00:01, 20.86it/s, loss=0.0478] \n",
      "training 189th epoch: 32it [00:01, 20.64it/s, loss=0.0476] \n",
      "training 190th epoch: 32it [00:01, 20.75it/s, loss=0.0475] \n",
      "training 191th epoch: 32it [00:01, 20.93it/s, loss=0.0474] \n",
      "training 192th epoch: 32it [00:01, 20.53it/s, loss=0.0472] \n",
      "training 193th epoch: 32it [00:01, 20.20it/s, loss=0.0471] \n",
      "training 194th epoch: 32it [00:01, 19.15it/s, loss=0.0469] \n",
      "training 195th epoch: 32it [00:01, 20.82it/s, loss=0.0468] \n",
      "training 196th epoch: 32it [00:01, 20.74it/s, loss=0.0466] \n",
      "training 197th epoch: 32it [00:01, 20.58it/s, loss=0.0465] \n",
      "training 198th epoch: 32it [00:01, 20.72it/s, loss=0.0464] \n",
      "training 199th epoch: 32it [00:01, 20.86it/s, loss=0.0462] \n",
      "training 200th epoch: 32it [00:01, 20.80it/s, loss=0.0461] \n",
      "training 201th epoch: 32it [00:01, 20.61it/s, loss=0.046]  \n",
      "training 202th epoch: 32it [00:01, 20.46it/s, loss=0.0458] \n",
      "training 203th epoch: 32it [00:01, 20.61it/s, loss=0.0457] \n",
      "training 204th epoch: 32it [00:01, 19.85it/s, loss=0.0456] \n",
      "training 205th epoch: 32it [00:01, 18.20it/s, loss=0.0454] \n",
      "training 206th epoch: 32it [00:01, 18.00it/s, loss=0.0453] \n",
      "training 207th epoch: 32it [00:01, 19.14it/s, loss=0.0452] \n",
      "training 208th epoch: 32it [00:01, 20.51it/s, loss=0.045]  \n",
      "training 209th epoch: 32it [00:01, 20.23it/s, loss=0.0449] \n",
      "training 210th epoch: 32it [00:01, 20.54it/s, loss=0.0448] \n",
      "training 211th epoch: 32it [00:01, 19.30it/s, loss=0.0446] \n",
      "training 212th epoch: 32it [00:01, 20.61it/s, loss=0.0445] \n",
      "training 213th epoch: 32it [00:01, 20.71it/s, loss=0.0444] \n",
      "training 214th epoch: 32it [00:01, 20.28it/s, loss=0.0443] \n",
      "training 215th epoch: 32it [00:01, 20.82it/s, loss=0.0441] \n",
      "training 216th epoch: 32it [00:01, 20.42it/s, loss=0.044]  \n",
      "training 217th epoch: 32it [00:01, 20.05it/s, loss=0.0439] \n",
      "training 218th epoch: 32it [00:01, 19.14it/s, loss=0.0438] \n",
      "training 219th epoch: 32it [00:01, 19.65it/s, loss=0.0436] \n",
      "training 220th epoch: 32it [00:01, 19.64it/s, loss=0.0435] \n",
      "training 221th epoch: 32it [00:01, 20.58it/s, loss=0.0434] \n",
      "training 222th epoch: 32it [00:01, 20.63it/s, loss=0.0433] \n",
      "training 223th epoch: 32it [00:01, 20.38it/s, loss=0.0431] \n",
      "training 224th epoch: 32it [00:01, 20.86it/s, loss=0.043]  \n",
      "training 225th epoch: 32it [00:01, 20.04it/s, loss=0.0429] \n",
      "training 226th epoch: 32it [00:01, 19.42it/s, loss=0.0428] \n",
      "training 227th epoch: 32it [00:01, 20.78it/s, loss=0.0427] \n",
      "training 228th epoch: 32it [00:01, 20.65it/s, loss=0.0426] \n",
      "training 229th epoch: 32it [00:01, 20.30it/s, loss=0.0424] \n",
      "training 230th epoch: 32it [00:01, 20.78it/s, loss=0.0423] \n",
      "training 231th epoch: 32it [00:01, 21.44it/s, loss=0.0422] \n",
      "training 232th epoch: 32it [00:01, 24.41it/s, loss=0.0421] \n",
      "training 233th epoch: 32it [00:01, 24.14it/s, loss=0.042]  \n",
      "training 234th epoch: 32it [00:01, 24.23it/s, loss=0.0419] \n",
      "training 235th epoch: 32it [00:01, 24.18it/s, loss=0.0417] \n",
      "training 236th epoch: 32it [00:01, 24.37it/s, loss=0.0416] \n",
      "training 237th epoch: 32it [00:01, 24.36it/s, loss=0.0415] \n",
      "training 238th epoch: 32it [00:01, 24.27it/s, loss=0.0414] \n",
      "training 239th epoch: 32it [00:01, 24.34it/s, loss=0.0413] \n",
      "training 240th epoch: 32it [00:01, 23.94it/s, loss=0.0412] \n",
      "training 241th epoch: 32it [00:01, 24.17it/s, loss=0.0411] \n",
      "training 242th epoch: 32it [00:01, 24.38it/s, loss=0.041]  \n",
      "training 243th epoch: 32it [00:01, 21.50it/s, loss=0.0409] \n",
      "training 244th epoch: 32it [00:01, 20.77it/s, loss=0.0408] \n",
      "training 245th epoch: 32it [00:01, 20.68it/s, loss=0.0406] \n",
      "training 246th epoch: 32it [00:01, 20.59it/s, loss=0.0405] \n",
      "training 247th epoch: 32it [00:01, 20.32it/s, loss=0.0404] \n",
      "training 248th epoch: 32it [00:01, 20.88it/s, loss=0.0403] \n",
      "training 249th epoch: 32it [00:01, 20.74it/s, loss=0.0402] \n",
      "training 250th epoch: 32it [00:01, 20.26it/s, loss=0.0401] \n",
      "training 251th epoch: 32it [00:01, 19.02it/s, loss=0.04]   \n",
      "training 252th epoch: 32it [00:01, 19.21it/s, loss=0.0399] \n",
      "training 253th epoch: 32it [00:01, 20.64it/s, loss=0.0398] \n",
      "training 254th epoch: 32it [00:01, 20.18it/s, loss=0.0397] \n",
      "training 255th epoch: 32it [00:01, 20.61it/s, loss=0.0396] \n",
      "training 256th epoch: 32it [00:01, 20.31it/s, loss=0.0395] \n",
      "training 257th epoch: 32it [00:01, 20.47it/s, loss=0.0394] \n",
      "training 258th epoch: 32it [00:01, 20.64it/s, loss=0.0393] \n",
      "training 259th epoch: 32it [00:01, 19.88it/s, loss=0.0392] \n",
      "training 260th epoch: 32it [00:01, 20.48it/s, loss=0.0391] \n",
      "training 261th epoch: 32it [00:01, 20.59it/s, loss=0.039]  \n",
      "training 262th epoch: 32it [00:01, 20.12it/s, loss=0.0389] \n",
      "training 263th epoch: 32it [00:01, 20.49it/s, loss=0.0388] \n",
      "training 264th epoch: 32it [00:01, 20.29it/s, loss=0.0387] \n",
      "training 265th epoch: 32it [00:01, 20.67it/s, loss=0.0386] \n",
      "training 266th epoch: 32it [00:01, 20.66it/s, loss=0.0385] \n",
      "training 267th epoch: 32it [00:01, 20.23it/s, loss=0.0384] \n",
      "training 268th epoch: 32it [00:01, 20.26it/s, loss=0.0383] \n",
      "training 269th epoch: 32it [00:01, 20.16it/s, loss=0.0382] \n",
      "training 270th epoch: 32it [00:01, 20.67it/s, loss=0.0381] \n",
      "training 271th epoch: 32it [00:01, 20.89it/s, loss=0.038]  \n",
      "training 272th epoch: 32it [00:01, 20.60it/s, loss=0.0379] \n",
      "training 273th epoch: 32it [00:01, 19.44it/s, loss=0.0378] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 274th epoch: 32it [00:01, 19.29it/s, loss=0.0377] \n",
      "training 275th epoch: 32it [00:01, 19.30it/s, loss=0.0376] \n",
      "training 276th epoch: 32it [00:01, 20.57it/s, loss=0.0376] \n",
      "training 277th epoch: 32it [00:01, 20.84it/s, loss=0.0375] \n",
      "training 278th epoch: 32it [00:01, 20.82it/s, loss=0.0374] \n",
      "training 279th epoch: 32it [00:01, 20.49it/s, loss=0.0373] \n",
      "training 280th epoch: 32it [00:01, 20.70it/s, loss=0.0372] \n",
      "training 281th epoch: 32it [00:01, 20.74it/s, loss=0.0371] \n",
      "training 282th epoch: 32it [00:01, 20.47it/s, loss=0.037]  \n",
      "training 283th epoch: 32it [00:01, 20.68it/s, loss=0.0369] \n",
      "training 284th epoch: 32it [00:01, 20.35it/s, loss=0.0368] \n",
      "training 285th epoch: 32it [00:01, 20.59it/s, loss=0.0367] \n",
      "training 286th epoch: 32it [00:01, 20.75it/s, loss=0.0366] \n",
      "training 287th epoch: 32it [00:01, 20.67it/s, loss=0.0366] \n",
      "training 288th epoch: 32it [00:01, 20.65it/s, loss=0.0365] \n",
      "training 289th epoch: 32it [00:01, 20.47it/s, loss=0.0364] \n",
      "training 290th epoch: 32it [00:01, 20.74it/s, loss=0.0363] \n",
      "training 291th epoch: 32it [00:01, 20.90it/s, loss=0.0362] \n",
      "training 292th epoch: 32it [00:01, 20.40it/s, loss=0.0361] \n",
      "training 293th epoch: 32it [00:01, 20.54it/s, loss=0.036]  \n",
      "training 294th epoch: 32it [00:01, 20.83it/s, loss=0.0359] \n",
      "training 295th epoch: 32it [00:01, 20.65it/s, loss=0.0359] \n",
      "training 296th epoch: 32it [00:01, 20.73it/s, loss=0.0358] \n",
      "training 297th epoch: 32it [00:01, 20.89it/s, loss=0.0357] \n",
      "training 298th epoch: 32it [00:01, 20.62it/s, loss=0.0356] \n",
      "training 299th epoch: 32it [00:01, 20.76it/s, loss=0.0355] \n",
      "training 300th epoch: 32it [00:01, 20.79it/s, loss=0.0354] \n",
      "training 301th epoch: 32it [00:01, 20.89it/s, loss=0.0354] \n",
      "training 302th epoch: 32it [00:01, 20.72it/s, loss=0.0353] \n",
      "training 303th epoch: 32it [00:01, 20.76it/s, loss=0.0352] \n",
      "training 304th epoch: 32it [00:01, 20.75it/s, loss=0.0351] \n",
      "training 305th epoch: 32it [00:01, 20.46it/s, loss=0.035]  \n",
      "training 306th epoch: 32it [00:01, 20.90it/s, loss=0.035]  \n",
      "training 307th epoch: 32it [00:01, 21.81it/s, loss=0.0349] \n",
      "training 308th epoch: 32it [00:01, 20.49it/s, loss=0.0348] \n",
      "training 309th epoch: 32it [00:01, 18.96it/s, loss=0.0347] \n",
      "training 310th epoch: 32it [00:01, 20.44it/s, loss=0.0346] \n",
      "training 311th epoch: 32it [00:01, 20.88it/s, loss=0.0346] \n",
      "training 312th epoch: 32it [00:01, 20.80it/s, loss=0.0345] \n",
      "training 313th epoch: 32it [00:01, 20.06it/s, loss=0.0344] \n",
      "training 314th epoch: 32it [00:01, 20.58it/s, loss=0.0343] \n",
      "training 315th epoch: 32it [00:01, 20.80it/s, loss=0.0342] \n",
      "training 316th epoch: 32it [00:01, 20.66it/s, loss=0.0342] \n",
      "training 317th epoch: 32it [00:01, 20.79it/s, loss=0.0341] \n",
      "training 318th epoch: 32it [00:01, 20.35it/s, loss=0.034]  \n",
      "training 319th epoch: 32it [00:01, 20.65it/s, loss=0.0339] \n",
      "training 320th epoch: 32it [00:01, 19.62it/s, loss=0.0339] \n",
      "training 321th epoch: 32it [00:01, 18.76it/s, loss=0.0338] \n",
      "training 322th epoch: 32it [00:01, 19.04it/s, loss=0.0337] \n",
      "training 323th epoch: 32it [00:01, 18.58it/s, loss=0.0336] \n",
      "training 324th epoch: 32it [00:01, 18.85it/s, loss=0.0335] \n",
      "training 325th epoch: 32it [00:01, 18.63it/s, loss=0.0335] \n",
      "training 326th epoch: 32it [00:01, 18.43it/s, loss=0.0334] \n",
      "training 327th epoch: 32it [00:01, 18.36it/s, loss=0.0333] \n",
      "training 328th epoch: 32it [00:01, 18.85it/s, loss=0.0333] \n",
      "training 329th epoch: 32it [00:01, 18.63it/s, loss=0.0332] \n",
      "training 330th epoch: 32it [00:01, 18.93it/s, loss=0.0331] \n",
      "training 331th epoch: 32it [00:01, 18.40it/s, loss=0.033]  \n",
      "training 332th epoch: 32it [00:01, 18.75it/s, loss=0.033]  \n",
      "training 333th epoch: 32it [00:01, 18.72it/s, loss=0.0329] \n",
      "training 334th epoch: 32it [00:01, 18.68it/s, loss=0.0328] \n",
      "training 335th epoch: 32it [00:01, 18.54it/s, loss=0.0327] \n",
      "training 336th epoch: 32it [00:01, 18.69it/s, loss=0.0327] \n",
      "training 337th epoch: 32it [00:01, 18.54it/s, loss=0.0326] \n",
      "training 338th epoch: 32it [00:01, 19.37it/s, loss=0.0325] \n",
      "training 339th epoch: 32it [00:01, 20.59it/s, loss=0.0325] \n",
      "training 340th epoch: 32it [00:01, 20.29it/s, loss=0.0324] \n",
      "training 341th epoch: 32it [00:01, 20.88it/s, loss=0.0323] \n",
      "training 342th epoch: 32it [00:01, 20.58it/s, loss=0.0322] \n",
      "training 343th epoch: 32it [00:01, 20.65it/s, loss=0.0322] \n",
      "training 344th epoch: 32it [00:01, 20.78it/s, loss=0.0321] \n",
      "training 345th epoch: 32it [00:01, 20.61it/s, loss=0.032]  \n",
      "training 346th epoch: 32it [00:01, 20.77it/s, loss=0.032]  \n",
      "training 347th epoch: 32it [00:01, 20.44it/s, loss=0.0319] \n",
      "training 348th epoch: 32it [00:01, 20.75it/s, loss=0.0318] \n",
      "training 349th epoch: 32it [00:01, 20.59it/s, loss=0.0318] \n",
      "training 350th epoch: 32it [00:01, 20.55it/s, loss=0.0317] \n",
      "training 351th epoch: 32it [00:01, 20.35it/s, loss=0.0316] \n",
      "training 352th epoch: 32it [00:01, 20.81it/s, loss=0.0316] \n",
      "training 353th epoch: 32it [00:01, 20.56it/s, loss=0.0315] \n",
      "training 354th epoch: 32it [00:01, 20.88it/s, loss=0.0314] \n",
      "training 355th epoch: 32it [00:01, 20.89it/s, loss=0.0314] \n",
      "training 356th epoch: 32it [00:01, 20.88it/s, loss=0.0313] \n",
      "training 357th epoch: 32it [00:01, 20.62it/s, loss=0.0312] \n",
      "training 358th epoch: 32it [00:01, 20.78it/s, loss=0.0312] \n",
      "training 359th epoch: 32it [00:01, 20.54it/s, loss=0.0311] \n",
      "training 360th epoch: 32it [00:01, 21.11it/s, loss=0.031]  \n",
      "training 361th epoch: 32it [00:01, 21.00it/s, loss=0.031]  \n",
      "training 362th epoch: 32it [00:01, 20.96it/s, loss=0.0309] \n",
      "training 363th epoch: 32it [00:01, 20.74it/s, loss=0.0308] \n",
      "training 364th epoch: 32it [00:01, 20.77it/s, loss=0.0308] \n",
      "training 365th epoch: 32it [00:01, 20.99it/s, loss=0.0307] \n",
      "training 366th epoch: 32it [00:01, 20.64it/s, loss=0.0307] \n",
      "training 367th epoch: 32it [00:01, 19.29it/s, loss=0.0306] \n",
      "training 368th epoch: 32it [00:01, 21.33it/s, loss=0.0305] \n",
      "training 369th epoch: 32it [00:01, 22.43it/s, loss=0.0305] \n",
      "training 370th epoch: 32it [00:01, 24.38it/s, loss=0.0304] \n",
      "training 371th epoch: 32it [00:01, 24.20it/s, loss=0.0303] \n",
      "training 372th epoch: 32it [00:01, 24.18it/s, loss=0.0303] \n",
      "training 373th epoch: 32it [00:01, 24.40it/s, loss=0.0302] \n",
      "training 374th epoch: 32it [00:01, 23.97it/s, loss=0.0302] \n",
      "training 375th epoch: 32it [00:01, 24.15it/s, loss=0.0301] \n",
      "training 376th epoch: 32it [00:01, 24.37it/s, loss=0.03]   \n",
      "training 377th epoch: 32it [00:01, 24.10it/s, loss=0.03]   \n",
      "training 378th epoch: 32it [00:01, 24.10it/s, loss=0.0299] \n",
      "training 379th epoch: 32it [00:01, 21.15it/s, loss=0.0299] \n",
      "training 380th epoch: 32it [00:01, 20.49it/s, loss=0.0298] \n",
      "training 381th epoch: 32it [00:01, 20.22it/s, loss=0.0297] \n",
      "training 382th epoch: 32it [00:01, 20.36it/s, loss=0.0297] \n",
      "training 383th epoch: 32it [00:01, 20.47it/s, loss=0.0296] \n",
      "training 384th epoch: 32it [00:01, 20.43it/s, loss=0.0296] \n",
      "training 385th epoch: 32it [00:01, 19.76it/s, loss=0.0295] \n",
      "training 386th epoch: 32it [00:01, 19.25it/s, loss=0.0294] \n",
      "training 387th epoch: 32it [00:01, 19.37it/s, loss=0.0294] \n",
      "training 388th epoch: 32it [00:01, 19.58it/s, loss=0.0293] \n",
      "training 389th epoch: 32it [00:01, 19.62it/s, loss=0.0293] \n",
      "training 390th epoch: 32it [00:01, 19.44it/s, loss=0.0292] \n",
      "training 391th epoch: 32it [00:01, 19.52it/s, loss=0.0291] \n",
      "training 392th epoch: 32it [00:01, 19.43it/s, loss=0.0291] \n",
      "training 393th epoch: 32it [00:01, 19.15it/s, loss=0.029]  \n",
      "training 394th epoch: 32it [00:01, 19.34it/s, loss=0.029]  \n",
      "training 395th epoch: 32it [00:01, 20.77it/s, loss=0.0289] \n",
      "training 396th epoch: 32it [00:01, 20.66it/s, loss=0.0289] \n",
      "training 397th epoch: 32it [00:01, 20.48it/s, loss=0.0288] \n",
      "training 398th epoch: 32it [00:01, 20.40it/s, loss=0.0287] \n",
      "training 399th epoch: 32it [00:01, 19.51it/s, loss=0.0287] \n",
      "training 400th epoch: 32it [00:01, 18.48it/s, loss=0.0286] \n",
      "training 401th epoch: 32it [00:01, 17.92it/s, loss=0.0286] \n",
      "training 402th epoch: 32it [00:01, 18.08it/s, loss=0.0285] \n",
      "training 403th epoch: 32it [00:01, 19.56it/s, loss=0.0285] \n",
      "training 404th epoch: 32it [00:01, 19.98it/s, loss=0.0284] \n",
      "training 405th epoch: 32it [00:01, 20.56it/s, loss=0.0284] \n",
      "training 406th epoch: 32it [00:01, 20.42it/s, loss=0.0283] \n",
      "training 407th epoch: 32it [00:01, 20.16it/s, loss=0.0283] \n",
      "training 408th epoch: 32it [00:01, 20.68it/s, loss=0.0282] \n",
      "training 409th epoch: 32it [00:01, 20.29it/s, loss=0.0281] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 410th epoch: 32it [00:01, 20.53it/s, loss=0.0281] \n",
      "training 411th epoch: 32it [00:01, 20.39it/s, loss=0.028]  \n",
      "training 412th epoch: 32it [00:01, 20.49it/s, loss=0.028]  \n",
      "training 413th epoch: 32it [00:01, 20.33it/s, loss=0.0279] \n",
      "training 414th epoch: 32it [00:01, 20.50it/s, loss=0.0279] \n",
      "training 415th epoch: 32it [00:01, 20.64it/s, loss=0.0278] \n",
      "training 416th epoch: 32it [00:01, 21.27it/s, loss=0.0278] \n",
      "training 417th epoch: 32it [00:01, 20.54it/s, loss=0.0277] \n",
      "training 418th epoch: 32it [00:01, 20.76it/s, loss=0.0277] \n",
      "training 419th epoch: 32it [00:01, 20.48it/s, loss=0.0276] \n",
      "training 420th epoch: 32it [00:01, 20.36it/s, loss=0.0276] \n",
      "training 421th epoch: 32it [00:01, 21.14it/s, loss=0.0275] \n",
      "training 422th epoch: 32it [00:01, 21.08it/s, loss=0.0275] \n",
      "training 423th epoch: 32it [00:01, 20.37it/s, loss=0.0274] \n",
      "training 424th epoch: 32it [00:01, 20.18it/s, loss=0.0274] \n",
      "training 425th epoch: 32it [00:01, 19.02it/s, loss=0.0273] \n",
      "training 426th epoch: 32it [00:01, 19.08it/s, loss=0.0273] \n",
      "training 427th epoch: 32it [00:01, 19.03it/s, loss=0.0272] \n",
      "training 428th epoch: 32it [00:01, 18.88it/s, loss=0.0272] \n",
      "training 429th epoch: 32it [00:01, 18.99it/s, loss=0.0271] \n",
      "training 430th epoch: 32it [00:01, 18.97it/s, loss=0.0271] \n",
      "training 431th epoch: 32it [00:01, 18.75it/s, loss=0.027]  \n",
      "training 432th epoch: 32it [00:01, 18.65it/s, loss=0.027]  \n",
      "training 433th epoch: 32it [00:01, 18.52it/s, loss=0.0269] \n",
      "training 434th epoch: 32it [00:01, 18.92it/s, loss=0.0269] \n",
      "training 435th epoch: 32it [00:01, 18.88it/s, loss=0.0268] \n",
      "training 436th epoch: 32it [00:01, 18.83it/s, loss=0.0268] \n",
      "training 437th epoch: 32it [00:01, 18.92it/s, loss=0.0267] \n",
      "training 438th epoch: 32it [00:01, 18.61it/s, loss=0.0267] \n",
      "training 439th epoch: 32it [00:01, 19.08it/s, loss=0.0266] \n",
      "training 440th epoch: 32it [00:01, 19.03it/s, loss=0.0266] \n",
      "training 441th epoch: 32it [00:01, 18.72it/s, loss=0.0265] \n",
      "training 442th epoch: 32it [00:01, 19.85it/s, loss=0.0265] \n",
      "training 443th epoch: 32it [00:01, 20.58it/s, loss=0.0264] \n",
      "training 444th epoch: 32it [00:01, 19.54it/s, loss=0.0264] \n",
      "training 445th epoch: 32it [00:01, 19.99it/s, loss=0.0263] \n",
      "training 446th epoch: 32it [00:01, 20.27it/s, loss=0.0263] \n",
      "training 447th epoch: 32it [00:01, 20.66it/s, loss=0.0262] \n",
      "training 448th epoch: 32it [00:01, 20.89it/s, loss=0.0262] \n",
      "training 449th epoch: 32it [00:01, 20.46it/s, loss=0.0261] \n",
      "training 450th epoch: 32it [00:01, 20.76it/s, loss=0.0261] \n",
      "training 451th epoch: 32it [00:01, 20.88it/s, loss=0.026]  \n",
      "training 452th epoch: 32it [00:01, 22.27it/s, loss=0.026]  \n",
      "training 453th epoch: 32it [00:01, 20.84it/s, loss=0.0259] \n",
      "training 454th epoch: 32it [00:01, 20.49it/s, loss=0.0259] \n",
      "training 455th epoch: 32it [00:01, 19.46it/s, loss=0.0259] \n",
      "training 456th epoch: 32it [00:01, 20.51it/s, loss=0.0258] \n",
      "training 457th epoch: 32it [00:01, 20.64it/s, loss=0.0258] \n",
      "training 458th epoch: 32it [00:01, 20.68it/s, loss=0.0257] \n",
      "training 459th epoch: 32it [00:01, 21.02it/s, loss=0.0257] \n",
      "training 460th epoch: 32it [00:01, 20.03it/s, loss=0.0256] \n",
      "training 461th epoch: 32it [00:01, 20.31it/s, loss=0.0256] \n",
      "training 462th epoch: 32it [00:01, 19.58it/s, loss=0.0255] \n",
      "training 463th epoch: 32it [00:01, 20.94it/s, loss=0.0255] \n",
      "training 464th epoch: 32it [00:01, 18.76it/s, loss=0.0254] \n",
      "training 465th epoch: 32it [00:01, 18.11it/s, loss=0.0254] \n",
      "training 466th epoch: 32it [00:01, 18.33it/s, loss=0.0254] \n",
      "training 467th epoch: 32it [00:01, 18.24it/s, loss=0.0253] \n",
      "training 468th epoch: 32it [00:01, 17.26it/s, loss=0.0253] \n",
      "training 469th epoch: 32it [00:01, 18.07it/s, loss=0.0252] \n",
      "training 470th epoch: 32it [00:01, 18.02it/s, loss=0.0252] \n",
      "training 471th epoch: 32it [00:01, 18.18it/s, loss=0.0251] \n",
      "training 472th epoch: 32it [00:01, 17.83it/s, loss=0.0251] \n",
      "training 473th epoch: 32it [00:01, 18.27it/s, loss=0.025]  \n",
      "training 474th epoch: 32it [00:01, 18.44it/s, loss=0.025]  \n",
      "training 475th epoch: 32it [00:01, 20.14it/s, loss=0.025]  \n",
      "training 476th epoch: 32it [00:01, 20.21it/s, loss=0.0249] \n",
      "training 477th epoch: 32it [00:01, 19.90it/s, loss=0.0249] \n",
      "training 478th epoch: 32it [00:01, 18.98it/s, loss=0.0248] \n",
      "training 479th epoch: 32it [00:01, 18.55it/s, loss=0.0248] \n",
      "training 480th epoch: 32it [00:01, 18.49it/s, loss=0.0248] \n",
      "training 481th epoch: 32it [00:01, 18.32it/s, loss=0.0247] \n",
      "training 482th epoch: 32it [00:01, 20.61it/s, loss=0.0247] \n",
      "training 483th epoch: 32it [00:01, 20.46it/s, loss=0.0246] \n",
      "training 484th epoch: 32it [00:01, 20.42it/s, loss=0.0246] \n",
      "training 485th epoch: 32it [00:01, 20.20it/s, loss=0.0245] \n",
      "training 486th epoch: 32it [00:01, 20.67it/s, loss=0.0245] \n",
      "training 487th epoch: 32it [00:01, 20.14it/s, loss=0.0245] \n",
      "training 488th epoch: 32it [00:01, 20.41it/s, loss=0.0244] \n",
      "training 489th epoch: 32it [00:01, 20.46it/s, loss=0.0244] \n",
      "training 490th epoch: 32it [00:01, 20.67it/s, loss=0.0243] \n",
      "training 491th epoch: 32it [00:01, 20.61it/s, loss=0.0243] \n",
      "training 492th epoch: 32it [00:01, 20.47it/s, loss=0.0243] \n",
      "training 493th epoch: 32it [00:01, 20.21it/s, loss=0.0242] \n",
      "training 494th epoch: 32it [00:01, 20.68it/s, loss=0.0242] \n",
      "training 495th epoch: 32it [00:01, 20.69it/s, loss=0.0241] \n",
      "training 496th epoch: 32it [00:01, 20.85it/s, loss=0.0241] \n",
      "training 497th epoch: 32it [00:01, 20.40it/s, loss=0.024]  \n",
      "training 498th epoch: 32it [00:01, 19.44it/s, loss=0.024]  \n",
      "training 499th epoch: 32it [00:01, 20.65it/s, loss=0.024]  \n",
      "training 500th epoch: 32it [00:01, 20.26it/s, loss=0.0239] \n",
      "training 501th epoch: 32it [00:01, 20.72it/s, loss=0.0239] \n",
      "training 502th epoch: 32it [00:01, 20.71it/s, loss=0.0239] \n",
      "training 503th epoch: 32it [00:01, 20.16it/s, loss=0.0238] \n",
      "training 504th epoch: 32it [00:01, 20.89it/s, loss=0.0238] \n",
      "training 505th epoch: 32it [00:01, 20.82it/s, loss=0.0237] \n",
      "training 506th epoch: 32it [00:01, 20.28it/s, loss=0.0237] \n",
      "training 507th epoch: 32it [00:01, 20.42it/s, loss=0.0237] \n",
      "training 508th epoch: 32it [00:01, 20.79it/s, loss=0.0236] \n",
      "training 509th epoch: 32it [00:01, 20.49it/s, loss=0.0236] \n",
      "training 510th epoch: 32it [00:01, 20.26it/s, loss=0.0235] \n",
      "training 511th epoch: 32it [00:01, 20.72it/s, loss=0.0235] \n",
      "training 512th epoch: 32it [00:01, 19.40it/s, loss=0.0235] \n",
      "training 513th epoch: 32it [00:01, 18.88it/s, loss=0.0234] \n",
      "training 514th epoch: 32it [00:01, 18.54it/s, loss=0.0234] \n",
      "training 515th epoch: 32it [00:01, 18.89it/s, loss=0.0233] \n",
      "training 516th epoch: 32it [00:01, 18.76it/s, loss=0.0233] \n",
      "training 517th epoch: 32it [00:01, 18.88it/s, loss=0.0233] \n",
      "training 518th epoch: 32it [00:01, 18.84it/s, loss=0.0232] \n",
      "training 519th epoch: 32it [00:01, 18.59it/s, loss=0.0232] \n",
      "training 520th epoch: 32it [00:01, 18.70it/s, loss=0.0232] \n",
      "training 521th epoch: 32it [00:01, 18.74it/s, loss=0.0231] \n",
      "training 522th epoch: 32it [00:01, 18.41it/s, loss=0.0231] \n",
      "training 523th epoch: 32it [00:01, 18.77it/s, loss=0.023]  \n",
      "training 524th epoch: 32it [00:01, 18.75it/s, loss=0.023]  \n",
      "training 525th epoch: 32it [00:01, 18.80it/s, loss=0.023]  \n",
      "training 526th epoch: 32it [00:01, 18.52it/s, loss=0.0229] \n",
      "training 527th epoch: 32it [00:01, 18.55it/s, loss=0.0229] \n",
      "training 528th epoch: 32it [00:01, 18.45it/s, loss=0.0229] \n",
      "training 529th epoch: 32it [00:01, 18.55it/s, loss=0.0228] \n",
      "training 530th epoch: 32it [00:01, 18.48it/s, loss=0.0228] \n",
      "training 531th epoch: 32it [00:01, 18.50it/s, loss=0.0228] \n",
      "training 532th epoch: 32it [00:01, 19.09it/s, loss=0.0227] \n",
      "training 533th epoch: 32it [00:01, 20.18it/s, loss=0.0227] \n",
      "training 534th epoch: 32it [00:01, 19.98it/s, loss=0.0226] \n",
      "training 535th epoch: 32it [00:01, 18.59it/s, loss=0.0226] \n",
      "training 536th epoch: 32it [00:01, 17.61it/s, loss=0.0226] \n",
      "training 537th epoch: 32it [00:01, 18.64it/s, loss=0.0225] \n",
      "training 538th epoch: 32it [00:01, 19.86it/s, loss=0.0225] \n",
      "training 539th epoch: 32it [00:01, 18.28it/s, loss=0.0225] \n",
      "training 540th epoch: 32it [00:01, 18.10it/s, loss=0.0224] \n",
      "training 541th epoch: 32it [00:01, 18.31it/s, loss=0.0224] \n",
      "training 542th epoch: 32it [00:01, 18.28it/s, loss=0.0224] \n",
      "training 543th epoch: 32it [00:01, 19.20it/s, loss=0.0223] \n",
      "training 544th epoch: 32it [00:01, 20.22it/s, loss=0.0223] \n",
      "training 545th epoch: 32it [00:01, 20.53it/s, loss=0.0223] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 546th epoch: 32it [00:01, 20.67it/s, loss=0.0222] \n",
      "training 547th epoch: 32it [00:01, 20.41it/s, loss=0.0222] \n",
      "training 548th epoch: 32it [00:01, 19.95it/s, loss=0.0222] \n",
      "training 549th epoch: 32it [00:01, 20.27it/s, loss=0.0221] \n",
      "training 550th epoch: 32it [00:01, 20.12it/s, loss=0.0221] \n",
      "training 551th epoch: 32it [00:01, 20.41it/s, loss=0.0221] \n",
      "training 552th epoch: 32it [00:01, 20.33it/s, loss=0.022]  \n",
      "training 553th epoch: 32it [00:01, 20.41it/s, loss=0.022]  \n",
      "training 554th epoch: 32it [00:01, 20.54it/s, loss=0.022]  \n",
      "training 555th epoch: 32it [00:01, 20.21it/s, loss=0.0219] \n",
      "training 556th epoch: 32it [00:01, 20.46it/s, loss=0.0219] \n",
      "training 557th epoch: 32it [00:01, 20.33it/s, loss=0.0218] \n",
      "training 558th epoch: 32it [00:01, 20.66it/s, loss=0.0218] \n",
      "training 559th epoch: 32it [00:01, 20.71it/s, loss=0.0218] \n",
      "training 560th epoch: 32it [00:01, 20.32it/s, loss=0.0217] \n",
      "training 561th epoch: 32it [00:01, 20.55it/s, loss=0.0217] \n",
      "training 562th epoch: 32it [00:01, 20.44it/s, loss=0.0217] \n",
      "training 563th epoch: 32it [00:01, 20.22it/s, loss=0.0217] \n",
      "training 564th epoch: 32it [00:01, 20.26it/s, loss=0.0216] \n",
      "training 565th epoch: 32it [00:01, 20.28it/s, loss=0.0216] \n",
      "training 566th epoch: 32it [00:01, 20.54it/s, loss=0.0216] \n",
      "training 567th epoch: 32it [00:01, 20.59it/s, loss=0.0215] \n",
      "training 568th epoch: 32it [00:01, 20.37it/s, loss=0.0215] \n",
      "training 569th epoch: 32it [00:01, 20.27it/s, loss=0.0215] \n",
      "training 570th epoch: 32it [00:01, 19.92it/s, loss=0.0214] \n",
      "training 571th epoch: 32it [00:01, 20.82it/s, loss=0.0214] \n",
      "training 572th epoch: 32it [00:01, 21.33it/s, loss=0.0214] \n",
      "training 573th epoch: 32it [00:01, 23.31it/s, loss=0.0213] \n",
      "training 574th epoch: 32it [00:01, 22.12it/s, loss=0.0213] \n",
      "training 575th epoch: 32it [00:01, 20.41it/s, loss=0.0213] \n",
      "training 576th epoch: 32it [00:01, 24.23it/s, loss=0.0212] \n",
      "training 577th epoch: 32it [00:01, 23.00it/s, loss=0.0212] \n",
      "training 578th epoch: 32it [00:01, 20.70it/s, loss=0.0212] \n",
      "training 579th epoch: 32it [00:01, 22.33it/s, loss=0.0211] \n",
      "training 580th epoch: 32it [00:01, 20.71it/s, loss=0.0211] \n",
      "training 581th epoch: 32it [00:01, 20.72it/s, loss=0.0211] \n",
      "training 582th epoch: 32it [00:01, 20.68it/s, loss=0.021]  \n",
      "training 583th epoch: 32it [00:01, 20.47it/s, loss=0.021]  \n",
      "training 584th epoch: 32it [00:01, 20.50it/s, loss=0.021]  \n",
      "training 585th epoch: 32it [00:01, 20.58it/s, loss=0.0209] \n",
      "training 586th epoch: 32it [00:01, 20.34it/s, loss=0.0209] \n",
      "training 587th epoch: 32it [00:01, 20.60it/s, loss=0.0209] \n",
      "training 588th epoch: 32it [00:01, 20.49it/s, loss=0.0209] \n",
      "training 589th epoch: 32it [00:01, 20.67it/s, loss=0.0208] \n",
      "training 590th epoch: 32it [00:01, 20.65it/s, loss=0.0208] \n",
      "training 591th epoch: 32it [00:01, 20.47it/s, loss=0.0208] \n",
      "training 592th epoch: 32it [00:01, 20.47it/s, loss=0.0207] \n",
      "training 593th epoch: 32it [00:01, 20.69it/s, loss=0.0207] \n",
      "training 594th epoch: 32it [00:01, 20.73it/s, loss=0.0207] \n",
      "training 595th epoch: 32it [00:01, 19.29it/s, loss=0.0206] \n",
      "training 596th epoch: 32it [00:01, 19.79it/s, loss=0.0206] \n",
      "training 597th epoch: 32it [00:01, 20.44it/s, loss=0.0206] \n",
      "training 598th epoch: 32it [00:01, 20.62it/s, loss=0.0206] \n",
      "training 599th epoch: 32it [00:01, 23.39it/s, loss=0.0205] \n",
      "training 600th epoch: 32it [00:01, 24.18it/s, loss=0.0205] \n",
      "training 601th epoch: 32it [00:01, 24.17it/s, loss=0.0205] \n",
      "training 602th epoch: 32it [00:01, 24.41it/s, loss=0.0204] \n",
      "training 603th epoch: 32it [00:01, 24.16it/s, loss=0.0204] \n",
      "training 604th epoch: 32it [00:01, 22.66it/s, loss=0.0204] \n",
      "training 605th epoch: 32it [00:01, 20.37it/s, loss=0.0203] \n",
      "training 606th epoch: 32it [00:01, 22.04it/s, loss=0.0203] \n",
      "training 607th epoch: 32it [00:01, 20.59it/s, loss=0.0203] \n",
      "training 608th epoch: 32it [00:01, 20.11it/s, loss=0.0203] \n",
      "training 609th epoch: 32it [00:01, 20.54it/s, loss=0.0202] \n",
      "training 610th epoch: 32it [00:01, 21.36it/s, loss=0.0202] \n",
      "training 611th epoch: 32it [00:01, 20.68it/s, loss=0.0202] \n",
      "training 612th epoch: 32it [00:01, 20.67it/s, loss=0.0201] \n",
      "training 613th epoch: 32it [00:01, 20.39it/s, loss=0.0201] \n",
      "training 614th epoch: 32it [00:01, 20.67it/s, loss=0.0201] \n",
      "training 615th epoch: 32it [00:01, 20.47it/s, loss=0.0201] \n",
      "training 616th epoch: 32it [00:01, 20.44it/s, loss=0.02]   \n",
      "training 617th epoch: 32it [00:01, 20.62it/s, loss=0.02]   \n",
      "training 618th epoch: 32it [00:01, 20.80it/s, loss=0.02]   \n",
      "training 619th epoch: 32it [00:01, 20.41it/s, loss=0.0199] \n",
      "training 620th epoch: 32it [00:01, 20.44it/s, loss=0.0199] \n",
      "training 621th epoch: 32it [00:01, 23.37it/s, loss=0.0199] \n",
      "training 622th epoch: 32it [00:01, 24.17it/s, loss=0.0199] \n",
      "training 623th epoch: 32it [00:01, 24.10it/s, loss=0.0198] \n",
      "training 624th epoch: 32it [00:01, 24.28it/s, loss=0.0198] \n",
      "training 625th epoch: 32it [00:01, 24.23it/s, loss=0.0198] \n",
      "training 626th epoch: 32it [00:01, 24.02it/s, loss=0.0197] \n",
      "training 627th epoch: 32it [00:01, 24.57it/s, loss=0.0197] \n",
      "training 628th epoch: 32it [00:01, 23.14it/s, loss=0.0197] \n",
      "training 629th epoch: 32it [00:01, 23.92it/s, loss=0.0197] \n",
      "training 630th epoch: 32it [00:01, 23.21it/s, loss=0.0196] \n",
      "training 631th epoch: 32it [00:01, 20.31it/s, loss=0.0196] \n",
      "training 632th epoch: 32it [00:01, 20.38it/s, loss=0.0196] \n",
      "training 633th epoch: 32it [00:01, 20.54it/s, loss=0.0196] \n",
      "training 634th epoch: 32it [00:01, 20.89it/s, loss=0.0195] \n",
      "training 635th epoch: 32it [00:01, 19.78it/s, loss=0.0195] \n",
      "training 636th epoch: 32it [00:01, 19.58it/s, loss=0.0195] \n",
      "training 637th epoch: 32it [00:01, 19.76it/s, loss=0.0194] \n",
      "training 638th epoch: 32it [00:01, 19.45it/s, loss=0.0194] \n",
      "training 639th epoch: 32it [00:01, 19.34it/s, loss=0.0194] \n",
      "training 640th epoch: 32it [00:01, 19.30it/s, loss=0.0194] \n",
      "training 641th epoch: 32it [00:01, 20.24it/s, loss=0.0193] \n",
      "training 642th epoch: 32it [00:01, 20.47it/s, loss=0.0193] \n",
      "training 643th epoch: 32it [00:01, 20.65it/s, loss=0.0193] \n",
      "training 644th epoch: 32it [00:01, 20.53it/s, loss=0.0193] \n",
      "training 645th epoch: 32it [00:01, 19.71it/s, loss=0.0192] \n",
      "training 646th epoch: 32it [00:01, 18.27it/s, loss=0.0192] \n",
      "training 647th epoch: 32it [00:01, 18.71it/s, loss=0.0192] \n",
      "training 648th epoch: 32it [00:01, 18.40it/s, loss=0.0192] \n",
      "training 649th epoch: 32it [00:01, 18.26it/s, loss=0.0191] \n",
      "training 650th epoch: 32it [00:01, 18.58it/s, loss=0.0191] \n",
      "training 651th epoch: 32it [00:01, 20.52it/s, loss=0.0191] \n",
      "training 652th epoch: 32it [00:01, 20.53it/s, loss=0.0191] \n",
      "training 653th epoch: 32it [00:01, 20.77it/s, loss=0.019]  \n",
      "training 654th epoch: 32it [00:01, 20.37it/s, loss=0.019]  \n",
      "training 655th epoch: 32it [00:01, 20.84it/s, loss=0.019]  \n",
      "training 656th epoch: 32it [00:01, 20.86it/s, loss=0.0189] \n",
      "training 657th epoch: 32it [00:01, 20.44it/s, loss=0.0189] \n",
      "training 658th epoch: 32it [00:01, 20.64it/s, loss=0.0189] \n",
      "training 659th epoch: 32it [00:01, 20.12it/s, loss=0.0189] \n",
      "training 660th epoch: 32it [00:01, 19.04it/s, loss=0.0188] \n",
      "training 661th epoch: 32it [00:01, 20.10it/s, loss=0.0188] \n",
      "training 662th epoch: 32it [00:01, 20.75it/s, loss=0.0188] \n",
      "training 663th epoch: 32it [00:01, 20.80it/s, loss=0.0188] \n",
      "training 664th epoch: 32it [00:01, 20.16it/s, loss=0.0187] \n",
      "training 665th epoch: 32it [00:01, 19.57it/s, loss=0.0187] \n",
      "training 666th epoch: 32it [00:01, 19.83it/s, loss=0.0187] \n",
      "training 667th epoch: 32it [00:01, 20.42it/s, loss=0.0187] \n",
      "training 668th epoch: 32it [00:01, 20.69it/s, loss=0.0186] \n",
      "training 669th epoch: 32it [00:01, 19.33it/s, loss=0.0186] \n",
      "training 670th epoch: 32it [00:01, 18.53it/s, loss=0.0186] \n",
      "training 671th epoch: 32it [00:01, 18.62it/s, loss=0.0186] \n",
      "training 672th epoch: 32it [00:01, 18.81it/s, loss=0.0185] \n",
      "training 673th epoch: 32it [00:01, 18.93it/s, loss=0.0185] \n",
      "training 674th epoch: 32it [00:01, 18.50it/s, loss=0.0185] \n",
      "training 675th epoch: 32it [00:01, 18.60it/s, loss=0.0185] \n",
      "training 676th epoch: 32it [00:01, 18.60it/s, loss=0.0184] \n",
      "training 677th epoch: 32it [00:01, 18.46it/s, loss=0.0184] \n",
      "training 678th epoch: 32it [00:01, 18.84it/s, loss=0.0184] \n",
      "training 679th epoch: 32it [00:01, 18.94it/s, loss=0.0184] \n",
      "training 680th epoch: 32it [00:01, 18.63it/s, loss=0.0184] \n",
      "training 681th epoch: 32it [00:01, 18.71it/s, loss=0.0183] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 682th epoch: 32it [00:01, 18.90it/s, loss=0.0183] \n",
      "training 683th epoch: 32it [00:01, 18.81it/s, loss=0.0183] \n",
      "training 684th epoch: 32it [00:01, 18.57it/s, loss=0.0183] \n",
      "training 685th epoch: 32it [00:01, 18.69it/s, loss=0.0182] \n",
      "training 686th epoch: 32it [00:01, 18.73it/s, loss=0.0182] \n",
      "training 687th epoch: 32it [00:01, 18.80it/s, loss=0.0182] \n",
      "training 688th epoch: 32it [00:01, 18.65it/s, loss=0.0182] \n",
      "training 689th epoch: 32it [00:01, 18.72it/s, loss=0.0181] \n",
      "training 690th epoch: 32it [00:01, 18.58it/s, loss=0.0181] \n",
      "training 691th epoch: 32it [00:01, 18.68it/s, loss=0.0181] \n",
      "training 692th epoch: 32it [00:01, 18.76it/s, loss=0.0181] \n",
      "training 693th epoch: 32it [00:01, 18.69it/s, loss=0.018]  \n",
      "training 694th epoch: 32it [00:01, 18.41it/s, loss=0.018]  \n",
      "training 695th epoch: 32it [00:01, 18.59it/s, loss=0.018]  \n",
      "training 696th epoch: 32it [00:01, 18.60it/s, loss=0.018]  \n",
      "training 697th epoch: 32it [00:01, 18.68it/s, loss=0.0179] \n",
      "training 698th epoch: 32it [00:01, 18.39it/s, loss=0.0179] \n",
      "training 699th epoch: 32it [00:01, 18.90it/s, loss=0.0179] \n",
      "training 700th epoch: 32it [00:01, 18.59it/s, loss=0.0179] \n",
      "training 701th epoch: 32it [00:01, 18.98it/s, loss=0.0179] \n",
      "training 702th epoch: 32it [00:01, 18.70it/s, loss=0.0178] \n",
      "training 703th epoch: 32it [00:01, 18.41it/s, loss=0.0178] \n",
      "training 704th epoch: 32it [00:01, 18.70it/s, loss=0.0178] \n",
      "training 705th epoch: 32it [00:01, 18.65it/s, loss=0.0178] \n",
      "training 706th epoch: 32it [00:01, 18.44it/s, loss=0.0177] \n",
      "training 707th epoch: 32it [00:01, 18.42it/s, loss=0.0177] \n",
      "training 708th epoch: 32it [00:01, 18.99it/s, loss=0.0177] \n",
      "training 709th epoch: 32it [00:01, 20.74it/s, loss=0.0177] \n",
      "training 710th epoch: 32it [00:01, 20.58it/s, loss=0.0177] \n",
      "training 711th epoch: 32it [00:01, 20.86it/s, loss=0.0176] \n",
      "training 712th epoch: 32it [00:01, 20.84it/s, loss=0.0176] \n",
      "training 713th epoch: 32it [00:01, 20.41it/s, loss=0.0176] \n",
      "training 714th epoch: 32it [00:01, 20.38it/s, loss=0.0176] \n",
      "training 715th epoch: 32it [00:01, 20.32it/s, loss=0.0175] \n",
      "training 716th epoch: 32it [00:01, 20.33it/s, loss=0.0175] \n",
      "training 717th epoch: 32it [00:01, 20.08it/s, loss=0.0175] \n",
      "training 718th epoch: 32it [00:01, 20.21it/s, loss=0.0175] \n",
      "training 719th epoch: 32it [00:01, 20.12it/s, loss=0.0175] \n",
      "training 720th epoch: 32it [00:01, 20.25it/s, loss=0.0174] \n",
      "training 721th epoch: 32it [00:01, 20.31it/s, loss=0.0174] \n",
      "training 722th epoch: 32it [00:01, 19.88it/s, loss=0.0174] \n",
      "training 723th epoch: 32it [00:01, 20.52it/s, loss=0.0174] \n",
      "training 724th epoch: 32it [00:01, 21.46it/s, loss=0.0173] \n",
      "training 725th epoch: 32it [00:01, 21.73it/s, loss=0.0173] \n",
      "training 726th epoch: 32it [00:01, 20.39it/s, loss=0.0173] \n",
      "training 727th epoch: 32it [00:01, 18.77it/s, loss=0.0173] \n",
      "training 728th epoch: 32it [00:01, 18.76it/s, loss=0.0173] \n",
      "training 729th epoch: 32it [00:01, 18.53it/s, loss=0.0172] \n",
      "training 730th epoch: 32it [00:01, 18.62it/s, loss=0.0172] \n",
      "training 731th epoch: 32it [00:01, 18.56it/s, loss=0.0172] \n",
      "training 732th epoch: 32it [00:01, 18.77it/s, loss=0.0172] \n",
      "training 733th epoch: 32it [00:01, 18.55it/s, loss=0.0172] \n",
      "training 734th epoch: 32it [00:01, 18.62it/s, loss=0.0171] \n",
      "training 735th epoch: 32it [00:01, 18.54it/s, loss=0.0171] \n",
      "training 736th epoch: 32it [00:01, 18.64it/s, loss=0.0171] \n",
      "training 737th epoch: 32it [00:01, 18.66it/s, loss=0.0171] \n",
      "training 738th epoch: 32it [00:01, 18.56it/s, loss=0.017]  \n",
      "training 739th epoch: 32it [00:01, 18.62it/s, loss=0.017]  \n",
      "training 740th epoch: 32it [00:01, 18.68it/s, loss=0.017]  \n",
      "training 741th epoch: 32it [00:01, 18.83it/s, loss=0.017]  \n",
      "training 742th epoch: 32it [00:01, 18.86it/s, loss=0.017]  \n",
      "training 743th epoch: 32it [00:01, 18.75it/s, loss=0.0169] \n",
      "training 744th epoch: 32it [00:01, 18.35it/s, loss=0.0169] \n",
      "training 745th epoch: 32it [00:01, 18.76it/s, loss=0.0169] \n",
      "training 746th epoch: 32it [00:01, 18.75it/s, loss=0.0169] \n",
      "training 747th epoch: 32it [00:01, 18.47it/s, loss=0.0169] \n",
      "training 748th epoch: 32it [00:01, 18.66it/s, loss=0.0168] \n",
      "training 749th epoch: 32it [00:01, 18.68it/s, loss=0.0168] \n",
      "training 750th epoch: 32it [00:01, 18.83it/s, loss=0.0168] \n",
      "training 751th epoch: 32it [00:01, 18.82it/s, loss=0.0168] \n",
      "training 752th epoch: 32it [00:01, 20.65it/s, loss=0.0168] \n",
      "training 753th epoch: 32it [00:01, 20.64it/s, loss=0.0167] \n",
      "training 754th epoch: 32it [00:01, 19.81it/s, loss=0.0167] \n",
      "training 755th epoch: 32it [00:01, 22.92it/s, loss=0.0167] \n",
      "training 756th epoch: 32it [00:01, 24.19it/s, loss=0.0167] \n",
      "training 757th epoch: 32it [00:01, 24.08it/s, loss=0.0167] \n",
      "training 758th epoch: 32it [00:01, 24.36it/s, loss=0.0166] \n",
      "training 759th epoch: 32it [00:01, 24.39it/s, loss=0.0166] \n",
      "training 760th epoch: 32it [00:01, 24.47it/s, loss=0.0166] \n",
      "training 761th epoch: 32it [00:01, 24.45it/s, loss=0.0166] \n",
      "training 762th epoch: 32it [00:01, 22.07it/s, loss=0.0166] \n",
      "training 763th epoch: 32it [00:01, 20.67it/s, loss=0.0165] \n",
      "training 764th epoch: 32it [00:01, 20.88it/s, loss=0.0165] \n",
      "training 765th epoch: 32it [00:01, 20.51it/s, loss=0.0165] \n",
      "training 766th epoch: 32it [00:01, 20.69it/s, loss=0.0165] \n",
      "training 767th epoch: 32it [00:01, 20.73it/s, loss=0.0165] \n",
      "training 768th epoch: 32it [00:01, 19.94it/s, loss=0.0164] \n",
      "training 769th epoch: 32it [00:01, 19.41it/s, loss=0.0164] \n",
      "training 770th epoch: 32it [00:01, 19.23it/s, loss=0.0164] \n",
      "training 771th epoch: 32it [00:01, 19.17it/s, loss=0.0164] \n",
      "training 772th epoch: 32it [00:01, 19.49it/s, loss=0.0164] \n",
      "training 773th epoch: 32it [00:01, 19.58it/s, loss=0.0163] \n",
      "training 774th epoch: 32it [00:01, 19.70it/s, loss=0.0163] \n",
      "training 775th epoch: 32it [00:01, 19.28it/s, loss=0.0163] \n",
      "training 776th epoch: 32it [00:01, 20.40it/s, loss=0.0163] \n",
      "training 777th epoch: 32it [00:01, 20.50it/s, loss=0.0163] \n",
      "training 778th epoch: 32it [00:01, 20.65it/s, loss=0.0162] \n",
      "training 779th epoch: 32it [00:01, 20.59it/s, loss=0.0162] \n",
      "training 780th epoch: 32it [00:01, 20.69it/s, loss=0.0162] \n",
      "training 781th epoch: 32it [00:01, 20.43it/s, loss=0.0162] \n",
      "training 782th epoch: 32it [00:01, 20.57it/s, loss=0.0162] \n",
      "training 783th epoch: 32it [00:01, 20.74it/s, loss=0.0161] \n",
      "training 784th epoch: 32it [00:01, 20.88it/s, loss=0.0161] \n",
      "training 785th epoch: 32it [00:01, 20.82it/s, loss=0.0161] \n",
      "training 786th epoch: 32it [00:01, 20.68it/s, loss=0.0161] \n",
      "training 787th epoch: 32it [00:01, 20.71it/s, loss=0.0161] \n",
      "training 788th epoch: 32it [00:01, 20.75it/s, loss=0.0161] \n",
      "training 789th epoch: 32it [00:01, 20.87it/s, loss=0.016]  \n",
      "training 790th epoch: 32it [00:01, 20.56it/s, loss=0.016]  \n",
      "training 791th epoch: 32it [00:01, 20.67it/s, loss=0.016]  \n",
      "training 792th epoch: 32it [00:01, 20.76it/s, loss=0.016]  \n",
      "training 793th epoch: 32it [00:01, 20.88it/s, loss=0.016]  \n",
      "training 794th epoch: 32it [00:01, 20.18it/s, loss=0.0159] \n",
      "training 795th epoch: 32it [00:01, 18.35it/s, loss=0.0159] \n",
      "training 796th epoch: 32it [00:01, 18.03it/s, loss=0.0159] \n",
      "training 797th epoch: 32it [00:01, 18.20it/s, loss=0.0159] \n",
      "training 798th epoch: 32it [00:01, 18.39it/s, loss=0.0159] \n",
      "training 799th epoch: 32it [00:01, 18.71it/s, loss=0.0158] \n",
      "training 800th epoch: 32it [00:01, 18.60it/s, loss=0.0158] \n",
      "training 801th epoch: 32it [00:01, 19.10it/s, loss=0.0158] \n",
      "training 802th epoch: 32it [00:01, 20.51it/s, loss=0.0158] \n",
      "training 803th epoch: 32it [00:01, 20.59it/s, loss=0.0158] \n",
      "training 804th epoch: 32it [00:01, 20.10it/s, loss=0.0158] \n",
      "training 805th epoch: 32it [00:01, 20.72it/s, loss=0.0157] \n",
      "training 806th epoch: 32it [00:01, 20.86it/s, loss=0.0157] \n",
      "training 807th epoch: 32it [00:01, 20.75it/s, loss=0.0157] \n",
      "training 808th epoch: 32it [00:01, 20.38it/s, loss=0.0157] \n",
      "training 809th epoch: 32it [00:01, 19.51it/s, loss=0.0157] \n",
      "training 810th epoch: 32it [00:01, 20.06it/s, loss=0.0156] \n",
      "training 811th epoch: 32it [00:01, 21.52it/s, loss=0.0156] \n",
      "training 812th epoch: 32it [00:01, 21.29it/s, loss=0.0156] \n",
      "training 813th epoch: 32it [00:01, 20.82it/s, loss=0.0156] \n",
      "training 814th epoch: 32it [00:01, 20.61it/s, loss=0.0156] \n",
      "training 815th epoch: 32it [00:01, 20.72it/s, loss=0.0156] \n",
      "training 816th epoch: 32it [00:01, 20.90it/s, loss=0.0155] \n",
      "training 817th epoch: 32it [00:01, 20.67it/s, loss=0.0155] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 818th epoch: 32it [00:01, 18.22it/s, loss=0.0155] \n",
      "training 819th epoch: 32it [00:01, 17.40it/s, loss=0.0155] \n",
      "training 820th epoch: 32it [00:01, 17.88it/s, loss=0.0155] \n",
      "training 821th epoch: 32it [00:01, 18.07it/s, loss=0.0155] \n",
      "training 822th epoch: 32it [00:01, 18.06it/s, loss=0.0154] \n",
      "training 823th epoch: 32it [00:01, 17.81it/s, loss=0.0154] \n",
      "training 824th epoch: 32it [00:01, 19.53it/s, loss=0.0154] \n",
      "training 825th epoch: 32it [00:01, 20.30it/s, loss=0.0154] \n",
      "training 826th epoch: 32it [00:01, 20.52it/s, loss=0.0154] \n",
      "training 827th epoch: 32it [00:01, 20.23it/s, loss=0.0154] \n",
      "training 828th epoch: 32it [00:01, 20.18it/s, loss=0.0153] \n",
      "training 829th epoch: 32it [00:01, 20.14it/s, loss=0.0153] \n",
      "training 830th epoch: 32it [00:01, 20.37it/s, loss=0.0153] \n",
      "training 831th epoch: 32it [00:01, 20.72it/s, loss=0.0153] \n",
      "training 832th epoch: 32it [00:01, 21.09it/s, loss=0.0153] \n",
      "training 833th epoch: 32it [00:01, 20.79it/s, loss=0.0152] \n",
      "training 834th epoch: 32it [00:01, 20.91it/s, loss=0.0152] \n",
      "training 835th epoch: 32it [00:01, 20.41it/s, loss=0.0152] \n",
      "training 836th epoch: 32it [00:01, 20.37it/s, loss=0.0152] \n",
      "training 837th epoch: 32it [00:01, 20.57it/s, loss=0.0152] \n",
      "training 838th epoch: 32it [00:01, 20.69it/s, loss=0.0152] \n",
      "training 839th epoch: 32it [00:01, 20.82it/s, loss=0.0151] \n",
      "training 840th epoch: 32it [00:01, 20.36it/s, loss=0.0151] \n",
      "training 841th epoch: 32it [00:01, 18.95it/s, loss=0.0151] \n",
      "training 842th epoch: 32it [00:01, 20.54it/s, loss=0.0151] \n",
      "training 843th epoch: 32it [00:01, 20.25it/s, loss=0.0151] \n",
      "training 844th epoch: 32it [00:01, 20.36it/s, loss=0.0151] \n",
      "training 845th epoch: 32it [00:01, 18.31it/s, loss=0.015]  \n",
      "training 846th epoch: 32it [00:01, 18.10it/s, loss=0.015]  \n",
      "training 847th epoch: 32it [00:01, 18.15it/s, loss=0.015]  \n",
      "training 848th epoch: 32it [00:01, 18.76it/s, loss=0.015]  \n",
      "training 849th epoch: 32it [00:01, 17.90it/s, loss=0.015]  \n",
      "training 850th epoch: 32it [00:01, 18.17it/s, loss=0.015]  \n",
      "training 851th epoch: 32it [00:01, 18.29it/s, loss=0.0149] \n",
      "training 852th epoch: 32it [00:01, 18.58it/s, loss=0.0149] \n",
      "training 853th epoch: 32it [00:01, 17.97it/s, loss=0.0149] \n",
      "training 854th epoch: 32it [00:01, 18.33it/s, loss=0.0149] \n",
      "training 855th epoch: 32it [00:01, 17.83it/s, loss=0.0149] \n",
      "training 856th epoch: 32it [00:01, 17.99it/s, loss=0.0149] \n",
      "training 857th epoch: 32it [00:01, 18.10it/s, loss=0.0148] \n",
      "training 858th epoch: 32it [00:01, 18.27it/s, loss=0.0148] \n",
      "training 859th epoch: 32it [00:01, 17.95it/s, loss=0.0148] \n",
      "training 860th epoch: 32it [00:01, 18.53it/s, loss=0.0148] \n",
      "training 861th epoch: 32it [00:01, 17.88it/s, loss=0.0148] \n",
      "training 862th epoch: 32it [00:01, 18.06it/s, loss=0.0148] \n",
      "training 863th epoch: 32it [00:01, 18.22it/s, loss=0.0148] \n",
      "training 864th epoch: 32it [00:01, 17.59it/s, loss=0.0147] \n",
      "training 865th epoch: 32it [00:01, 17.99it/s, loss=0.0147] \n",
      "training 866th epoch: 32it [00:01, 18.18it/s, loss=0.0147] \n",
      "training 867th epoch: 32it [00:01, 19.75it/s, loss=0.0147] \n",
      "training 868th epoch: 32it [00:01, 20.82it/s, loss=0.0147] \n",
      "training 869th epoch: 32it [00:01, 20.66it/s, loss=0.0147] \n",
      "training 870th epoch: 32it [00:01, 20.59it/s, loss=0.0146] \n",
      "training 871th epoch: 32it [00:01, 20.66it/s, loss=0.0146] \n",
      "training 872th epoch: 32it [00:01, 20.65it/s, loss=0.0146] \n",
      "training 873th epoch: 32it [00:01, 20.55it/s, loss=0.0146] \n",
      "training 874th epoch: 32it [00:01, 20.57it/s, loss=0.0146] \n",
      "training 875th epoch: 32it [00:01, 20.25it/s, loss=0.0146] \n",
      "training 876th epoch: 32it [00:01, 20.72it/s, loss=0.0145] \n",
      "training 877th epoch: 32it [00:01, 20.62it/s, loss=0.0145] \n",
      "training 878th epoch: 32it [00:01, 20.71it/s, loss=0.0145] \n",
      "training 879th epoch: 32it [00:01, 20.86it/s, loss=0.0145] \n",
      "training 880th epoch: 32it [00:01, 20.45it/s, loss=0.0145] \n",
      "training 881th epoch: 32it [00:01, 20.86it/s, loss=0.0145] \n",
      "training 882th epoch: 32it [00:01, 20.61it/s, loss=0.0145] \n",
      "training 883th epoch: 32it [00:01, 20.65it/s, loss=0.0144] \n",
      "training 884th epoch: 32it [00:01, 20.75it/s, loss=0.0144] \n",
      "training 885th epoch: 32it [00:01, 20.65it/s, loss=0.0144] \n",
      "training 886th epoch: 32it [00:01, 20.72it/s, loss=0.0144] \n",
      "training 887th epoch: 32it [00:01, 22.88it/s, loss=0.0144] \n",
      "training 888th epoch: 32it [00:01, 24.35it/s, loss=0.0144] \n",
      "training 889th epoch: 32it [00:01, 24.56it/s, loss=0.0143] \n",
      "training 890th epoch: 32it [00:01, 24.14it/s, loss=0.0143] \n",
      "training 891th epoch: 32it [00:01, 23.39it/s, loss=0.0143] \n",
      "training 892th epoch: 32it [00:01, 20.57it/s, loss=0.0143] \n",
      "training 893th epoch: 32it [00:01, 20.45it/s, loss=0.0143] \n",
      "training 894th epoch: 32it [00:01, 20.68it/s, loss=0.0143] \n",
      "training 895th epoch: 32it [00:01, 20.32it/s, loss=0.0143] \n",
      "training 896th epoch: 32it [00:01, 20.68it/s, loss=0.0142] \n",
      "training 897th epoch: 32it [00:01, 20.64it/s, loss=0.0142] \n",
      "training 898th epoch: 32it [00:01, 20.43it/s, loss=0.0142] \n",
      "training 899th epoch: 32it [00:01, 20.75it/s, loss=0.0142] \n",
      "training 900th epoch: 32it [00:01, 20.82it/s, loss=0.0142] \n",
      "training 901th epoch: 32it [00:01, 20.55it/s, loss=0.0142] \n",
      "training 902th epoch: 32it [00:01, 20.25it/s, loss=0.0142] \n",
      "training 903th epoch: 32it [00:01, 20.55it/s, loss=0.0141] \n",
      "training 904th epoch: 32it [00:01, 20.24it/s, loss=0.0141] \n",
      "training 905th epoch: 32it [00:01, 20.40it/s, loss=0.0141] \n",
      "training 906th epoch: 32it [00:01, 20.13it/s, loss=0.0141] \n",
      "training 907th epoch: 32it [00:01, 20.17it/s, loss=0.0141] \n",
      "training 908th epoch: 32it [00:01, 20.66it/s, loss=0.0141] \n",
      "training 909th epoch: 32it [00:01, 20.61it/s, loss=0.0141] \n",
      "training 910th epoch: 32it [00:01, 20.61it/s, loss=0.014]  \n",
      "training 911th epoch: 32it [00:01, 20.31it/s, loss=0.014]  \n",
      "training 912th epoch: 32it [00:01, 20.70it/s, loss=0.014]  \n",
      "training 913th epoch: 32it [00:01, 20.62it/s, loss=0.014]  \n",
      "training 914th epoch: 32it [00:01, 20.38it/s, loss=0.014]  \n",
      "training 915th epoch: 32it [00:01, 20.49it/s, loss=0.014]  \n",
      "training 916th epoch: 32it [00:01, 20.76it/s, loss=0.0139] \n",
      "training 917th epoch: 32it [00:01, 20.81it/s, loss=0.0139] \n",
      "training 918th epoch: 32it [00:01, 20.84it/s, loss=0.0139] \n",
      "training 919th epoch: 32it [00:01, 20.73it/s, loss=0.0139] \n",
      "training 920th epoch: 32it [00:01, 20.74it/s, loss=0.0139] \n",
      "training 921th epoch: 32it [00:01, 20.49it/s, loss=0.0139] \n",
      "training 922th epoch: 32it [00:01, 20.72it/s, loss=0.0139] \n",
      "training 923th epoch: 32it [00:01, 20.63it/s, loss=0.0138] \n",
      "training 924th epoch: 32it [00:01, 20.39it/s, loss=0.0138] \n",
      "training 925th epoch: 32it [00:01, 20.09it/s, loss=0.0138] \n",
      "training 926th epoch: 32it [00:01, 18.63it/s, loss=0.0138] \n",
      "training 927th epoch: 32it [00:01, 18.14it/s, loss=0.0138] \n",
      "training 928th epoch: 32it [00:01, 18.45it/s, loss=0.0138] \n",
      "training 929th epoch: 32it [00:01, 18.56it/s, loss=0.0138] \n",
      "training 930th epoch: 32it [00:01, 18.19it/s, loss=0.0138] \n",
      "training 931th epoch: 32it [00:01, 18.86it/s, loss=0.0137] \n",
      "training 932th epoch: 32it [00:01, 19.29it/s, loss=0.0137] \n",
      "training 933th epoch: 32it [00:01, 18.87it/s, loss=0.0137] \n",
      "training 934th epoch: 32it [00:01, 18.92it/s, loss=0.0137] \n",
      "training 935th epoch: 32it [00:01, 18.74it/s, loss=0.0137] \n",
      "training 936th epoch: 32it [00:01, 19.14it/s, loss=0.0137] \n",
      "training 937th epoch: 32it [00:01, 18.50it/s, loss=0.0137] \n",
      "training 938th epoch: 32it [00:01, 18.47it/s, loss=0.0136] \n",
      "training 939th epoch: 32it [00:01, 18.61it/s, loss=0.0136] \n",
      "training 940th epoch: 32it [00:01, 18.55it/s, loss=0.0136] \n",
      "training 941th epoch: 32it [00:01, 18.64it/s, loss=0.0136] \n",
      "training 942th epoch: 32it [00:01, 19.16it/s, loss=0.0136] \n",
      "training 943th epoch: 32it [00:01, 18.63it/s, loss=0.0136] \n",
      "training 944th epoch: 32it [00:01, 18.89it/s, loss=0.0136] \n",
      "training 945th epoch: 32it [00:01, 18.99it/s, loss=0.0135] \n",
      "training 946th epoch: 32it [00:01, 18.50it/s, loss=0.0135] \n",
      "training 947th epoch: 32it [00:01, 18.42it/s, loss=0.0135] \n",
      "training 948th epoch: 32it [00:01, 18.61it/s, loss=0.0135] \n",
      "training 949th epoch: 32it [00:01, 18.48it/s, loss=0.0135] \n",
      "training 950th epoch: 32it [00:01, 18.56it/s, loss=0.0135] \n",
      "training 951th epoch: 32it [00:01, 18.59it/s, loss=0.0135] \n",
      "training 952th epoch: 32it [00:01, 18.46it/s, loss=0.0134] \n",
      "training 953th epoch: 32it [00:01, 18.68it/s, loss=0.0134] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 954th epoch: 32it [00:01, 18.94it/s, loss=0.0134] \n",
      "training 955th epoch: 32it [00:01, 18.83it/s, loss=0.0134] \n",
      "training 956th epoch: 32it [00:01, 20.31it/s, loss=0.0134] \n",
      "training 957th epoch: 32it [00:01, 20.70it/s, loss=0.0134] \n",
      "training 958th epoch: 32it [00:01, 20.30it/s, loss=0.0134] \n",
      "training 959th epoch: 32it [00:01, 20.75it/s, loss=0.0134] \n",
      "training 960th epoch: 32it [00:01, 20.40it/s, loss=0.0133] \n",
      "training 961th epoch: 32it [00:01, 20.51it/s, loss=0.0133] \n",
      "training 962th epoch: 32it [00:01, 20.49it/s, loss=0.0133] \n",
      "training 963th epoch: 32it [00:01, 20.40it/s, loss=0.0133] \n",
      "training 964th epoch: 32it [00:01, 20.37it/s, loss=0.0133] \n",
      "training 965th epoch: 32it [00:01, 20.48it/s, loss=0.0133] \n",
      "training 966th epoch: 32it [00:01, 20.43it/s, loss=0.0133] \n",
      "training 967th epoch: 32it [00:01, 20.96it/s, loss=0.0132] \n",
      "training 968th epoch: 32it [00:01, 20.51it/s, loss=0.0132] \n",
      "training 969th epoch: 32it [00:01, 20.51it/s, loss=0.0132] \n",
      "training 970th epoch: 32it [00:01, 20.77it/s, loss=0.0132] \n",
      "training 971th epoch: 32it [00:01, 20.64it/s, loss=0.0132] \n",
      "training 972th epoch: 32it [00:01, 20.56it/s, loss=0.0132] \n",
      "training 973th epoch: 32it [00:01, 20.47it/s, loss=0.0132] \n",
      "training 974th epoch: 32it [00:01, 20.67it/s, loss=0.0132] \n",
      "training 975th epoch: 32it [00:01, 20.81it/s, loss=0.0131] \n",
      "training 976th epoch: 32it [00:01, 20.19it/s, loss=0.0131] \n",
      "training 977th epoch: 32it [00:01, 18.25it/s, loss=0.0131] \n",
      "training 978th epoch: 32it [00:01, 18.14it/s, loss=0.0131] \n",
      "training 979th epoch: 32it [00:01, 18.06it/s, loss=0.0131] \n",
      "training 980th epoch: 32it [00:01, 18.25it/s, loss=0.0131] \n",
      "training 981th epoch: 32it [00:01, 18.67it/s, loss=0.0131] \n",
      "training 982th epoch: 32it [00:01, 18.51it/s, loss=0.0131] \n",
      "training 983th epoch: 32it [00:01, 18.68it/s, loss=0.013]  \n",
      "training 984th epoch: 32it [00:01, 18.51it/s, loss=0.013]  \n",
      "training 985th epoch: 32it [00:01, 19.10it/s, loss=0.013]  \n",
      "training 986th epoch: 32it [00:01, 20.42it/s, loss=0.013]  \n",
      "training 987th epoch: 32it [00:01, 20.22it/s, loss=0.013]  \n",
      "training 988th epoch: 32it [00:01, 20.48it/s, loss=0.013]  \n",
      "training 989th epoch: 32it [00:01, 19.90it/s, loss=0.013]  \n",
      "training 990th epoch: 32it [00:01, 20.52it/s, loss=0.013]  \n",
      "training 991th epoch: 32it [00:01, 20.36it/s, loss=0.0129] \n",
      "training 992th epoch: 32it [00:01, 20.55it/s, loss=0.0129] \n",
      "training 993th epoch: 32it [00:01, 20.62it/s, loss=0.0129] \n",
      "training 994th epoch: 32it [00:01, 20.36it/s, loss=0.0129] \n",
      "training 995th epoch: 32it [00:01, 20.25it/s, loss=0.0129] \n",
      "training 996th epoch: 32it [00:01, 20.02it/s, loss=0.0129] \n",
      "training 997th epoch: 32it [00:01, 20.32it/s, loss=0.0129] \n",
      "training 998th epoch: 32it [00:01, 20.71it/s, loss=0.0129] \n",
      "training 999th epoch: 32it [00:01, 20.62it/s, loss=0.0128] \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, dataloader):\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # get inputs and labels\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            print('** inputs[0]: {}'.format(inputs[0]))\n",
    "            print('** labels[0]: {}'.format(labels[0]))\n",
    "\n",
    "            # transpose inputs and labels\n",
    "            inputs = inputs.transpose(1, 0)\n",
    "            labels = labels.transpose(1, 0)\n",
    "\n",
    "            # initialize hidden for encoder\n",
    "            batch_size = inputs.size()[1]\n",
    "            encoder_hidden = encoder.initHidden(batch_size)\n",
    "            encoder_outputs = torch.zeros(MAX_LENGTH, batch_size, hidden_size, device=device)\n",
    "            encoder_outputs_ = torch.zeros(MAX_LENGTH, batch_size, hidden_size, device=device)\n",
    "            \n",
    "            # encoding\n",
    "            for j, inp in enumerate(inputs):\n",
    "                inp = inp.unsqueeze(0)\n",
    "                encoder_output, encoder_hidden = encoder(inp, encoder_hidden)\n",
    "                #print('** encoder_output_{}: {}'.format(i, encoder_output.shape))\n",
    "                encoder_outputs[j] = encoder_output\n",
    "                encoder_outputs_[j] += encoder_output[:,0]\n",
    "\n",
    "            print('** encoder_outputs.sum() VS encoder_outputs.sum(): {:.4f} VS {:.4f}'.format(encoder_outputs.sum(), encoder_outputs_.sum()))\n",
    "            \n",
    "            # initialize hidden for decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_inputs = torch.tensor([[bos]*batch_size], device=device)\n",
    "\n",
    "            pred = []\n",
    "            # decoding\n",
    "            for inp in labels:\n",
    "                #inp = inp.unsqueeze(0)\n",
    "\n",
    "                print('** inp: {}'.format(inp[0]))\n",
    "                print('** decoder_inputs shape: {}'.format(decoder_inputs.shape))\n",
    "                if use_attention:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_inputs, decoder_hidden, encoder_outputs_)\n",
    "                    print('** decoder_output shape: {}'.format(decoder_output.shape))\n",
    "                    decoder_output = decoder_output.argmax(1)\n",
    "                    print('** decoder_output[0]: {}'.format(decoder_output[0]))\n",
    "                    pred.append(decoder_output.cpu().numpy())\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_inputs, decoder_hidden)\n",
    "                    print('** decoder_output shape: {}'.format(decoder_output.shape))\n",
    "                    decoder_output = decoder_output.argmax(1)\n",
    "                    print('** decoder_output[0]: {}'.format(decoder_output[0]))\n",
    "                    pred.append(decoder_output.cpu().numpy())\n",
    "\n",
    "                decoder_inputs = inp.unsqueeze(0)\n",
    "\n",
    "            # re-transpose for validation\n",
    "            inputs = inputs.transpose(1, 0)\n",
    "            labels = labels.transpose(1, 0).cpu().numpy()\n",
    "            #return\n",
    "\n",
    "            # stack-up prediction for validation\n",
    "            pred = np.stack(pred, axis=1)\n",
    "            accuracy = (pred == labels).astype(np.int).mean()\n",
    "            print('{} VS {} → {:.4f}'.format(labels.shape, pred.shape, accuracy))\n",
    "            \n",
    "        return labels, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** inputs[0]: tensor([18, 21, 16, 12, 21, 12, 18, 16, 28, 28, 28, 23,  1,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([13, 10, 15, 19, 10, 19, 13, 15,  3,  3,  3,  8,  1,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -2726.1765 VS 852.4473\n",
      "** inp: 13\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 13\n",
      "** inp: 10\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 10\n",
      "** inp: 15\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 15\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 19\n",
      "** inp: 10\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 10\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 3\n",
      "** inp: 13\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 3\n",
      "** inp: 15\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 8\n",
      "** inp: 3\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 3\n",
      "** inp: 3\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 8\n",
      "** inp: 3\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 8\n",
      "** inp: 8\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 8\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7098\n",
      "** inputs[0]: tensor([10,  9, 12,  3, 20,  6, 14, 26, 25,  6, 24, 28,  1,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([21, 22, 19, 28, 11, 25, 17,  5,  6, 25,  7,  3,  1,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -2317.2146 VS -5136.2944\n",
      "** inp: 21\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 21\n",
      "** inp: 22\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 11\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 11\n",
      "** inp: 28\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 11\n",
      "** inp: 11\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 21\n",
      "** inp: 25\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 21\n",
      "** inp: 17\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 21\n",
      "** inp: 5\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 6\n",
      "** inp: 6\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 7\n",
      "** inp: 25\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 21\n",
      "** inp: 7\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 7\n",
      "** inp: 3\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 3\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7366\n",
      "** inputs[0]: tensor([28, 11, 13,  3, 12, 25, 12, 25,  9,  1,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([ 3, 20, 18, 28, 19,  6, 19,  6, 22,  1,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -3006.8442 VS -710.6979\n",
      "** inp: 3\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 3\n",
      "** inp: 20\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 20\n",
      "** inp: 18\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 6\n",
      "** inp: 28\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 20\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 20\n",
      "** inp: 6\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 22\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 22\n",
      "** inp: 6\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 22\n",
      "** inp: 22\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 22\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7098\n",
      "** inputs[0]: tensor([11,  5, 17, 27, 26, 26, 15, 26, 12,  1,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([20, 26, 14,  4,  5,  5, 16,  5, 19,  1,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -2314.6895 VS -5818.2842\n",
      "** inp: 20\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 20\n",
      "** inp: 26\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 26\n",
      "** inp: 14\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 4\n",
      "** inp: 4\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 4\n",
      "** inp: 5\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 19\n",
      "** inp: 5\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 19\n",
      "** inp: 16\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 16\n",
      "** inp: 5\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 19\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 23\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7054\n",
      "** inputs[0]: tensor([22,  7, 16, 26,  9,  5,  4, 10, 15,  3, 27,  1,  2,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([ 9, 24, 15,  5, 22, 26, 27, 21, 16, 28,  4,  1,  2,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -2706.4817 VS -3211.1824\n",
      "** inp: 9\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 9\n",
      "** inp: 24\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 24\n",
      "** inp: 15\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 5\n",
      "** inp: 5\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 9\n",
      "** inp: 22\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 26\n",
      "** inp: 26\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 26\n",
      "** inp: 27\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 16\n",
      "** inp: 21\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 4\n",
      "** inp: 16\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 16\n",
      "** inp: 28\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 4\n",
      "** inp: 4\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7121\n",
      "** inputs[0]: tensor([18, 23, 12, 24,  6,  1,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([13,  8, 19,  7, 25,  1,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -1925.5475 VS 843.2606\n",
      "** inp: 13\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 13\n",
      "** inp: 8\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 8\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 26\n",
      "** inp: 7\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 7\n",
      "** inp: 25\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 13\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 10\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7232\n",
      "** inputs[0]: tensor([ 4, 24, 21, 24, 15,  1,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([27,  7, 10,  7, 16,  1,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -700.4343 VS 264.2885\n",
      "** inp: 27\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 27\n",
      "** inp: 7\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 7\n",
      "** inp: 10\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 6\n",
      "** inp: 7\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 27\n",
      "** inp: 16\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 16\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "(8, 14) VS (8, 14) → 0.8036\n"
     ]
    }
   ],
   "source": [
    "labels, pred = validate(encoder, decoder, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 16, 14, 22, 20, 12, 19, 12, 11,  1,  2,  2,  2,  2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 16, 14, 14, 19, 11, 19, 16, 11, 16,  2,  2,  2,  2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(sequence):\n",
    "    # some reserved words\n",
    "    bos = 0\n",
    "    eos = 1\n",
    "    pad = 2\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # make input tensor\n",
    "        seq = list(map(lambda s: a2i[s], sequence))\n",
    "        n_pad = MAX_LENGTH - len(seq) - 2\n",
    "        inputs = [bos] + seq + [eos]# + [pad]*n_pad\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long).to(device)\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        print(inputs.shape)\n",
    "\n",
    "\n",
    "        # transpose inputs and labels\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        #labels = labels.transpose(1, 0)\n",
    "\n",
    "        # initialize hidden for encoder\n",
    "        batch_size = inputs.size()[1]    \n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, batch_size, hidden_size, device=device)\n",
    "        print('** encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "        # encoding\n",
    "        for i, inp in enumerate(inputs):\n",
    "            inp = inp.unsqueeze(0)\n",
    "            encoder_output, encoder_hidden = encoder(inp, encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_output[:, 0]\n",
    "\n",
    "        # initialize hidden for decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        pred = []\n",
    "        #target = [bos]\n",
    "        #target = torch.tensor(target).to(device)\n",
    "\n",
    "        print('** encoder_outputs sum: {}'.format(encoder_outputs.sum()))\n",
    "        print('** encoder_hidden sum: {}'.format(encoder_hidden.sum()))\n",
    "\n",
    "        #target = target.unsqueeze(0)\n",
    "        decoder_input = torch.tensor([[bos]], device=device)  # SOS\n",
    "        # decoding\n",
    "        for i in range(MAX_LENGTH):\n",
    "\n",
    "            if i == 7:\n",
    "                break\n",
    "\n",
    "            if use_attention:\n",
    "                print('** decoder_input: {}'.format(decoder_input.shape))\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                #decoder_output = decoder_output.argmax(1)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                print('** topi: {}'.format(topi.data.item()))\n",
    "                if topi.item() == eos:\n",
    "                    pred.append('</s>')\n",
    "                    break\n",
    "                else:\n",
    "                    pred.append(i2a[topi.item()])\n",
    "\n",
    "                decoder_input = topi.detach()\n",
    "        #else:\n",
    "        #    decoder_output, decoder_hidden = decoder(inp, decoder_hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "** encoder_outputs: torch.Size([15, 1, 256])\n",
      "** encoder_outputs sum: -29.475393295288086\n",
      "** encoder_hidden sum: -8.637638092041016\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n"
     ]
    }
   ],
   "source": [
    "predict_sequence('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_output, encoder_hidden = encoder(inputs, encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = labels.size()[1]\n",
    "# decoder_hidden = decoder.initHidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder(inputs, decoder_hidden, encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5]), torch.Size([3]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "inp = torch.randn(3, 5, requires_grad=True)\n",
    "lbl = torch.tensor([1, 0, 4])\n",
    "o = criterion(m(inp), lbl)\n",
    "inp.shape, lbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7729, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 15]) torch.Size([32, 15])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    inputs, labels = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataloader, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    # zero_grad for encoder/decoder optimizer\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # get inputs and labels\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # transpose inputs and labels\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        labels = labels.transpose(1, 0)\n",
    "        \n",
    "        # initialize hidden for encoder\n",
    "        batch_size = inputs.size()[1]\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_outputs = torch.zeros(15, 256, device=device)\n",
    "        \n",
    "        # encoding\n",
    "        for ip in inputs:\n",
    "            ip = ip.unsqueeze(0)\n",
    "            encoder_output, encoder_hidden = encoder(ip, encoder_hidden)\n",
    "            print('** encoder_output_{}: {}'.format(i, encoder_output.shape))\n",
    "            #print('** encoder_hidden_{}: {}'.format(i, encoder_hidden.shape))\n",
    "            \n",
    "        # initialize hidden for decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        loss = 0\n",
    "    \n",
    "        # decoding\n",
    "        for inp in labels:\n",
    "            #inp = inp.unsqueeze(0)\n",
    "            decoder_output, decoder_hidden = decoder(inp.unsqueeze(0), decoder_hidden)\n",
    "            \n",
    "            decoder_output = decoder_output.squeeze(0)\n",
    "            \n",
    "            #print('** decoder_hidden_{}: {}'.format(i, decoder_hidden.shape))\n",
    "            loss_it = criterion(decoder_output, inp)\n",
    "            loss += criterion(decoder_output, inp)\n",
    "            #print('** label vs pred: {} vs {} → {:.4f}'.format(inp.shape, decoder_output.shape, loss_it))\n",
    "        \n",
    "        #encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "        #decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "        #print('encoder, decoder: {:.4f} {:.4f}'.format(encoder_sum, decoder_sum))\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        #encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "        #decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "        #print('total loss after backward: {:.4f}'.format(loss))\n",
    "        #print('encoder, decoder: {:.4f} {:.4f}'.format(encoder_sum, decoder_sum))\n",
    "        \n",
    "        # update encoder/decoder\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        #encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "        #decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "        #print('total loss after step: {:.4f}'.format(loss))\n",
    "        #print('encoder, decoder: {:.4f} {:.4f}'.format(encoder_sum, decoder_sum))\n",
    "        print('{}th iteration → total loss after update: {:.4f}'.format(i, loss))\n",
    "        #print('-------------------------------------')\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, dataloader):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # get inputs and labels\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # transpose inputs and labels\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        labels = labels.transpose(1, 0)\n",
    "        \n",
    "        # initialize hidden for encoder\n",
    "        batch_size = inputs.size()[1]\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        \n",
    "        # encoding\n",
    "        for ip in inputs:\n",
    "            ip = ip.unsqueeze(0)\n",
    "            encoder_output, encoder_hidden = encoder(ip, encoder_hidden)\n",
    "            \n",
    "        # initialize hidden for decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        pred = []\n",
    "        # decoding\n",
    "        for inp in labels:\n",
    "            decoder_output, decoder_hidden = decoder(inp.unsqueeze(0), decoder_hidden)\n",
    "            decoder_output = decoder_output.squeeze(0)            \n",
    "            decoder_output = decoder_output.argmax(1)\n",
    "            pred.append(decoder_output.cpu().numpy())\n",
    "            #print('** decoder_output: {}'.format(decoder_output.cpu().numpy().shape))\n",
    "            \n",
    "        # re-transpose for validation\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        labels = labels.transpose(1, 0).cpu().numpy()\n",
    "        \n",
    "        # stack-up prediction for validation\n",
    "        pred = np.stack(pred, axis=1)\n",
    "        \n",
    "        # calculate batch-accurac y\n",
    "        accuracy = (pred == labels).astype(np.int).mean()\n",
    "        print('{} VS {} → {:.4f}'.format(labels.shape, pred.shape, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate(encoder, decoder, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'encoder_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a0614719763c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-2b51e65c3d20>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, dataloader, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#inp = inp.unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/publish/env_pub/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'encoder_output'"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.from_numpy(np.random.randint(0, 29, (32)))\n",
    "# out = torch.from_numpy(np.random.random((32, 29)))\n",
    "# criterion(out, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *************** NMT ***************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0+cu101'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 다운로드 → https://download.pytorch.org/tutorial/data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고 → https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # SOS 와 EOS 포함\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유니 코드 문자열을 일반 ASCII로 변환하십시오.\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소문자, 다듬기, 그리고 문자가 아닌 문자 제거\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # 파일을 읽고 줄로 분리\n",
    "    lines = open('../data/attention-ntm/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # 모든 줄을 쌍으로 분리하고 정규화\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # 쌍을 뒤집고, Lang 인스턴스 생성\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "['j ai honte de mon corps .', 'i m ashamed of my body .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without Attention\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        print('DecoderRNN_INPUT: input: {} → {:.4f}'.format(input.shape, input.sum()))\n",
    "        print('DecoderRNN_INPUT: hidden: {} → {:.4f}'.format(hidden.shape, hidden.sum()))\n",
    "        \n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        print('** output: {} → {:.4f}'.format(output.shape, output.sum()))\n",
    "        \n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        print('** gru-output: {} → {:.4f}'.format(output.shape, output.sum()))\n",
    "        print('** gru-hidden: {} → {:.4f}'.format(hidden.shape, hidden.sum()))\n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        \n",
    "        print('DecoderRNN_OUTPUT: output: {} → {:.4f}'.format(output.shape, output.sum()))\n",
    "        print('DecoderRNN_OUTPUT: hidden: {} → {:.4f}'.format(hidden.shape, hidden.sum()))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with attention\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        print('AttnDecoderRNN_INPUT: input → embedded: {} → {}'.format(input.shape, embedded.shape))\n",
    "        print('AttnDecoderRNN_INPUT: hidden: {}'.format(hidden.shape))\n",
    "        print('AttnDecoderRNN_INPUT: encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "        \n",
    "        #print('** embedded[0]: {}'.format(embedded[0].shape))\n",
    "        #print('** hidden[0]: {}'.format(hidden[0].shape))\n",
    "        #print('** concatenate embedded[0] and hidden[0]: {}'.format(torch.cat((embedded[0], hidden[0]), 1).shape))\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        #print('** attn_weights: {}'.format(attn_weights.shape))\n",
    "        #print('** attn_weights: {}'.format(attn_weights))\n",
    "        #print('** attn_weights.sum(): {}'.format(attn_weights.sum().shape))\n",
    "        #print('** attn_weights.sum(): {:.4f}'.format(attn_weights.sum()))\n",
    "        \n",
    "        #print('** attn_weights unsqueezed: {}'.format(attn_weights.unsqueeze(0).shape))\n",
    "        #print('** encoder_outputs unsqueezed: {}'.format(encoder_outputs.unsqueeze(0).shape))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        #print('** attn_applied: {}'.format(attn_applied.shape))\n",
    "        #print('** attn_applied[0]: {}'.format(attn_applied[0].shape))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #print('** gru-output: {}'.format(output.shape))\n",
    "        #print('** gru-hidden: {}'.format(hidden.shape))\n",
    "\n",
    "        #print('** output: {}'.format(output[0].shape))\n",
    "        #print('** self.out: {}'.format(self.out))\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        print('AttnDecoderRNN_OUTPUT: {}'.format(output.shape))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH, with_attention=True):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    print('** input_tensor[0]: {}'.format(input_tensor[0]))\n",
    "    print('** target_tensor[0]: {}'.format(target_tensor[0]))\n",
    "    return\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    print('encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        print('{} - {}'.format(ei, encoder_output[0, 0].shape))\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    print('decoder_hidden: {}'.format(decoder_hidden.shape))\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing 포함: 목표를 다음 입력으로 전달\n",
    "        for di in range(target_length):\n",
    "            if with_attention:\n",
    "                print('# {}th decoding'.format(di))\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                print('train:decoder_attention_{}: {}'.format(di, decoder_attention.shape))\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            \n",
    "\n",
    "    else:\n",
    "        # Teacher forcing 미포함: 자신의 예측을 다음 입력으로 사용\n",
    "        for di in range(target_length):\n",
    "            if with_attention:\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                print('train:decoder_attention_{}: {}'.format(di, decoder_attention.shape))\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # 입력으로 사용할 부분을 히스토리에서 분리\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01, with_attention=True):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # print_every 마다 초기화\n",
    "    plot_loss_total = 0  # plot_every 마다 초기화\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        print(input_tensor.shape, target_tensor.shape)\n",
    "        print(input_tensor)\n",
    "        print(target_tensor)\n",
    "        print('----------------------')\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion, with_attention=with_attention)\n",
    "        return\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # 주기적인 간격에 이 locator가 tick을 설정\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 256\n",
    "# encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "# decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Input validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1]) torch.Size([8, 1])\n",
      "tensor([[ 123],\n",
      "        [ 245],\n",
      "        [ 124],\n",
      "        [ 246],\n",
      "        [ 963],\n",
      "        [  34],\n",
      "        [ 101],\n",
      "        [2194],\n",
      "        [   5],\n",
      "        [   1]], device='cuda:0')\n",
      "tensor([[ 77],\n",
      "        [ 78],\n",
      "        [147],\n",
      "        [ 22],\n",
      "        [986],\n",
      "        [588],\n",
      "        [  4],\n",
      "        [  1]], device='cuda:0')\n",
      "----------------------\n",
      "** input_tensor[0]: tensor([123], device='cuda:0')\n",
      "** target_tensor[0]: tensor([77], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, decoder1, 75000, print_every=5000, with_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([1, 1]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0686, 0.1335, 0.0626, 0.0603, 0.0611, 0.1706, 0.0989, 0.1053, 0.1473,\n",
      "         0.0919]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.1012, 0.1292, 0.0967, 0.0339, 0.0354, 0.0609, 0.1461, 0.1979, 0.0466,\n",
      "         0.1521]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.1087, 0.0949, 0.0921, 0.1498, 0.1004, 0.1222, 0.1379, 0.0641, 0.0732,\n",
      "         0.0568]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0820, 0.1084, 0.0732, 0.1596, 0.1375, 0.1399, 0.1155, 0.0548, 0.0725,\n",
      "         0.0566]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0877, 0.1126, 0.1402, 0.0841, 0.0808, 0.1144, 0.0742, 0.0642, 0.1119,\n",
      "         0.1299]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0817, 0.1592, 0.0996, 0.0300, 0.0527, 0.1788, 0.2290, 0.0573, 0.0516,\n",
      "         0.0601]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0734, 0.0784, 0.1388, 0.1089, 0.0898, 0.1060, 0.0431, 0.0508, 0.2422,\n",
      "         0.0685]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0897, 0.0796, 0.1038, 0.1083, 0.1236, 0.0908, 0.0404, 0.0815, 0.2121,\n",
      "         0.0705]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0677, 0.1122, 0.0989, 0.0805, 0.2018, 0.1609, 0.0741, 0.0505, 0.1025,\n",
      "         0.0510]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0678, 0.0835, 0.1067, 0.0788, 0.0884, 0.0733, 0.1775, 0.1112, 0.1139,\n",
      "         0.0990]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4b216434e0>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # colorbar로 그림 설정\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # 축 설정\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # 매 틱마다 라벨 보여주기\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([1, 1]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0618, 0.1636, 0.0533, 0.0789, 0.0618, 0.1500, 0.0878, 0.1144, 0.1363,\n",
      "         0.0921]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0965, 0.1081, 0.0944, 0.0375, 0.0357, 0.0828, 0.1353, 0.1679, 0.0539,\n",
      "         0.1879]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0477, 0.0793, 0.0998, 0.1223, 0.0445, 0.0813, 0.0403, 0.1080, 0.3197,\n",
      "         0.0572]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0941, 0.0949, 0.0970, 0.1319, 0.1118, 0.1242, 0.1313, 0.0688, 0.0912,\n",
      "         0.0547]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "input = elle a cinq ans de moins que moi .\n",
      "output = tennis number rich <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/heavy_data/jkfirst/workspace/git/publish/env_pub/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/heavy_data/jkfirst/workspace/git/publish/env_pub/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "#evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "#evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "#evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pub",
   "language": "python",
   "name": "env_pub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
