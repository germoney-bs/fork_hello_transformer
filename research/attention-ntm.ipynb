{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference: https://towardsdatascience.com/attention-seq2seq-with-pytorch-learning-to-invert-a-sequence-34faf4133e53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0+cu101'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_map = {\n",
    "    'a':'z',\n",
    "    'b':'y',\n",
    "    'c':'x',\n",
    "    'd':'w',\n",
    "    'e':'v',\n",
    "    'f':'u',\n",
    "    'g':'t',\n",
    "    'h':'s',\n",
    "    'i':'r',\n",
    "    'j':'q',\n",
    "    'k':'p',\n",
    "    'l':'o',\n",
    "    'm':'n',\n",
    "    'n':'m',\n",
    "    'o':'l',\n",
    "    'p':'k',\n",
    "    'q':'j',\n",
    "    'r':'i',\n",
    "    's':'h',\n",
    "    't':'g',\n",
    "    'u':'f',\n",
    "    'v':'e',\n",
    "    'w':'d',\n",
    "    'x':'c',\n",
    "    'y':'b',\n",
    "    'z':'a'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2i = {\n",
    "    '<s>':0,\n",
    "    '</s>':1,\n",
    "    '<pad>':2,\n",
    "    'a':3,\n",
    "    'b':4,\n",
    "    'c':5,\n",
    "    'd':6,\n",
    "    'e':7,\n",
    "    'f':8,\n",
    "    'g':9,\n",
    "    'h':10,\n",
    "    'i':11,\n",
    "    'j':12,\n",
    "    'k':13,\n",
    "    'l':14,\n",
    "    'm':15,\n",
    "    'n':16,\n",
    "    'o':17,\n",
    "    'p':18,\n",
    "    'q':19,\n",
    "    'r':20,\n",
    "    's':21,\n",
    "    't':22,\n",
    "    'u':23,\n",
    "    'v':24,\n",
    "    'w':25,\n",
    "    'x':26,\n",
    "    'y':27,\n",
    "    'z':28,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2a = {v:k for k, v in a2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<s>',\n",
       " 1: '</s>',\n",
       " 2: '<pad>',\n",
       " 3: 'a',\n",
       " 4: 'b',\n",
       " 5: 'c',\n",
       " 6: 'd',\n",
       " 7: 'e',\n",
       " 8: 'f',\n",
       " 9: 'g',\n",
       " 10: 'h',\n",
       " 11: 'i',\n",
       " 12: 'j',\n",
       " 13: 'k',\n",
       " 14: 'l',\n",
       " 15: 'm',\n",
       " 16: 'n',\n",
       " 17: 'o',\n",
       " 18: 'p',\n",
       " 19: 'q',\n",
       " 20: 'r',\n",
       " 21: 's',\n",
       " 22: 't',\n",
       " 23: 'u',\n",
       " 24: 'v',\n",
       " 25: 'w',\n",
       " 26: 'x',\n",
       " 27: 'y',\n",
       " 28: 'z'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_alphabet_index():\n",
    "    random_length = np.random.randint(5, MAX_LENGTH-2)    # -2 because of <s> and </s>\n",
    "    #random_length = 14\n",
    "    random_alphabet_index = np.random.randint(0, 26, random_length) + 3\n",
    "    return random_alphabet_index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetToyDataset(Dataset):\n",
    "    def __init__(self, n_dataset=1000):\n",
    "        bos = 0\n",
    "        eos = 1\n",
    "        pad = 2\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "        for _ in range(n_dataset):\n",
    "            # make input example\n",
    "            aindex = generate_random_alphabet_index()\n",
    "            \n",
    "            # index to alphabet\n",
    "            alphabet = list(map(lambda a: i2a[a], aindex))\n",
    "            \n",
    "            # inversing\n",
    "            inversed_alphabet = list(map(lambda a: inverse_map[a], alphabet))\n",
    "            \n",
    "            # alphabet to index\n",
    "            iindex = list(map(lambda ia: a2i[ia], inversed_alphabet))\n",
    "            \n",
    "            # add bos, eos and pad\n",
    "            n_pad = MAX_LENGTH - len(aindex) - 2\n",
    "            #aindex = [bos] + aindex + [eos] + [pad]*n_pad\n",
    "            aindex = aindex + [eos] + [pad]*n_pad\n",
    "            #iindex = [bos] + iindex + [eos] + [pad]*n_pad\n",
    "            iindex = iindex + [eos] + [pad]*n_pad\n",
    "            \n",
    "            # add to examples\n",
    "            self.inputs.append(aindex)\n",
    "            self.labels.append(iindex)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return [\n",
    "            torch.tensor(self.inputs[index], dtype=torch.long),\n",
    "            torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AlphabetToyDataset()\n",
    "valid_dataset = AlphabetToyDataset(n_dataset=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_index_to_alphabet(index):\n",
    "    alphabet = list(map(lambda i: i2a[i], index))\n",
    "    return ' '.join(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aindex_14: f h c l x x </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "iindex_14: u s x o c c </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "** aindex_14: tensor([ 8, 10,  5, 14, 26, 26,  1,  2,  2,  2,  2,  2,  2,  2])\n",
      "** iindex_14: tensor([23, 21, 26, 17,  5,  5,  1,  2,  2,  2,  2,  2,  2,  2])\n",
      "------------\n",
      "aindex_14: x e w a g p m </s> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "iindex_14: c v d z t k n </s> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "** aindex_14: tensor([26,  7, 25,  3,  9, 18, 15,  1,  2,  2,  2,  2,  2,  2])\n",
      "** iindex_14: tensor([ 5, 24,  6, 28, 22, 13, 16,  1,  2,  2,  2,  2,  2,  2])\n",
      "------------\n",
      "aindex_14: f w y s r </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "iindex_14: u d b h i </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "** aindex_14: tensor([ 8, 25, 27, 21, 20,  1,  2,  2,  2,  2,  2,  2,  2,  2])\n",
      "** iindex_14: tensor([23,  6,  4, 10, 11,  1,  2,  2,  2,  2,  2,  2,  2,  2])\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    ex = train_dataset[i]\n",
    "    aindex, iindex = ex\n",
    "    \n",
    "    print('aindex_{}: {}'.format(len(aindex), convert_index_to_alphabet(aindex.numpy())))\n",
    "    print('iindex_{}: {}'.format(len(iindex), convert_index_to_alphabet(iindex.numpy())))\n",
    "    print('** aindex_{}: {}'.format(len(aindex), aindex))\n",
    "    print('** iindex_{}: {}'.format(len(iindex), iindex))\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AlphabetEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.embedding(inputs)\n",
    "        #print('** embedding: {}'.format(self.embedding(input).shape))\n",
    "        #print('** embedded: {}'.format(embedded.shape))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(AlphabetDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = F.relu(embedded)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetAttentionDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length):\n",
    "        super(AlphabetAttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden, encoder_outputs):\n",
    "        #print('AlphabetAttentionDecoder_INPUT: inputs: {}'.format(inputs.shape))\n",
    "        #print('AlphabetAttentionDecoder_INPUT: hidden: {}'.format(hidden.shape))\n",
    "        #print('AlphabetAttentionDecoder_INPUT: encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = F.relu(embedded)\n",
    "        #print('** embedded: {}'.format(embedded.shape))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        \n",
    "        # add attention\n",
    "        scores = torch.cat((embedded[0], hidden[0]), dim=1)\n",
    "        #print('** scores: {}'.format(scores.shape))\n",
    "        attn_weights = F.softmax(self.attn(scores), dim=1)\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs.transpose(1, 0))\n",
    "        #print('** attn_weights: {}'.format(attn_weights.shape))\n",
    "        #print('** attn_weights sum: {}'.format(attn_weights.sum()))\n",
    "        #print('** encoder_outputs transposed: {}'.format(encoder_outputs.transpose(1, 0).shape))\n",
    "        #print('** attn_applied: {}'.format(attn_applied.shape))\n",
    "        \n",
    "        # make output\n",
    "        output = torch.cat((attn_applied.transpose(1, 0), embedded), 2)\n",
    "        output = self.attn_combine(output)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        #print('** gru-output: {}'.format(output.shape))\n",
    "        #print('** gru-hidden: {}'.format(hidden.shape))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #print('** output[0]: {}'.format(output[0].shape))\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        \n",
    "        #print('AlphabetAttentionDecoder_OUTPUT: output: {}'.format(output.shape))\n",
    "        #print('------------------------------------------')\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetInversionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        hidden_size = 256\n",
    "        encoder = AlphabetEncoder(26+3, hidden_size).to(device)\n",
    "        decoder = AlphabetDecoder(hidden_size, 26+3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_attention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder = AlphabetEncoder(26+3, hidden_size).to(device)\n",
    "decoder = AlphabetDecoder(hidden_size, 26+3).to(device)\n",
    "#decoder = AlphabetAttentionDecoder(hidden_size, 26+3, MAX_LENGTH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_t = inputs.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# encoder_hidden = encoder.initHidden(batch_size)\n",
    "# encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = 0\n",
    "eos = 1\n",
    "pad = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=5):\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    # zero_grad for encoder/decoder optimizer\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # zero_grad for encoder/decoder optimizer\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        tbar = tqdm(enumerate(dataloader), desc='training {}th epoch'.format(epoch))\n",
    "        ####################################\n",
    "        for i, batch in tbar:\n",
    "\n",
    "            # get inputs and labels\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #print('** inputs[0]: {}'.format(inputs[0]))\n",
    "            #print('** labels[0]: {}'.format(labels[0]))\n",
    "            \n",
    "            # transpose inputs and labels\n",
    "            inputs = inputs.transpose(1, 0)\n",
    "            labels = labels.transpose(1, 0)\n",
    "\n",
    "            # initialize hidden for encoder\n",
    "            batch_size = inputs.size()[1]\n",
    "            max_length = inputs.size()[0]\n",
    "            encoder_hidden = encoder.initHidden(batch_size)\n",
    "            encoder_outputs = torch.zeros(max_length, batch_size, hidden_size, device=device)\n",
    "\n",
    "            # encoding\n",
    "            for j, inp in enumerate(inputs):\n",
    "                inp = inp.unsqueeze(0)\n",
    "                encoder_output, encoder_hidden = encoder(inp, encoder_hidden)\n",
    "                #print('** encoder_output_{}: {}'.format(i, encoder_output.shape))\n",
    "                encoder_outputs[j] += encoder_output[:,0]\n",
    "\n",
    "            # initialize hidden for decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            loss = 0\n",
    "\n",
    "            decoder_inputs = torch.tensor([[bos]*batch_size], device=device)\n",
    "            #print('** decoder_inputs: {}'.format(decoder_inputs.shape))\n",
    "\n",
    "            # decoding\n",
    "            for inp in labels:\n",
    "                #inp = inp.unsqueeze(0)\n",
    "                #print('** inp: {}'.format(inp))\n",
    "                #print('** inp shape: {}'.format(inp.shape))\n",
    "                #return\n",
    "\n",
    "                #print('** decoder_inputs: {}'.format(decoder_inputs.shape))\n",
    "                #print('** decoder_inputs[:,0]: {}'.format(decoder_inputs[:,0]))\n",
    "                if use_attention:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_inputs, decoder_hidden, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_inputs, decoder_hidden)\n",
    "\n",
    "                #print('** inp shape VS decoder_output shape: {} VS {}'.format(inp.shape, decoder_output.shape))\n",
    "                #print('** inp[0]: {}'.format(inp[0]))\n",
    "                #print('** decoder_output[0]: {}'.format(decoder_output[0]))\n",
    "                #print('** -----')\n",
    "                loss_it = criterion(decoder_output, inp)\n",
    "                loss += loss_it\n",
    "\n",
    "                decoder_inputs = inp.unsqueeze(0)\n",
    "                #print('** label vs pred: {} vs {} → {:.4f}'.format(inp.shape, decoder_output.shape, loss_it))\n",
    "\n",
    "            #return\n",
    "            #print('total loss before backward: {:.4f}'.format(loss))\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "            decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "            #print('total loss after backward: {:.4f}'.format(loss))\n",
    "            #print('encoder, decoder: {:.4f} {:.4f}'.format(encoder_sum, decoder_sum))\n",
    "\n",
    "            # update encoder/decoder\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "            decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "            #print('encoder, decoder: {:.4f} {:.4f}'.format(encoder_sum, decoder_sum))\n",
    "            #print('total loss after step: {:.4f}'.format(loss))\n",
    "            #print('{}-{}th iteration → total loss after update: {:.4f}'.format(epoch, i, loss))\n",
    "            \n",
    "            tbar.set_postfix(loss=loss.data.item())\n",
    "            #return\n",
    "            #break\n",
    "        ####################################\n",
    "        #return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 1000it [00:46, 21.40it/s, loss=3.47e+4]\n",
      "training 1th epoch: 1000it [00:43, 22.92it/s, loss=4.62e+4]\n",
      "training 2th epoch: 1000it [00:45, 21.89it/s, loss=4.95e+4]\n",
      "training 3th epoch: 1000it [00:48, 20.74it/s, loss=7.37e+4]\n",
      "training 4th epoch: 117it [00:05, 20.90it/s, loss=3.95e+4]"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:01, 19.34it/s, loss=0.731] \n",
      "training 1th epoch: 32it [00:01, 24.39it/s, loss=0.701] \n",
      "training 2th epoch: 32it [00:01, 24.50it/s, loss=0.674] \n",
      "training 3th epoch: 32it [00:01, 24.39it/s, loss=0.65]  \n",
      "training 4th epoch: 32it [00:01, 24.23it/s, loss=0.627] \n",
      "training 5th epoch: 32it [00:01, 20.64it/s, loss=0.606] \n",
      "training 6th epoch: 32it [00:01, 20.59it/s, loss=0.586] \n",
      "training 7th epoch: 32it [00:01, 20.65it/s, loss=0.568] \n",
      "training 8th epoch: 32it [00:01, 20.56it/s, loss=0.551] \n",
      "training 9th epoch: 32it [00:01, 20.37it/s, loss=0.535] \n",
      "training 10th epoch: 32it [00:01, 19.61it/s, loss=0.52]  \n",
      "training 11th epoch: 32it [00:01, 17.94it/s, loss=0.505] \n",
      "training 12th epoch: 32it [00:01, 18.60it/s, loss=0.492] \n",
      "training 13th epoch: 32it [00:01, 18.15it/s, loss=0.479] \n",
      "training 14th epoch: 32it [00:01, 17.93it/s, loss=0.467] \n",
      "training 15th epoch: 32it [00:01, 17.15it/s, loss=0.456] \n",
      "training 16th epoch: 32it [00:01, 17.86it/s, loss=0.445] \n",
      "training 17th epoch: 32it [00:01, 18.24it/s, loss=0.434] \n",
      "training 18th epoch: 32it [00:01, 18.40it/s, loss=0.424] \n",
      "training 19th epoch: 32it [00:01, 18.17it/s, loss=0.415] \n",
      "training 20th epoch: 32it [00:01, 18.34it/s, loss=0.406] \n",
      "training 21th epoch: 32it [00:01, 18.16it/s, loss=0.397] \n",
      "training 22th epoch: 32it [00:01, 18.19it/s, loss=0.389] \n",
      "training 23th epoch: 32it [00:01, 18.04it/s, loss=0.381] \n",
      "training 24th epoch: 32it [00:01, 18.35it/s, loss=0.373] \n",
      "training 25th epoch: 32it [00:01, 18.25it/s, loss=0.366] \n",
      "training 26th epoch: 32it [00:01, 18.27it/s, loss=0.359] \n",
      "training 27th epoch: 32it [00:01, 18.43it/s, loss=0.352] \n",
      "training 28th epoch: 32it [00:01, 18.29it/s, loss=0.346] \n",
      "training 29th epoch: 32it [00:01, 18.11it/s, loss=0.34]  \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:01, 18.09it/s, loss=0.333] \n",
      "training 1th epoch: 32it [00:01, 19.98it/s, loss=0.328] \n",
      "training 2th epoch: 32it [00:01, 20.25it/s, loss=0.322] \n",
      "training 3th epoch: 32it [00:01, 20.15it/s, loss=0.316] \n",
      "training 4th epoch: 32it [00:01, 20.59it/s, loss=0.311] \n",
      "training 5th epoch: 32it [00:01, 20.78it/s, loss=0.306] \n",
      "training 6th epoch: 32it [00:01, 20.38it/s, loss=0.301] \n",
      "training 7th epoch: 32it [00:01, 20.51it/s, loss=0.296] \n",
      "training 8th epoch: 32it [00:01, 20.24it/s, loss=0.292] \n",
      "training 9th epoch: 32it [00:01, 20.58it/s, loss=0.287] \n",
      "training 10th epoch: 32it [00:01, 20.31it/s, loss=0.283] \n",
      "training 11th epoch: 32it [00:01, 20.66it/s, loss=0.279] \n",
      "training 12th epoch: 32it [00:01, 20.48it/s, loss=0.275] \n",
      "training 13th epoch: 32it [00:01, 19.97it/s, loss=0.271] \n",
      "training 14th epoch: 32it [00:01, 19.20it/s, loss=0.267] \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:02, 15.73it/s, loss=0.484] \n",
      "training 1th epoch: 32it [00:01, 17.67it/s, loss=0.474] \n",
      "training 2th epoch: 32it [00:02, 15.08it/s, loss=0.464] \n",
      "training 3th epoch: 32it [00:02, 15.08it/s, loss=0.454] \n",
      "training 4th epoch: 32it [00:02, 14.94it/s, loss=0.444] \n",
      "training 5th epoch: 32it [00:02, 14.74it/s, loss=0.435] \n",
      "training 6th epoch: 32it [00:02, 14.57it/s, loss=0.426] \n",
      "training 7th epoch: 32it [00:02, 14.56it/s, loss=0.418] \n",
      "training 8th epoch: 32it [00:02, 14.64it/s, loss=0.41]  \n",
      "training 9th epoch: 32it [00:02, 14.62it/s, loss=0.402] \n",
      "training 10th epoch: 32it [00:02, 13.98it/s, loss=0.394] \n",
      "training 11th epoch: 32it [00:02, 14.65it/s, loss=0.387] \n",
      "training 12th epoch: 32it [00:02, 14.71it/s, loss=0.38]  \n",
      "training 13th epoch: 32it [00:02, 14.66it/s, loss=0.373] \n",
      "training 14th epoch: 32it [00:02, 14.67it/s, loss=0.366] \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:02, 14.56it/s, loss=0.36]  \n",
      "training 1th epoch: 32it [00:02, 14.58it/s, loss=0.354] \n",
      "training 2th epoch: 32it [00:02, 14.68it/s, loss=0.348] \n",
      "training 3th epoch: 32it [00:02, 14.74it/s, loss=0.342] \n",
      "training 4th epoch: 32it [00:02, 14.80it/s, loss=0.336] \n",
      "training 5th epoch: 32it [00:02, 14.75it/s, loss=0.331] \n",
      "training 6th epoch: 32it [00:02, 14.78it/s, loss=0.325] \n",
      "training 7th epoch: 32it [00:02, 14.84it/s, loss=0.32]  \n",
      "training 8th epoch: 32it [00:02, 14.73it/s, loss=0.315] \n",
      "training 9th epoch: 32it [00:02, 14.80it/s, loss=0.31]  \n",
      "training 10th epoch: 32it [00:02, 14.54it/s, loss=0.305] \n",
      "training 11th epoch: 32it [00:02, 14.74it/s, loss=0.301] \n",
      "training 12th epoch: 32it [00:02, 14.75it/s, loss=0.296] \n",
      "training 13th epoch: 32it [00:02, 14.68it/s, loss=0.292] \n",
      "training 14th epoch: 32it [00:02, 14.63it/s, loss=0.287] \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:01, 22.82it/s, loss=0.263] \n",
      "training 1th epoch: 32it [00:01, 24.46it/s, loss=0.259] \n",
      "training 2th epoch: 32it [00:01, 24.31it/s, loss=0.256] \n",
      "training 3th epoch: 32it [00:01, 24.25it/s, loss=0.252] \n",
      "training 4th epoch: 32it [00:01, 24.69it/s, loss=0.249] \n",
      "training 5th epoch: 32it [00:01, 24.34it/s, loss=0.245] \n",
      "training 6th epoch: 32it [00:01, 24.31it/s, loss=0.242] \n",
      "training 7th epoch: 32it [00:01, 21.87it/s, loss=0.239] \n",
      "training 8th epoch: 32it [00:01, 20.72it/s, loss=0.236] \n",
      "training 9th epoch: 32it [00:01, 20.59it/s, loss=0.233] \n",
      "training 10th epoch: 32it [00:01, 20.71it/s, loss=0.23]  \n",
      "training 11th epoch: 32it [00:01, 20.57it/s, loss=0.227] \n",
      "training 12th epoch: 32it [00:01, 20.69it/s, loss=0.225] \n",
      "training 13th epoch: 32it [00:01, 20.70it/s, loss=0.222] \n",
      "training 14th epoch: 32it [00:01, 20.51it/s, loss=0.219] \n",
      "training 15th epoch: 32it [00:01, 20.65it/s, loss=0.217] \n",
      "training 16th epoch: 32it [00:01, 20.51it/s, loss=0.214] \n",
      "training 17th epoch: 32it [00:01, 20.31it/s, loss=0.212] \n",
      "training 18th epoch: 32it [00:01, 20.19it/s, loss=0.209] \n",
      "training 19th epoch: 32it [00:01, 20.80it/s, loss=0.207] \n",
      "training 20th epoch: 32it [00:01, 20.41it/s, loss=0.205] \n",
      "training 21th epoch: 32it [00:01, 20.08it/s, loss=0.202] \n",
      "training 22th epoch: 32it [00:01, 20.74it/s, loss=0.2]   \n",
      "training 23th epoch: 32it [00:01, 20.72it/s, loss=0.198] \n",
      "training 24th epoch: 32it [00:01, 20.23it/s, loss=0.196] \n",
      "training 25th epoch: 32it [00:01, 19.92it/s, loss=0.194] \n",
      "training 26th epoch: 32it [00:01, 20.41it/s, loss=0.192] \n",
      "training 27th epoch: 32it [00:01, 20.56it/s, loss=0.19]  \n",
      "training 28th epoch: 32it [00:01, 20.72it/s, loss=0.188] \n",
      "training 29th epoch: 32it [00:01, 18.82it/s, loss=0.186] \n",
      "training 30th epoch: 32it [00:01, 18.78it/s, loss=0.184] \n",
      "training 31th epoch: 32it [00:01, 18.58it/s, loss=0.182] \n",
      "training 32th epoch: 32it [00:01, 18.65it/s, loss=0.18]  \n",
      "training 33th epoch: 32it [00:01, 18.70it/s, loss=0.178] \n",
      "training 34th epoch: 32it [00:01, 18.78it/s, loss=0.177] \n",
      "training 35th epoch: 32it [00:01, 18.66it/s, loss=0.175] \n",
      "training 36th epoch: 32it [00:01, 18.36it/s, loss=0.173] \n",
      "training 37th epoch: 32it [00:01, 18.99it/s, loss=0.172] \n",
      "training 38th epoch: 32it [00:01, 18.85it/s, loss=0.17]  \n",
      "training 39th epoch: 32it [00:01, 18.86it/s, loss=0.168] \n",
      "training 40th epoch: 32it [00:01, 18.71it/s, loss=0.167] \n",
      "training 41th epoch: 32it [00:01, 18.91it/s, loss=0.165] \n",
      "training 42th epoch: 32it [00:01, 18.99it/s, loss=0.164] \n",
      "training 43th epoch: 32it [00:01, 18.88it/s, loss=0.162] \n",
      "training 44th epoch: 32it [00:01, 18.87it/s, loss=0.161] \n",
      "training 45th epoch: 32it [00:01, 19.05it/s, loss=0.159] \n",
      "training 46th epoch: 32it [00:01, 18.90it/s, loss=0.158] \n",
      "training 47th epoch: 32it [00:01, 18.47it/s, loss=0.157] \n",
      "training 48th epoch: 32it [00:01, 18.74it/s, loss=0.155] \n",
      "training 49th epoch: 32it [00:01, 18.94it/s, loss=0.154] \n",
      "training 50th epoch: 32it [00:01, 18.93it/s, loss=0.153] \n",
      "training 51th epoch: 32it [00:01, 18.82it/s, loss=0.151] \n",
      "training 52th epoch: 32it [00:01, 18.96it/s, loss=0.15]  \n",
      "training 53th epoch: 32it [00:01, 19.63it/s, loss=0.149] \n",
      "training 54th epoch: 32it [00:01, 20.40it/s, loss=0.148] \n",
      "training 55th epoch: 32it [00:01, 20.43it/s, loss=0.146] \n",
      "training 56th epoch: 32it [00:01, 20.64it/s, loss=0.145] \n",
      "training 57th epoch: 32it [00:01, 20.77it/s, loss=0.144] \n",
      "training 58th epoch: 32it [00:01, 20.49it/s, loss=0.143] \n",
      "training 59th epoch: 32it [00:01, 20.81it/s, loss=0.142] \n",
      "training 60th epoch: 32it [00:01, 20.65it/s, loss=0.141] \n",
      "training 61th epoch: 32it [00:01, 20.84it/s, loss=0.139] \n",
      "training 62th epoch: 32it [00:01, 20.60it/s, loss=0.138] \n",
      "training 63th epoch: 32it [00:01, 20.67it/s, loss=0.137] \n",
      "training 64th epoch: 32it [00:01, 20.64it/s, loss=0.136] \n",
      "training 65th epoch: 32it [00:01, 20.44it/s, loss=0.135] \n",
      "training 66th epoch: 32it [00:01, 20.81it/s, loss=0.134] \n",
      "training 67th epoch: 32it [00:01, 20.50it/s, loss=0.133] \n",
      "training 68th epoch: 32it [00:01, 20.75it/s, loss=0.132] \n",
      "training 69th epoch: 32it [00:01, 20.56it/s, loss=0.131] \n",
      "training 70th epoch: 32it [00:01, 20.83it/s, loss=0.13]  \n",
      "training 71th epoch: 32it [00:01, 20.49it/s, loss=0.129] \n",
      "training 72th epoch: 32it [00:01, 20.74it/s, loss=0.128] \n",
      "training 73th epoch: 32it [00:01, 20.50it/s, loss=0.127] \n",
      "training 74th epoch: 32it [00:01, 20.72it/s, loss=0.126] \n",
      "training 75th epoch: 32it [00:01, 20.68it/s, loss=0.126] \n",
      "training 76th epoch: 32it [00:01, 20.61it/s, loss=0.125] \n",
      "training 77th epoch: 32it [00:01, 20.12it/s, loss=0.124] \n",
      "training 78th epoch: 32it [00:01, 20.41it/s, loss=0.123] \n",
      "training 79th epoch: 32it [00:01, 20.28it/s, loss=0.122] \n",
      "training 80th epoch: 32it [00:01, 20.30it/s, loss=0.121] \n",
      "training 81th epoch: 32it [00:01, 19.82it/s, loss=0.12]  \n",
      "training 82th epoch: 32it [00:01, 20.17it/s, loss=0.12]  \n",
      "training 83th epoch: 32it [00:01, 20.38it/s, loss=0.119] \n",
      "training 84th epoch: 32it [00:01, 19.27it/s, loss=0.118] \n",
      "training 85th epoch: 32it [00:01, 20.08it/s, loss=0.117] \n",
      "training 86th epoch: 32it [00:01, 20.63it/s, loss=0.116] \n",
      "training 87th epoch: 32it [00:01, 20.06it/s, loss=0.116] \n",
      "training 88th epoch: 32it [00:01, 20.39it/s, loss=0.115] \n",
      "training 89th epoch: 32it [00:01, 20.31it/s, loss=0.114] \n",
      "training 90th epoch: 32it [00:01, 20.73it/s, loss=0.113] \n",
      "training 91th epoch: 32it [00:01, 20.56it/s, loss=0.113] \n",
      "training 92th epoch: 32it [00:01, 20.62it/s, loss=0.112] \n",
      "training 93th epoch: 32it [00:01, 20.36it/s, loss=0.111] \n",
      "training 94th epoch: 32it [00:01, 20.56it/s, loss=0.11]  \n",
      "training 95th epoch: 32it [00:01, 20.65it/s, loss=0.11]  \n",
      "training 96th epoch: 32it [00:01, 20.65it/s, loss=0.109] \n",
      "training 97th epoch: 32it [00:01, 20.43it/s, loss=0.108]  \n",
      "training 98th epoch: 32it [00:01, 20.45it/s, loss=0.108]  \n",
      "training 99th epoch: 32it [00:01, 20.81it/s, loss=0.107] \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 32it [00:01, 20.43it/s, loss=0.106] \n",
      "training 1th epoch: 32it [00:01, 20.16it/s, loss=0.106]  \n",
      "training 2th epoch: 32it [00:01, 20.72it/s, loss=0.105] \n",
      "training 3th epoch: 32it [00:01, 20.42it/s, loss=0.104] \n",
      "training 4th epoch: 32it [00:01, 19.83it/s, loss=0.104]  \n",
      "training 5th epoch: 32it [00:01, 18.51it/s, loss=0.103]  \n",
      "training 6th epoch: 32it [00:01, 18.71it/s, loss=0.102]  \n",
      "training 7th epoch: 32it [00:01, 18.63it/s, loss=0.102]  \n",
      "training 8th epoch: 32it [00:01, 18.68it/s, loss=0.101]  \n",
      "training 9th epoch: 32it [00:01, 18.58it/s, loss=0.101]  \n",
      "training 10th epoch: 32it [00:01, 18.70it/s, loss=0.1]    \n",
      "training 11th epoch: 32it [00:01, 18.68it/s, loss=0.0995] \n",
      "training 12th epoch: 32it [00:01, 18.68it/s, loss=0.0989] \n",
      "training 13th epoch: 32it [00:01, 18.61it/s, loss=0.0984] \n",
      "training 14th epoch: 32it [00:01, 18.40it/s, loss=0.0978] \n",
      "training 15th epoch: 32it [00:01, 18.51it/s, loss=0.0972] \n",
      "training 16th epoch: 32it [00:01, 18.50it/s, loss=0.0967] \n",
      "training 17th epoch: 32it [00:01, 18.92it/s, loss=0.0961] \n",
      "training 18th epoch: 32it [00:01, 19.63it/s, loss=0.0956] \n",
      "training 19th epoch: 32it [00:01, 19.88it/s, loss=0.0951] \n",
      "training 20th epoch: 32it [00:01, 20.61it/s, loss=0.0945] \n",
      "training 21th epoch: 32it [00:01, 20.65it/s, loss=0.094]  \n",
      "training 22th epoch: 32it [00:01, 20.53it/s, loss=0.0935] \n",
      "training 23th epoch: 32it [00:01, 20.61it/s, loss=0.093]  \n",
      "training 24th epoch: 32it [00:01, 20.54it/s, loss=0.0925] \n",
      "training 25th epoch: 32it [00:01, 20.28it/s, loss=0.092]  \n",
      "training 26th epoch: 32it [00:01, 20.42it/s, loss=0.0915] \n",
      "training 27th epoch: 32it [00:01, 20.67it/s, loss=0.091]  \n",
      "training 28th epoch: 32it [00:01, 20.66it/s, loss=0.0905] \n",
      "training 29th epoch: 32it [00:01, 20.02it/s, loss=0.09]   \n",
      "training 30th epoch: 32it [00:01, 20.74it/s, loss=0.0895] \n",
      "training 31th epoch: 32it [00:01, 20.81it/s, loss=0.0891] \n",
      "training 32th epoch: 32it [00:01, 20.50it/s, loss=0.0886] \n",
      "training 33th epoch: 32it [00:01, 20.72it/s, loss=0.0881] \n",
      "training 34th epoch: 32it [00:01, 20.84it/s, loss=0.0877] \n",
      "training 35th epoch: 32it [00:01, 20.68it/s, loss=0.0872]\n",
      "training 36th epoch: 32it [00:01, 20.44it/s, loss=0.0868] \n",
      "training 37th epoch: 32it [00:01, 20.78it/s, loss=0.0863] \n",
      "training 38th epoch: 32it [00:01, 20.88it/s, loss=0.0859] \n",
      "training 39th epoch: 32it [00:01, 22.34it/s, loss=0.0854] \n",
      "training 40th epoch: 32it [00:01, 22.19it/s, loss=0.085]  \n",
      "training 41th epoch: 32it [00:01, 20.65it/s, loss=0.0846] \n",
      "training 42th epoch: 32it [00:01, 20.55it/s, loss=0.0841] \n",
      "training 43th epoch: 32it [00:01, 20.62it/s, loss=0.0837] \n",
      "training 44th epoch: 32it [00:01, 20.89it/s, loss=0.0833]\n",
      "training 45th epoch: 32it [00:01, 20.75it/s, loss=0.0829] \n",
      "training 46th epoch: 32it [00:01, 20.84it/s, loss=0.0825] \n",
      "training 47th epoch: 32it [00:01, 20.84it/s, loss=0.0821] \n",
      "training 48th epoch: 32it [00:01, 20.66it/s, loss=0.0817] \n",
      "training 49th epoch: 32it [00:01, 20.67it/s, loss=0.0813] \n",
      "training 50th epoch: 32it [00:01, 20.44it/s, loss=0.0809] \n",
      "training 51th epoch: 32it [00:01, 20.74it/s, loss=0.0805] \n",
      "training 52th epoch: 32it [00:01, 20.86it/s, loss=0.0801] \n",
      "training 53th epoch: 32it [00:01, 20.56it/s, loss=0.0797] \n",
      "training 54th epoch: 32it [00:01, 20.65it/s, loss=0.0793] \n",
      "training 55th epoch: 32it [00:01, 20.72it/s, loss=0.079]  \n",
      "training 56th epoch: 32it [00:01, 20.82it/s, loss=0.0786] \n",
      "training 57th epoch: 32it [00:01, 19.99it/s, loss=0.0782] \n",
      "training 58th epoch: 32it [00:01, 18.06it/s, loss=0.0779] \n",
      "training 59th epoch: 32it [00:01, 18.78it/s, loss=0.0775] \n",
      "training 60th epoch: 32it [00:01, 18.39it/s, loss=0.0771] \n",
      "training 61th epoch: 32it [00:01, 18.19it/s, loss=0.0768] \n",
      "training 62th epoch: 32it [00:01, 18.15it/s, loss=0.0764] \n",
      "training 63th epoch: 32it [00:01, 18.20it/s, loss=0.0761] \n",
      "training 64th epoch: 32it [00:01, 18.35it/s, loss=0.0757] \n",
      "training 65th epoch: 32it [00:01, 18.30it/s, loss=0.0754] \n",
      "training 66th epoch: 32it [00:01, 18.37it/s, loss=0.075]  \n",
      "training 67th epoch: 32it [00:01, 18.30it/s, loss=0.0747] \n",
      "training 68th epoch: 32it [00:01, 18.55it/s, loss=0.0744] \n",
      "training 69th epoch: 32it [00:01, 18.22it/s, loss=0.074]  \n",
      "training 70th epoch: 32it [00:01, 18.65it/s, loss=0.0737] \n",
      "training 71th epoch: 32it [00:01, 18.50it/s, loss=0.0734] \n",
      "training 72th epoch: 32it [00:01, 18.21it/s, loss=0.073]  \n",
      "training 73th epoch: 32it [00:01, 18.44it/s, loss=0.0727] \n",
      "training 74th epoch: 32it [00:01, 18.12it/s, loss=0.0724] \n",
      "training 75th epoch: 32it [00:01, 18.39it/s, loss=0.0721] \n",
      "training 76th epoch: 32it [00:01, 18.18it/s, loss=0.0718] \n",
      "training 77th epoch: 32it [00:01, 18.19it/s, loss=0.0714] \n",
      "training 78th epoch: 32it [00:01, 18.48it/s, loss=0.0711] \n",
      "training 79th epoch: 32it [00:01, 18.48it/s, loss=0.0708] \n",
      "training 80th epoch: 32it [00:01, 18.15it/s, loss=0.0705] \n",
      "training 81th epoch: 32it [00:01, 18.15it/s, loss=0.0702] \n",
      "training 82th epoch: 32it [00:01, 17.69it/s, loss=0.0699] \n",
      "training 83th epoch: 32it [00:01, 19.34it/s, loss=0.0696] \n",
      "training 84th epoch: 32it [00:01, 20.48it/s, loss=0.0693] \n",
      "training 85th epoch: 32it [00:01, 20.12it/s, loss=0.069]  \n",
      "training 86th epoch: 32it [00:01, 20.50it/s, loss=0.0688] \n",
      "training 87th epoch: 32it [00:01, 20.50it/s, loss=0.0685] \n",
      "training 88th epoch: 32it [00:01, 20.78it/s, loss=0.0682] \n",
      "training 89th epoch: 32it [00:01, 20.77it/s, loss=0.0679] \n",
      "training 90th epoch: 32it [00:01, 19.51it/s, loss=0.0676] \n",
      "training 91th epoch: 32it [00:01, 19.41it/s, loss=0.0673] \n",
      "training 92th epoch: 32it [00:01, 20.13it/s, loss=0.0671] \n",
      "training 93th epoch: 32it [00:01, 20.33it/s, loss=0.0668] \n",
      "training 94th epoch: 32it [00:01, 20.70it/s, loss=0.0665] \n",
      "training 95th epoch: 32it [00:01, 20.53it/s, loss=0.0662] \n",
      "training 96th epoch: 32it [00:01, 20.54it/s, loss=0.066]  \n",
      "training 97th epoch: 32it [00:01, 20.58it/s, loss=0.0657] \n",
      "training 98th epoch: 32it [00:01, 20.46it/s, loss=0.0654] \n",
      "training 99th epoch: 32it [00:01, 18.47it/s, loss=0.0652] \n",
      "training 100th epoch: 32it [00:01, 18.27it/s, loss=0.0649] \n",
      "training 101th epoch: 32it [00:01, 18.61it/s, loss=0.0647] \n",
      "training 102th epoch: 32it [00:01, 18.83it/s, loss=0.0644] \n",
      "training 103th epoch: 32it [00:01, 19.01it/s, loss=0.0641] \n",
      "training 104th epoch: 32it [00:01, 18.65it/s, loss=0.0639] \n",
      "training 105th epoch: 32it [00:01, 18.55it/s, loss=0.0636] \n",
      "training 106th epoch: 32it [00:01, 18.54it/s, loss=0.0634] \n",
      "training 107th epoch: 32it [00:01, 18.84it/s, loss=0.0631] \n",
      "training 108th epoch: 32it [00:01, 18.78it/s, loss=0.0629] \n",
      "training 109th epoch: 32it [00:01, 18.77it/s, loss=0.0627] \n",
      "training 110th epoch: 32it [00:01, 18.74it/s, loss=0.0624] \n",
      "training 111th epoch: 32it [00:01, 18.95it/s, loss=0.0622] \n",
      "training 112th epoch: 32it [00:01, 21.15it/s, loss=0.0619] \n",
      "training 113th epoch: 32it [00:01, 21.75it/s, loss=0.0617] \n",
      "training 114th epoch: 32it [00:01, 20.71it/s, loss=0.0615] \n",
      "training 115th epoch: 32it [00:01, 21.02it/s, loss=0.0612] \n",
      "training 116th epoch: 32it [00:01, 20.89it/s, loss=0.061]  \n",
      "training 117th epoch: 32it [00:01, 20.79it/s, loss=0.0608] \n",
      "training 118th epoch: 32it [00:01, 20.76it/s, loss=0.0605] \n",
      "training 119th epoch: 32it [00:01, 20.77it/s, loss=0.0603] \n",
      "training 120th epoch: 32it [00:01, 20.68it/s, loss=0.0601] \n",
      "training 121th epoch: 32it [00:01, 20.72it/s, loss=0.0599] \n",
      "training 122th epoch: 32it [00:01, 20.36it/s, loss=0.0596] \n",
      "training 123th epoch: 32it [00:01, 20.23it/s, loss=0.0594] \n",
      "training 124th epoch: 32it [00:01, 20.83it/s, loss=0.0592] \n",
      "training 125th epoch: 32it [00:01, 20.85it/s, loss=0.059]  \n",
      "training 126th epoch: 32it [00:01, 20.67it/s, loss=0.0588] \n",
      "training 127th epoch: 32it [00:01, 19.89it/s, loss=0.0586] \n",
      "training 128th epoch: 32it [00:01, 20.25it/s, loss=0.0583] \n",
      "training 129th epoch: 32it [00:01, 20.71it/s, loss=0.0581] \n",
      "training 130th epoch: 32it [00:01, 24.27it/s, loss=0.0579] \n",
      "training 131th epoch: 32it [00:01, 24.55it/s, loss=0.0577] \n",
      "training 132th epoch: 32it [00:01, 23.24it/s, loss=0.0575] \n",
      "training 133th epoch: 32it [00:01, 23.46it/s, loss=0.0573] \n",
      "training 134th epoch: 32it [00:01, 22.35it/s, loss=0.0571] \n",
      "training 135th epoch: 32it [00:01, 20.91it/s, loss=0.0569] \n",
      "training 136th epoch: 32it [00:01, 20.74it/s, loss=0.0567] \n",
      "training 137th epoch: 32it [00:01, 20.58it/s, loss=0.0565] \n",
      "training 138th epoch: 32it [00:01, 20.39it/s, loss=0.0563] \n",
      "training 139th epoch: 32it [00:01, 20.54it/s, loss=0.0561] \n",
      "training 140th epoch: 32it [00:01, 20.54it/s, loss=0.0559] \n",
      "training 141th epoch: 32it [00:01, 20.26it/s, loss=0.0557] \n",
      "training 142th epoch: 32it [00:01, 20.75it/s, loss=0.0555] \n",
      "training 143th epoch: 32it [00:01, 20.59it/s, loss=0.0553] \n",
      "training 144th epoch: 32it [00:01, 20.48it/s, loss=0.0551] \n",
      "training 145th epoch: 32it [00:01, 20.60it/s, loss=0.0549] \n",
      "training 146th epoch: 32it [00:01, 20.59it/s, loss=0.0547] \n",
      "training 147th epoch: 32it [00:01, 20.55it/s, loss=0.0546] \n",
      "training 148th epoch: 32it [00:01, 21.37it/s, loss=0.0544] \n",
      "training 149th epoch: 32it [00:01, 21.32it/s, loss=0.0542] \n",
      "training 150th epoch: 32it [00:01, 20.46it/s, loss=0.054]  \n",
      "training 151th epoch: 32it [00:01, 20.66it/s, loss=0.0538] \n",
      "training 152th epoch: 32it [00:01, 20.80it/s, loss=0.0536] \n",
      "training 153th epoch: 32it [00:01, 20.84it/s, loss=0.0535] \n",
      "training 154th epoch: 32it [00:01, 20.89it/s, loss=0.0533] \n",
      "training 155th epoch: 32it [00:01, 20.48it/s, loss=0.0531] \n",
      "training 156th epoch: 32it [00:01, 20.53it/s, loss=0.0529] \n",
      "training 157th epoch: 32it [00:01, 22.07it/s, loss=0.0527] \n",
      "training 158th epoch: 32it [00:01, 24.06it/s, loss=0.0526] \n",
      "training 159th epoch: 32it [00:01, 24.21it/s, loss=0.0524] \n",
      "training 160th epoch: 32it [00:01, 24.24it/s, loss=0.0522] \n",
      "training 161th epoch: 32it [00:01, 24.68it/s, loss=0.0521] \n",
      "training 162th epoch: 32it [00:01, 24.37it/s, loss=0.0519] \n",
      "training 163th epoch: 32it [00:01, 24.32it/s, loss=0.0517] \n",
      "training 164th epoch: 32it [00:01, 23.50it/s, loss=0.0515] \n",
      "training 165th epoch: 32it [00:01, 20.82it/s, loss=0.0514] \n",
      "training 166th epoch: 32it [00:01, 20.81it/s, loss=0.0512] \n",
      "training 167th epoch: 32it [00:01, 19.12it/s, loss=0.051]  \n",
      "training 168th epoch: 32it [00:01, 20.40it/s, loss=0.0509] \n",
      "training 169th epoch: 32it [00:01, 20.67it/s, loss=0.0507] \n",
      "training 170th epoch: 32it [00:01, 20.13it/s, loss=0.0506] \n",
      "training 171th epoch: 32it [00:01, 20.07it/s, loss=0.0504] \n",
      "training 172th epoch: 32it [00:01, 20.36it/s, loss=0.0502] \n",
      "training 173th epoch: 32it [00:01, 20.59it/s, loss=0.0501] \n",
      "training 174th epoch: 32it [00:01, 20.54it/s, loss=0.0499] \n",
      "training 175th epoch: 32it [00:01, 20.49it/s, loss=0.0498] \n",
      "training 176th epoch: 32it [00:01, 20.71it/s, loss=0.0496] \n",
      "training 177th epoch: 32it [00:01, 20.83it/s, loss=0.0494] \n",
      "training 178th epoch: 32it [00:01, 20.47it/s, loss=0.0493] \n",
      "training 179th epoch: 32it [00:01, 20.61it/s, loss=0.0491] \n",
      "training 180th epoch: 32it [00:01, 20.74it/s, loss=0.049]  \n",
      "training 181th epoch: 32it [00:01, 20.39it/s, loss=0.0488] \n",
      "training 182th epoch: 32it [00:01, 22.67it/s, loss=0.0487] \n",
      "training 183th epoch: 32it [00:01, 24.26it/s, loss=0.0485] \n",
      "training 184th epoch: 32it [00:01, 22.64it/s, loss=0.0484] \n",
      "training 185th epoch: 32it [00:01, 20.34it/s, loss=0.0482] \n",
      "training 186th epoch: 32it [00:01, 19.84it/s, loss=0.0481] \n",
      "training 187th epoch: 32it [00:01, 20.54it/s, loss=0.0479] \n",
      "training 188th epoch: 32it [00:01, 20.86it/s, loss=0.0478] \n",
      "training 189th epoch: 32it [00:01, 20.64it/s, loss=0.0476] \n",
      "training 190th epoch: 32it [00:01, 20.75it/s, loss=0.0475] \n",
      "training 191th epoch: 32it [00:01, 20.93it/s, loss=0.0474] \n",
      "training 192th epoch: 32it [00:01, 20.53it/s, loss=0.0472] \n",
      "training 193th epoch: 32it [00:01, 20.20it/s, loss=0.0471] \n",
      "training 194th epoch: 32it [00:01, 19.15it/s, loss=0.0469] \n",
      "training 195th epoch: 32it [00:01, 20.82it/s, loss=0.0468] \n",
      "training 196th epoch: 32it [00:01, 20.74it/s, loss=0.0466] \n",
      "training 197th epoch: 32it [00:01, 20.58it/s, loss=0.0465] \n",
      "training 198th epoch: 32it [00:01, 20.72it/s, loss=0.0464] \n",
      "training 199th epoch: 32it [00:01, 20.86it/s, loss=0.0462] \n",
      "training 200th epoch: 32it [00:01, 20.80it/s, loss=0.0461] \n",
      "training 201th epoch: 32it [00:01, 20.61it/s, loss=0.046]  \n",
      "training 202th epoch: 32it [00:01, 20.46it/s, loss=0.0458] \n",
      "training 203th epoch: 32it [00:01, 20.61it/s, loss=0.0457] \n",
      "training 204th epoch: 32it [00:01, 19.85it/s, loss=0.0456] \n",
      "training 205th epoch: 32it [00:01, 18.20it/s, loss=0.0454] \n",
      "training 206th epoch: 32it [00:01, 18.00it/s, loss=0.0453] \n",
      "training 207th epoch: 32it [00:01, 19.14it/s, loss=0.0452] \n",
      "training 208th epoch: 32it [00:01, 20.51it/s, loss=0.045]  \n",
      "training 209th epoch: 32it [00:01, 20.23it/s, loss=0.0449] \n",
      "training 210th epoch: 32it [00:01, 20.54it/s, loss=0.0448] \n",
      "training 211th epoch: 32it [00:01, 19.30it/s, loss=0.0446] \n",
      "training 212th epoch: 32it [00:01, 20.61it/s, loss=0.0445] \n",
      "training 213th epoch: 32it [00:01, 20.71it/s, loss=0.0444] \n",
      "training 214th epoch: 32it [00:01, 20.28it/s, loss=0.0443] \n",
      "training 215th epoch: 32it [00:01, 20.82it/s, loss=0.0441] \n",
      "training 216th epoch: 32it [00:01, 20.42it/s, loss=0.044]  \n",
      "training 217th epoch: 32it [00:01, 20.05it/s, loss=0.0439] \n",
      "training 218th epoch: 32it [00:01, 19.14it/s, loss=0.0438] \n",
      "training 219th epoch: 32it [00:01, 19.65it/s, loss=0.0436] \n",
      "training 220th epoch: 32it [00:01, 19.64it/s, loss=0.0435] \n",
      "training 221th epoch: 32it [00:01, 20.58it/s, loss=0.0434] \n",
      "training 222th epoch: 32it [00:01, 20.63it/s, loss=0.0433] \n",
      "training 223th epoch: 32it [00:01, 20.38it/s, loss=0.0431] \n",
      "training 224th epoch: 32it [00:01, 20.86it/s, loss=0.043]  \n",
      "training 225th epoch: 32it [00:01, 20.04it/s, loss=0.0429] \n",
      "training 226th epoch: 32it [00:01, 19.42it/s, loss=0.0428] \n",
      "training 227th epoch: 32it [00:01, 20.78it/s, loss=0.0427] \n",
      "training 228th epoch: 32it [00:01, 20.65it/s, loss=0.0426] \n",
      "training 229th epoch: 32it [00:01, 20.30it/s, loss=0.0424] \n",
      "training 230th epoch: 32it [00:01, 20.78it/s, loss=0.0423] \n",
      "training 231th epoch: 32it [00:01, 21.44it/s, loss=0.0422] \n",
      "training 232th epoch: 32it [00:01, 24.41it/s, loss=0.0421] \n",
      "training 233th epoch: 32it [00:01, 24.14it/s, loss=0.042]  \n",
      "training 234th epoch: 32it [00:01, 24.23it/s, loss=0.0419] \n",
      "training 235th epoch: 32it [00:01, 24.18it/s, loss=0.0417] \n",
      "training 236th epoch: 32it [00:01, 24.37it/s, loss=0.0416] \n",
      "training 237th epoch: 32it [00:01, 24.36it/s, loss=0.0415] \n",
      "training 238th epoch: 32it [00:01, 24.27it/s, loss=0.0414] \n",
      "training 239th epoch: 32it [00:01, 24.34it/s, loss=0.0413] \n",
      "training 240th epoch: 32it [00:01, 23.94it/s, loss=0.0412] \n",
      "training 241th epoch: 32it [00:01, 24.17it/s, loss=0.0411] \n",
      "training 242th epoch: 32it [00:01, 24.38it/s, loss=0.041]  \n",
      "training 243th epoch: 32it [00:01, 21.50it/s, loss=0.0409] \n",
      "training 244th epoch: 32it [00:01, 20.77it/s, loss=0.0408] \n",
      "training 245th epoch: 32it [00:01, 20.68it/s, loss=0.0406] \n",
      "training 246th epoch: 32it [00:01, 20.59it/s, loss=0.0405] \n",
      "training 247th epoch: 32it [00:01, 20.32it/s, loss=0.0404] \n",
      "training 248th epoch: 32it [00:01, 20.88it/s, loss=0.0403] \n",
      "training 249th epoch: 32it [00:01, 20.74it/s, loss=0.0402] \n",
      "training 250th epoch: 32it [00:01, 20.26it/s, loss=0.0401] \n",
      "training 251th epoch: 32it [00:01, 19.02it/s, loss=0.04]   \n",
      "training 252th epoch: 32it [00:01, 19.21it/s, loss=0.0399] \n",
      "training 253th epoch: 32it [00:01, 20.64it/s, loss=0.0398] \n",
      "training 254th epoch: 32it [00:01, 20.18it/s, loss=0.0397] \n",
      "training 255th epoch: 32it [00:01, 20.61it/s, loss=0.0396] \n",
      "training 256th epoch: 32it [00:01, 20.31it/s, loss=0.0395] \n",
      "training 257th epoch: 32it [00:01, 20.47it/s, loss=0.0394] \n",
      "training 258th epoch: 32it [00:01, 20.64it/s, loss=0.0393] \n",
      "training 259th epoch: 32it [00:01, 19.88it/s, loss=0.0392] \n",
      "training 260th epoch: 32it [00:01, 20.48it/s, loss=0.0391] \n",
      "training 261th epoch: 32it [00:01, 20.59it/s, loss=0.039]  \n",
      "training 262th epoch: 32it [00:01, 20.12it/s, loss=0.0389] \n",
      "training 263th epoch: 32it [00:01, 20.49it/s, loss=0.0388] \n",
      "training 264th epoch: 32it [00:01, 20.29it/s, loss=0.0387] \n",
      "training 265th epoch: 32it [00:01, 20.67it/s, loss=0.0386] \n",
      "training 266th epoch: 32it [00:01, 20.66it/s, loss=0.0385] \n",
      "training 267th epoch: 32it [00:01, 20.23it/s, loss=0.0384] \n",
      "training 268th epoch: 32it [00:01, 20.26it/s, loss=0.0383] \n",
      "training 269th epoch: 32it [00:01, 20.16it/s, loss=0.0382] \n",
      "training 270th epoch: 32it [00:01, 20.67it/s, loss=0.0381] \n",
      "training 271th epoch: 32it [00:01, 20.89it/s, loss=0.038]  \n",
      "training 272th epoch: 32it [00:01, 20.60it/s, loss=0.0379] \n",
      "training 273th epoch: 32it [00:01, 19.44it/s, loss=0.0378] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 274th epoch: 32it [00:01, 19.29it/s, loss=0.0377] \n",
      "training 275th epoch: 32it [00:01, 19.30it/s, loss=0.0376] \n",
      "training 276th epoch: 32it [00:01, 20.57it/s, loss=0.0376] \n",
      "training 277th epoch: 32it [00:01, 20.84it/s, loss=0.0375] \n",
      "training 278th epoch: 32it [00:01, 20.82it/s, loss=0.0374] \n",
      "training 279th epoch: 32it [00:01, 20.49it/s, loss=0.0373] \n",
      "training 280th epoch: 32it [00:01, 20.70it/s, loss=0.0372] \n",
      "training 281th epoch: 32it [00:01, 20.74it/s, loss=0.0371] \n",
      "training 282th epoch: 32it [00:01, 20.47it/s, loss=0.037]  \n",
      "training 283th epoch: 32it [00:01, 20.68it/s, loss=0.0369] \n",
      "training 284th epoch: 32it [00:01, 20.35it/s, loss=0.0368] \n",
      "training 285th epoch: 32it [00:01, 20.59it/s, loss=0.0367] \n",
      "training 286th epoch: 32it [00:01, 20.75it/s, loss=0.0366] \n",
      "training 287th epoch: 32it [00:01, 20.67it/s, loss=0.0366] \n",
      "training 288th epoch: 32it [00:01, 20.65it/s, loss=0.0365] \n",
      "training 289th epoch: 32it [00:01, 20.47it/s, loss=0.0364] \n",
      "training 290th epoch: 32it [00:01, 20.74it/s, loss=0.0363] \n",
      "training 291th epoch: 32it [00:01, 20.90it/s, loss=0.0362] \n",
      "training 292th epoch: 32it [00:01, 20.40it/s, loss=0.0361] \n",
      "training 293th epoch: 32it [00:01, 20.54it/s, loss=0.036]  \n",
      "training 294th epoch: 32it [00:01, 20.83it/s, loss=0.0359] \n",
      "training 295th epoch: 32it [00:01, 20.65it/s, loss=0.0359] \n",
      "training 296th epoch: 32it [00:01, 20.73it/s, loss=0.0358] \n",
      "training 297th epoch: 32it [00:01, 20.89it/s, loss=0.0357] \n",
      "training 298th epoch: 32it [00:01, 20.62it/s, loss=0.0356] \n",
      "training 299th epoch: 32it [00:01, 20.76it/s, loss=0.0355] \n",
      "training 300th epoch: 32it [00:01, 20.79it/s, loss=0.0354] \n",
      "training 301th epoch: 32it [00:01, 20.89it/s, loss=0.0354] \n",
      "training 302th epoch: 32it [00:01, 20.72it/s, loss=0.0353] \n",
      "training 303th epoch: 32it [00:01, 20.76it/s, loss=0.0352] \n",
      "training 304th epoch: 32it [00:01, 20.75it/s, loss=0.0351] \n",
      "training 305th epoch: 32it [00:01, 20.46it/s, loss=0.035]  \n",
      "training 306th epoch: 32it [00:01, 20.90it/s, loss=0.035]  \n",
      "training 307th epoch: 32it [00:01, 21.81it/s, loss=0.0349] \n",
      "training 308th epoch: 32it [00:01, 20.49it/s, loss=0.0348] \n",
      "training 309th epoch: 32it [00:01, 18.96it/s, loss=0.0347] \n",
      "training 310th epoch: 32it [00:01, 20.44it/s, loss=0.0346] \n",
      "training 311th epoch: 32it [00:01, 20.88it/s, loss=0.0346] \n",
      "training 312th epoch: 32it [00:01, 20.80it/s, loss=0.0345] \n",
      "training 313th epoch: 32it [00:01, 20.06it/s, loss=0.0344] \n",
      "training 314th epoch: 32it [00:01, 20.58it/s, loss=0.0343] \n",
      "training 315th epoch: 32it [00:01, 20.80it/s, loss=0.0342] \n",
      "training 316th epoch: 32it [00:01, 20.66it/s, loss=0.0342] \n",
      "training 317th epoch: 32it [00:01, 20.79it/s, loss=0.0341] \n",
      "training 318th epoch: 32it [00:01, 20.35it/s, loss=0.034]  \n",
      "training 319th epoch: 32it [00:01, 20.65it/s, loss=0.0339] \n",
      "training 320th epoch: 32it [00:01, 19.62it/s, loss=0.0339] \n",
      "training 321th epoch: 32it [00:01, 18.76it/s, loss=0.0338] \n",
      "training 322th epoch: 32it [00:01, 19.04it/s, loss=0.0337] \n",
      "training 323th epoch: 32it [00:01, 18.58it/s, loss=0.0336] \n",
      "training 324th epoch: 32it [00:01, 18.85it/s, loss=0.0335] \n",
      "training 325th epoch: 32it [00:01, 18.63it/s, loss=0.0335] \n",
      "training 326th epoch: 32it [00:01, 18.43it/s, loss=0.0334] \n",
      "training 327th epoch: 32it [00:01, 18.36it/s, loss=0.0333] \n",
      "training 328th epoch: 32it [00:01, 18.85it/s, loss=0.0333] \n",
      "training 329th epoch: 32it [00:01, 18.63it/s, loss=0.0332] \n",
      "training 330th epoch: 32it [00:01, 18.93it/s, loss=0.0331] \n",
      "training 331th epoch: 32it [00:01, 18.40it/s, loss=0.033]  \n",
      "training 332th epoch: 32it [00:01, 18.75it/s, loss=0.033]  \n",
      "training 333th epoch: 32it [00:01, 18.72it/s, loss=0.0329] \n",
      "training 334th epoch: 32it [00:01, 18.68it/s, loss=0.0328] \n",
      "training 335th epoch: 32it [00:01, 18.54it/s, loss=0.0327] \n",
      "training 336th epoch: 32it [00:01, 18.69it/s, loss=0.0327] \n",
      "training 337th epoch: 32it [00:01, 18.54it/s, loss=0.0326] \n",
      "training 338th epoch: 32it [00:01, 19.37it/s, loss=0.0325] \n",
      "training 339th epoch: 32it [00:01, 20.59it/s, loss=0.0325] \n",
      "training 340th epoch: 32it [00:01, 20.29it/s, loss=0.0324] \n",
      "training 341th epoch: 32it [00:01, 20.88it/s, loss=0.0323] \n",
      "training 342th epoch: 32it [00:01, 20.58it/s, loss=0.0322] \n",
      "training 343th epoch: 32it [00:01, 20.65it/s, loss=0.0322] \n",
      "training 344th epoch: 32it [00:01, 20.78it/s, loss=0.0321] \n",
      "training 345th epoch: 32it [00:01, 20.61it/s, loss=0.032]  \n",
      "training 346th epoch: 32it [00:01, 20.77it/s, loss=0.032]  \n",
      "training 347th epoch: 32it [00:01, 20.44it/s, loss=0.0319] \n",
      "training 348th epoch: 32it [00:01, 20.75it/s, loss=0.0318] \n",
      "training 349th epoch: 32it [00:01, 20.59it/s, loss=0.0318] \n",
      "training 350th epoch: 32it [00:01, 20.55it/s, loss=0.0317] \n",
      "training 351th epoch: 32it [00:01, 20.35it/s, loss=0.0316] \n",
      "training 352th epoch: 32it [00:01, 20.81it/s, loss=0.0316] \n",
      "training 353th epoch: 32it [00:01, 20.56it/s, loss=0.0315] \n",
      "training 354th epoch: 32it [00:01, 20.88it/s, loss=0.0314] \n",
      "training 355th epoch: 32it [00:01, 20.89it/s, loss=0.0314] \n",
      "training 356th epoch: 32it [00:01, 20.88it/s, loss=0.0313] \n",
      "training 357th epoch: 32it [00:01, 20.62it/s, loss=0.0312] \n",
      "training 358th epoch: 32it [00:01, 20.78it/s, loss=0.0312] \n",
      "training 359th epoch: 32it [00:01, 20.54it/s, loss=0.0311] \n",
      "training 360th epoch: 32it [00:01, 21.11it/s, loss=0.031]  \n",
      "training 361th epoch: 32it [00:01, 21.00it/s, loss=0.031]  \n",
      "training 362th epoch: 32it [00:01, 20.96it/s, loss=0.0309] \n",
      "training 363th epoch: 32it [00:01, 20.74it/s, loss=0.0308] \n",
      "training 364th epoch: 32it [00:01, 20.77it/s, loss=0.0308] \n",
      "training 365th epoch: 32it [00:01, 20.99it/s, loss=0.0307] \n",
      "training 366th epoch: 32it [00:01, 20.64it/s, loss=0.0307] \n",
      "training 367th epoch: 32it [00:01, 19.29it/s, loss=0.0306] \n",
      "training 368th epoch: 32it [00:01, 21.33it/s, loss=0.0305] \n",
      "training 369th epoch: 32it [00:01, 22.43it/s, loss=0.0305] \n",
      "training 370th epoch: 32it [00:01, 24.38it/s, loss=0.0304] \n",
      "training 371th epoch: 32it [00:01, 24.20it/s, loss=0.0303] \n",
      "training 372th epoch: 32it [00:01, 24.18it/s, loss=0.0303] \n",
      "training 373th epoch: 32it [00:01, 24.40it/s, loss=0.0302] \n",
      "training 374th epoch: 32it [00:01, 23.97it/s, loss=0.0302] \n",
      "training 375th epoch: 32it [00:01, 24.15it/s, loss=0.0301] \n",
      "training 376th epoch: 32it [00:01, 24.37it/s, loss=0.03]   \n",
      "training 377th epoch: 32it [00:01, 24.10it/s, loss=0.03]   \n",
      "training 378th epoch: 32it [00:01, 24.10it/s, loss=0.0299] \n",
      "training 379th epoch: 32it [00:01, 21.15it/s, loss=0.0299] \n",
      "training 380th epoch: 32it [00:01, 20.49it/s, loss=0.0298] \n",
      "training 381th epoch: 32it [00:01, 20.22it/s, loss=0.0297] \n",
      "training 382th epoch: 32it [00:01, 20.36it/s, loss=0.0297] \n",
      "training 383th epoch: 32it [00:01, 20.47it/s, loss=0.0296] \n",
      "training 384th epoch: 32it [00:01, 20.43it/s, loss=0.0296] \n",
      "training 385th epoch: 32it [00:01, 19.76it/s, loss=0.0295] \n",
      "training 386th epoch: 32it [00:01, 19.25it/s, loss=0.0294] \n",
      "training 387th epoch: 32it [00:01, 19.37it/s, loss=0.0294] \n",
      "training 388th epoch: 32it [00:01, 19.58it/s, loss=0.0293] \n",
      "training 389th epoch: 32it [00:01, 19.62it/s, loss=0.0293] \n",
      "training 390th epoch: 32it [00:01, 19.44it/s, loss=0.0292] \n",
      "training 391th epoch: 32it [00:01, 19.52it/s, loss=0.0291] \n",
      "training 392th epoch: 32it [00:01, 19.43it/s, loss=0.0291] \n",
      "training 393th epoch: 32it [00:01, 19.15it/s, loss=0.029]  \n",
      "training 394th epoch: 32it [00:01, 19.34it/s, loss=0.029]  \n",
      "training 395th epoch: 32it [00:01, 20.77it/s, loss=0.0289] \n",
      "training 396th epoch: 32it [00:01, 20.66it/s, loss=0.0289] \n",
      "training 397th epoch: 32it [00:01, 20.48it/s, loss=0.0288] \n",
      "training 398th epoch: 32it [00:01, 20.40it/s, loss=0.0287] \n",
      "training 399th epoch: 32it [00:01, 19.51it/s, loss=0.0287] \n",
      "training 400th epoch: 32it [00:01, 18.48it/s, loss=0.0286] \n",
      "training 401th epoch: 32it [00:01, 17.92it/s, loss=0.0286] \n",
      "training 402th epoch: 32it [00:01, 18.08it/s, loss=0.0285] \n",
      "training 403th epoch: 32it [00:01, 19.56it/s, loss=0.0285] \n",
      "training 404th epoch: 32it [00:01, 19.98it/s, loss=0.0284] \n",
      "training 405th epoch: 32it [00:01, 20.56it/s, loss=0.0284] \n",
      "training 406th epoch: 32it [00:01, 20.42it/s, loss=0.0283] \n",
      "training 407th epoch: 32it [00:01, 20.16it/s, loss=0.0283] \n",
      "training 408th epoch: 32it [00:01, 20.68it/s, loss=0.0282] \n",
      "training 409th epoch: 32it [00:01, 20.29it/s, loss=0.0281] \n",
      "training 410th epoch: 32it [00:01, 20.53it/s, loss=0.0281] \n",
      "training 411th epoch: 32it [00:01, 20.39it/s, loss=0.028]  \n",
      "training 412th epoch: 32it [00:01, 20.49it/s, loss=0.028]  \n",
      "training 413th epoch: 32it [00:01, 20.33it/s, loss=0.0279] \n",
      "training 414th epoch: 32it [00:01, 20.50it/s, loss=0.0279] \n",
      "training 415th epoch: 32it [00:01, 20.64it/s, loss=0.0278] \n",
      "training 416th epoch: 32it [00:01, 21.27it/s, loss=0.0278] \n",
      "training 417th epoch: 32it [00:01, 20.54it/s, loss=0.0277] \n",
      "training 418th epoch: 32it [00:01, 20.76it/s, loss=0.0277] \n",
      "training 419th epoch: 32it [00:01, 20.48it/s, loss=0.0276] \n",
      "training 420th epoch: 32it [00:01, 20.36it/s, loss=0.0276] \n",
      "training 421th epoch: 32it [00:01, 21.14it/s, loss=0.0275] \n",
      "training 422th epoch: 32it [00:01, 21.08it/s, loss=0.0275] \n",
      "training 423th epoch: 32it [00:01, 20.37it/s, loss=0.0274] \n",
      "training 424th epoch: 32it [00:01, 20.18it/s, loss=0.0274] \n",
      "training 425th epoch: 32it [00:01, 19.02it/s, loss=0.0273] \n",
      "training 426th epoch: 32it [00:01, 19.08it/s, loss=0.0273] \n",
      "training 427th epoch: 32it [00:01, 19.03it/s, loss=0.0272] \n",
      "training 428th epoch: 32it [00:01, 18.88it/s, loss=0.0272] \n",
      "training 429th epoch: 32it [00:01, 18.99it/s, loss=0.0271] \n",
      "training 430th epoch: 32it [00:01, 18.97it/s, loss=0.0271] \n",
      "training 431th epoch: 32it [00:01, 18.75it/s, loss=0.027]  \n",
      "training 432th epoch: 32it [00:01, 18.65it/s, loss=0.027]  \n",
      "training 433th epoch: 32it [00:01, 18.52it/s, loss=0.0269] \n",
      "training 434th epoch: 32it [00:01, 18.92it/s, loss=0.0269] \n",
      "training 435th epoch: 32it [00:01, 18.88it/s, loss=0.0268] \n",
      "training 436th epoch: 32it [00:01, 18.83it/s, loss=0.0268] \n",
      "training 437th epoch: 32it [00:01, 18.92it/s, loss=0.0267] \n",
      "training 438th epoch: 32it [00:01, 18.61it/s, loss=0.0267] \n",
      "training 439th epoch: 32it [00:01, 19.08it/s, loss=0.0266] \n",
      "training 440th epoch: 32it [00:01, 19.03it/s, loss=0.0266] \n",
      "training 441th epoch: 32it [00:01, 18.72it/s, loss=0.0265] \n",
      "training 442th epoch: 32it [00:01, 19.85it/s, loss=0.0265] \n",
      "training 443th epoch: 32it [00:01, 20.58it/s, loss=0.0264] \n",
      "training 444th epoch: 32it [00:01, 19.54it/s, loss=0.0264] \n",
      "training 445th epoch: 32it [00:01, 19.99it/s, loss=0.0263] \n",
      "training 446th epoch: 32it [00:01, 20.27it/s, loss=0.0263] \n",
      "training 447th epoch: 32it [00:01, 20.66it/s, loss=0.0262] \n",
      "training 448th epoch: 32it [00:01, 20.89it/s, loss=0.0262] \n",
      "training 449th epoch: 32it [00:01, 20.46it/s, loss=0.0261] \n",
      "training 450th epoch: 32it [00:01, 20.76it/s, loss=0.0261] \n",
      "training 451th epoch: 32it [00:01, 20.88it/s, loss=0.026]  \n",
      "training 452th epoch: 32it [00:01, 22.27it/s, loss=0.026]  \n",
      "training 453th epoch: 32it [00:01, 20.84it/s, loss=0.0259] \n",
      "training 454th epoch: 32it [00:01, 20.49it/s, loss=0.0259] \n",
      "training 455th epoch: 32it [00:01, 19.46it/s, loss=0.0259] \n",
      "training 456th epoch: 32it [00:01, 20.51it/s, loss=0.0258] \n",
      "training 457th epoch: 32it [00:01, 20.64it/s, loss=0.0258] \n",
      "training 458th epoch: 32it [00:01, 20.68it/s, loss=0.0257] \n",
      "training 459th epoch: 32it [00:01, 21.02it/s, loss=0.0257] \n",
      "training 460th epoch: 32it [00:01, 20.03it/s, loss=0.0256] \n",
      "training 461th epoch: 32it [00:01, 20.31it/s, loss=0.0256] \n",
      "training 462th epoch: 32it [00:01, 19.58it/s, loss=0.0255] \n",
      "training 463th epoch: 32it [00:01, 20.94it/s, loss=0.0255] \n",
      "training 464th epoch: 32it [00:01, 18.76it/s, loss=0.0254] \n",
      "training 465th epoch: 32it [00:01, 18.11it/s, loss=0.0254] \n",
      "training 466th epoch: 32it [00:01, 18.33it/s, loss=0.0254] \n",
      "training 467th epoch: 32it [00:01, 18.24it/s, loss=0.0253] \n",
      "training 468th epoch: 32it [00:01, 17.26it/s, loss=0.0253] \n",
      "training 469th epoch: 32it [00:01, 18.07it/s, loss=0.0252] \n",
      "training 470th epoch: 32it [00:01, 18.02it/s, loss=0.0252] \n",
      "training 471th epoch: 32it [00:01, 18.18it/s, loss=0.0251] \n",
      "training 472th epoch: 32it [00:01, 17.83it/s, loss=0.0251] \n",
      "training 473th epoch: 32it [00:01, 18.27it/s, loss=0.025]  \n",
      "training 474th epoch: 32it [00:01, 18.44it/s, loss=0.025]  \n",
      "training 475th epoch: 32it [00:01, 20.14it/s, loss=0.025]  \n",
      "training 476th epoch: 32it [00:01, 20.21it/s, loss=0.0249] \n",
      "training 477th epoch: 32it [00:01, 19.90it/s, loss=0.0249] \n",
      "training 478th epoch: 32it [00:01, 18.98it/s, loss=0.0248] \n",
      "training 479th epoch: 32it [00:01, 18.55it/s, loss=0.0248] \n",
      "training 480th epoch: 32it [00:01, 18.49it/s, loss=0.0248] \n",
      "training 481th epoch: 32it [00:01, 18.32it/s, loss=0.0247] \n",
      "training 482th epoch: 32it [00:01, 20.61it/s, loss=0.0247] \n",
      "training 483th epoch: 32it [00:01, 20.46it/s, loss=0.0246] \n",
      "training 484th epoch: 32it [00:01, 20.42it/s, loss=0.0246] \n",
      "training 485th epoch: 32it [00:01, 20.20it/s, loss=0.0245] \n",
      "training 486th epoch: 32it [00:01, 20.67it/s, loss=0.0245] \n",
      "training 487th epoch: 32it [00:01, 20.14it/s, loss=0.0245] \n",
      "training 488th epoch: 32it [00:01, 20.41it/s, loss=0.0244] \n",
      "training 489th epoch: 32it [00:01, 20.46it/s, loss=0.0244] \n",
      "training 490th epoch: 32it [00:01, 20.67it/s, loss=0.0243] \n",
      "training 491th epoch: 32it [00:01, 20.61it/s, loss=0.0243] \n",
      "training 492th epoch: 32it [00:01, 20.47it/s, loss=0.0243] \n",
      "training 493th epoch: 32it [00:01, 20.21it/s, loss=0.0242] \n",
      "training 494th epoch: 32it [00:01, 20.68it/s, loss=0.0242] \n",
      "training 495th epoch: 32it [00:01, 20.69it/s, loss=0.0241] \n",
      "training 496th epoch: 32it [00:01, 20.85it/s, loss=0.0241] \n",
      "training 497th epoch: 32it [00:01, 20.40it/s, loss=0.024]  \n",
      "training 498th epoch: 32it [00:01, 19.44it/s, loss=0.024]  \n",
      "training 499th epoch: 32it [00:01, 20.65it/s, loss=0.024]  \n",
      "training 500th epoch: 32it [00:01, 20.26it/s, loss=0.0239] \n",
      "training 501th epoch: 32it [00:01, 20.72it/s, loss=0.0239] \n",
      "training 502th epoch: 32it [00:01, 20.71it/s, loss=0.0239] \n",
      "training 503th epoch: 32it [00:01, 20.16it/s, loss=0.0238] \n",
      "training 504th epoch: 32it [00:01, 20.89it/s, loss=0.0238] \n",
      "training 505th epoch: 32it [00:01, 20.82it/s, loss=0.0237] \n",
      "training 506th epoch: 32it [00:01, 20.28it/s, loss=0.0237] \n",
      "training 507th epoch: 32it [00:01, 20.42it/s, loss=0.0237] \n",
      "training 508th epoch: 32it [00:01, 20.79it/s, loss=0.0236] \n",
      "training 509th epoch: 32it [00:01, 20.49it/s, loss=0.0236] \n",
      "training 510th epoch: 32it [00:01, 20.26it/s, loss=0.0235] \n",
      "training 511th epoch: 32it [00:01, 20.72it/s, loss=0.0235] \n",
      "training 512th epoch: 32it [00:01, 19.40it/s, loss=0.0235] \n",
      "training 513th epoch: 32it [00:01, 18.88it/s, loss=0.0234] \n",
      "training 514th epoch: 32it [00:01, 18.54it/s, loss=0.0234] \n",
      "training 515th epoch: 32it [00:01, 18.89it/s, loss=0.0233] \n",
      "training 516th epoch: 32it [00:01, 18.76it/s, loss=0.0233] \n",
      "training 517th epoch: 32it [00:01, 18.88it/s, loss=0.0233] \n",
      "training 518th epoch: 32it [00:01, 18.84it/s, loss=0.0232] \n",
      "training 519th epoch: 32it [00:01, 18.59it/s, loss=0.0232] \n",
      "training 520th epoch: 32it [00:01, 18.70it/s, loss=0.0232] \n",
      "training 521th epoch: 32it [00:01, 18.74it/s, loss=0.0231] \n",
      "training 522th epoch: 32it [00:01, 18.41it/s, loss=0.0231] \n",
      "training 523th epoch: 32it [00:01, 18.77it/s, loss=0.023]  \n",
      "training 524th epoch: 32it [00:01, 18.75it/s, loss=0.023]  \n",
      "training 525th epoch: 32it [00:01, 18.80it/s, loss=0.023]  \n",
      "training 526th epoch: 32it [00:01, 18.52it/s, loss=0.0229] \n",
      "training 527th epoch: 32it [00:01, 18.55it/s, loss=0.0229] \n",
      "training 528th epoch: 32it [00:01, 18.45it/s, loss=0.0229] \n",
      "training 529th epoch: 32it [00:01, 18.55it/s, loss=0.0228] \n",
      "training 530th epoch: 32it [00:01, 18.48it/s, loss=0.0228] \n",
      "training 531th epoch: 32it [00:01, 18.50it/s, loss=0.0228] \n",
      "training 532th epoch: 32it [00:01, 19.09it/s, loss=0.0227] \n",
      "training 533th epoch: 32it [00:01, 20.18it/s, loss=0.0227] \n",
      "training 534th epoch: 32it [00:01, 19.98it/s, loss=0.0226] \n",
      "training 535th epoch: 32it [00:01, 18.59it/s, loss=0.0226] \n",
      "training 536th epoch: 32it [00:01, 17.61it/s, loss=0.0226] \n",
      "training 537th epoch: 32it [00:01, 18.64it/s, loss=0.0225] \n",
      "training 538th epoch: 32it [00:01, 19.86it/s, loss=0.0225] \n",
      "training 539th epoch: 32it [00:01, 18.28it/s, loss=0.0225] \n",
      "training 540th epoch: 32it [00:01, 18.10it/s, loss=0.0224] \n",
      "training 541th epoch: 32it [00:01, 18.31it/s, loss=0.0224] \n",
      "training 542th epoch: 32it [00:01, 18.28it/s, loss=0.0224] \n",
      "training 543th epoch: 32it [00:01, 19.20it/s, loss=0.0223] \n",
      "training 544th epoch: 32it [00:01, 20.22it/s, loss=0.0223] \n",
      "training 545th epoch: 32it [00:01, 20.53it/s, loss=0.0223] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 546th epoch: 32it [00:01, 20.67it/s, loss=0.0222] \n",
      "training 547th epoch: 32it [00:01, 20.41it/s, loss=0.0222] \n",
      "training 548th epoch: 32it [00:01, 19.95it/s, loss=0.0222] \n",
      "training 549th epoch: 32it [00:01, 20.27it/s, loss=0.0221] \n",
      "training 550th epoch: 32it [00:01, 20.12it/s, loss=0.0221] \n",
      "training 551th epoch: 32it [00:01, 20.41it/s, loss=0.0221] \n",
      "training 552th epoch: 32it [00:01, 20.33it/s, loss=0.022]  \n",
      "training 553th epoch: 32it [00:01, 20.41it/s, loss=0.022]  \n",
      "training 554th epoch: 32it [00:01, 20.54it/s, loss=0.022]  \n",
      "training 555th epoch: 32it [00:01, 20.21it/s, loss=0.0219] \n",
      "training 556th epoch: 32it [00:01, 20.46it/s, loss=0.0219] \n",
      "training 557th epoch: 32it [00:01, 20.33it/s, loss=0.0218] \n",
      "training 558th epoch: 32it [00:01, 20.66it/s, loss=0.0218] \n",
      "training 559th epoch: 32it [00:01, 20.71it/s, loss=0.0218] \n",
      "training 560th epoch: 32it [00:01, 20.32it/s, loss=0.0217] \n",
      "training 561th epoch: 32it [00:01, 20.55it/s, loss=0.0217] \n",
      "training 562th epoch: 32it [00:01, 20.44it/s, loss=0.0217] \n",
      "training 563th epoch: 32it [00:01, 20.22it/s, loss=0.0217] \n",
      "training 564th epoch: 32it [00:01, 20.26it/s, loss=0.0216] \n",
      "training 565th epoch: 32it [00:01, 20.28it/s, loss=0.0216] \n",
      "training 566th epoch: 32it [00:01, 20.54it/s, loss=0.0216] \n",
      "training 567th epoch: 32it [00:01, 20.59it/s, loss=0.0215] \n",
      "training 568th epoch: 32it [00:01, 20.37it/s, loss=0.0215] \n",
      "training 569th epoch: 32it [00:01, 20.27it/s, loss=0.0215] \n",
      "training 570th epoch: 32it [00:01, 19.92it/s, loss=0.0214] \n",
      "training 571th epoch: 32it [00:01, 20.82it/s, loss=0.0214] \n",
      "training 572th epoch: 32it [00:01, 21.33it/s, loss=0.0214] \n",
      "training 573th epoch: 32it [00:01, 23.31it/s, loss=0.0213] \n",
      "training 574th epoch: 32it [00:01, 22.12it/s, loss=0.0213] \n",
      "training 575th epoch: 32it [00:01, 20.41it/s, loss=0.0213] \n",
      "training 576th epoch: 32it [00:01, 24.23it/s, loss=0.0212] \n",
      "training 577th epoch: 32it [00:01, 23.00it/s, loss=0.0212] \n",
      "training 578th epoch: 32it [00:01, 20.70it/s, loss=0.0212] \n",
      "training 579th epoch: 32it [00:01, 22.33it/s, loss=0.0211] \n",
      "training 580th epoch: 32it [00:01, 20.71it/s, loss=0.0211] \n",
      "training 581th epoch: 32it [00:01, 20.72it/s, loss=0.0211] \n",
      "training 582th epoch: 32it [00:01, 20.68it/s, loss=0.021]  \n",
      "training 583th epoch: 32it [00:01, 20.47it/s, loss=0.021]  \n",
      "training 584th epoch: 32it [00:01, 20.50it/s, loss=0.021]  \n",
      "training 585th epoch: 32it [00:01, 20.58it/s, loss=0.0209] \n",
      "training 586th epoch: 32it [00:01, 20.34it/s, loss=0.0209] \n",
      "training 587th epoch: 32it [00:01, 20.60it/s, loss=0.0209] \n",
      "training 588th epoch: 32it [00:01, 20.49it/s, loss=0.0209] \n",
      "training 589th epoch: 32it [00:01, 20.67it/s, loss=0.0208] \n",
      "training 590th epoch: 32it [00:01, 20.65it/s, loss=0.0208] \n",
      "training 591th epoch: 32it [00:01, 20.47it/s, loss=0.0208] \n",
      "training 592th epoch: 32it [00:01, 20.47it/s, loss=0.0207] \n",
      "training 593th epoch: 32it [00:01, 20.69it/s, loss=0.0207] \n",
      "training 594th epoch: 32it [00:01, 20.73it/s, loss=0.0207] \n",
      "training 595th epoch: 32it [00:01, 19.29it/s, loss=0.0206] \n",
      "training 596th epoch: 32it [00:01, 19.79it/s, loss=0.0206] \n",
      "training 597th epoch: 32it [00:01, 20.44it/s, loss=0.0206] \n",
      "training 598th epoch: 32it [00:01, 20.62it/s, loss=0.0206] \n",
      "training 599th epoch: 32it [00:01, 23.39it/s, loss=0.0205] \n",
      "training 600th epoch: 32it [00:01, 24.18it/s, loss=0.0205] \n",
      "training 601th epoch: 32it [00:01, 24.17it/s, loss=0.0205] \n",
      "training 602th epoch: 32it [00:01, 24.41it/s, loss=0.0204] \n",
      "training 603th epoch: 32it [00:01, 24.16it/s, loss=0.0204] \n",
      "training 604th epoch: 32it [00:01, 22.66it/s, loss=0.0204] \n",
      "training 605th epoch: 32it [00:01, 20.37it/s, loss=0.0203] \n",
      "training 606th epoch: 32it [00:01, 22.04it/s, loss=0.0203] \n",
      "training 607th epoch: 32it [00:01, 20.59it/s, loss=0.0203] \n",
      "training 608th epoch: 32it [00:01, 20.11it/s, loss=0.0203] \n",
      "training 609th epoch: 32it [00:01, 20.54it/s, loss=0.0202] \n",
      "training 610th epoch: 32it [00:01, 21.36it/s, loss=0.0202] \n",
      "training 611th epoch: 32it [00:01, 20.68it/s, loss=0.0202] \n",
      "training 612th epoch: 32it [00:01, 20.67it/s, loss=0.0201] \n",
      "training 613th epoch: 32it [00:01, 20.39it/s, loss=0.0201] \n",
      "training 614th epoch: 32it [00:01, 20.67it/s, loss=0.0201] \n",
      "training 615th epoch: 32it [00:01, 20.47it/s, loss=0.0201] \n",
      "training 616th epoch: 32it [00:01, 20.44it/s, loss=0.02]   \n",
      "training 617th epoch: 32it [00:01, 20.62it/s, loss=0.02]   \n",
      "training 618th epoch: 32it [00:01, 20.80it/s, loss=0.02]   \n",
      "training 619th epoch: 32it [00:01, 20.41it/s, loss=0.0199] \n",
      "training 620th epoch: 32it [00:01, 20.44it/s, loss=0.0199] \n",
      "training 621th epoch: 32it [00:01, 23.37it/s, loss=0.0199] \n",
      "training 622th epoch: 32it [00:01, 24.17it/s, loss=0.0199] \n",
      "training 623th epoch: 32it [00:01, 24.10it/s, loss=0.0198] \n",
      "training 624th epoch: 32it [00:01, 24.28it/s, loss=0.0198] \n",
      "training 625th epoch: 32it [00:01, 24.23it/s, loss=0.0198] \n",
      "training 626th epoch: 32it [00:01, 24.02it/s, loss=0.0197] \n",
      "training 627th epoch: 32it [00:01, 24.57it/s, loss=0.0197] \n",
      "training 628th epoch: 32it [00:01, 23.14it/s, loss=0.0197] \n",
      "training 629th epoch: 32it [00:01, 23.92it/s, loss=0.0197] \n",
      "training 630th epoch: 32it [00:01, 23.21it/s, loss=0.0196] \n",
      "training 631th epoch: 32it [00:01, 20.31it/s, loss=0.0196] \n",
      "training 632th epoch: 32it [00:01, 20.38it/s, loss=0.0196] \n",
      "training 633th epoch: 32it [00:01, 20.54it/s, loss=0.0196] \n",
      "training 634th epoch: 32it [00:01, 20.89it/s, loss=0.0195] \n",
      "training 635th epoch: 32it [00:01, 19.78it/s, loss=0.0195] \n",
      "training 636th epoch: 32it [00:01, 19.58it/s, loss=0.0195] \n",
      "training 637th epoch: 32it [00:01, 19.76it/s, loss=0.0194] \n",
      "training 638th epoch: 32it [00:01, 19.45it/s, loss=0.0194] \n",
      "training 639th epoch: 32it [00:01, 19.34it/s, loss=0.0194] \n",
      "training 640th epoch: 32it [00:01, 19.30it/s, loss=0.0194] \n",
      "training 641th epoch: 32it [00:01, 20.24it/s, loss=0.0193] \n",
      "training 642th epoch: 32it [00:01, 20.47it/s, loss=0.0193] \n",
      "training 643th epoch: 32it [00:01, 20.65it/s, loss=0.0193] \n",
      "training 644th epoch: 32it [00:01, 20.53it/s, loss=0.0193] \n",
      "training 645th epoch: 32it [00:01, 19.71it/s, loss=0.0192] \n",
      "training 646th epoch: 32it [00:01, 18.27it/s, loss=0.0192] \n",
      "training 647th epoch: 32it [00:01, 18.71it/s, loss=0.0192] \n",
      "training 648th epoch: 32it [00:01, 18.40it/s, loss=0.0192] \n",
      "training 649th epoch: 32it [00:01, 18.26it/s, loss=0.0191] \n",
      "training 650th epoch: 32it [00:01, 18.58it/s, loss=0.0191] \n",
      "training 651th epoch: 32it [00:01, 20.52it/s, loss=0.0191] \n",
      "training 652th epoch: 32it [00:01, 20.53it/s, loss=0.0191] \n",
      "training 653th epoch: 32it [00:01, 20.77it/s, loss=0.019]  \n",
      "training 654th epoch: 32it [00:01, 20.37it/s, loss=0.019]  \n",
      "training 655th epoch: 32it [00:01, 20.84it/s, loss=0.019]  \n",
      "training 656th epoch: 32it [00:01, 20.86it/s, loss=0.0189] \n",
      "training 657th epoch: 32it [00:01, 20.44it/s, loss=0.0189] \n",
      "training 658th epoch: 32it [00:01, 20.64it/s, loss=0.0189] \n",
      "training 659th epoch: 32it [00:01, 20.12it/s, loss=0.0189] \n",
      "training 660th epoch: 32it [00:01, 19.04it/s, loss=0.0188] \n",
      "training 661th epoch: 32it [00:01, 20.10it/s, loss=0.0188] \n",
      "training 662th epoch: 32it [00:01, 20.75it/s, loss=0.0188] \n",
      "training 663th epoch: 32it [00:01, 20.80it/s, loss=0.0188] \n",
      "training 664th epoch: 32it [00:01, 20.16it/s, loss=0.0187] \n",
      "training 665th epoch: 32it [00:01, 19.57it/s, loss=0.0187] \n",
      "training 666th epoch: 32it [00:01, 19.83it/s, loss=0.0187] \n",
      "training 667th epoch: 32it [00:01, 20.42it/s, loss=0.0187] \n",
      "training 668th epoch: 32it [00:01, 20.69it/s, loss=0.0186] \n",
      "training 669th epoch: 32it [00:01, 19.33it/s, loss=0.0186] \n",
      "training 670th epoch: 32it [00:01, 18.53it/s, loss=0.0186] \n",
      "training 671th epoch: 32it [00:01, 18.62it/s, loss=0.0186] \n",
      "training 672th epoch: 32it [00:01, 18.81it/s, loss=0.0185] \n",
      "training 673th epoch: 32it [00:01, 18.93it/s, loss=0.0185] \n",
      "training 674th epoch: 32it [00:01, 18.50it/s, loss=0.0185] \n",
      "training 675th epoch: 32it [00:01, 18.60it/s, loss=0.0185] \n",
      "training 676th epoch: 32it [00:01, 18.60it/s, loss=0.0184] \n",
      "training 677th epoch: 32it [00:01, 18.46it/s, loss=0.0184] \n",
      "training 678th epoch: 32it [00:01, 18.84it/s, loss=0.0184] \n",
      "training 679th epoch: 32it [00:01, 18.94it/s, loss=0.0184] \n",
      "training 680th epoch: 32it [00:01, 18.63it/s, loss=0.0184] \n",
      "training 681th epoch: 32it [00:01, 18.71it/s, loss=0.0183] \n",
      "training 682th epoch: 32it [00:01, 18.90it/s, loss=0.0183] \n",
      "training 683th epoch: 32it [00:01, 18.81it/s, loss=0.0183] \n",
      "training 684th epoch: 32it [00:01, 18.57it/s, loss=0.0183] \n",
      "training 685th epoch: 32it [00:01, 18.69it/s, loss=0.0182] \n",
      "training 686th epoch: 32it [00:01, 18.73it/s, loss=0.0182] \n",
      "training 687th epoch: 32it [00:01, 18.80it/s, loss=0.0182] \n",
      "training 688th epoch: 32it [00:01, 18.65it/s, loss=0.0182] \n",
      "training 689th epoch: 32it [00:01, 18.72it/s, loss=0.0181] \n",
      "training 690th epoch: 32it [00:01, 18.58it/s, loss=0.0181] \n",
      "training 691th epoch: 32it [00:01, 18.68it/s, loss=0.0181] \n",
      "training 692th epoch: 32it [00:01, 18.76it/s, loss=0.0181] \n",
      "training 693th epoch: 32it [00:01, 18.69it/s, loss=0.018]  \n",
      "training 694th epoch: 32it [00:01, 18.41it/s, loss=0.018]  \n",
      "training 695th epoch: 32it [00:01, 18.59it/s, loss=0.018]  \n",
      "training 696th epoch: 32it [00:01, 18.60it/s, loss=0.018]  \n",
      "training 697th epoch: 32it [00:01, 18.68it/s, loss=0.0179] \n",
      "training 698th epoch: 32it [00:01, 18.39it/s, loss=0.0179] \n",
      "training 699th epoch: 32it [00:01, 18.90it/s, loss=0.0179] \n",
      "training 700th epoch: 32it [00:01, 18.59it/s, loss=0.0179] \n",
      "training 701th epoch: 32it [00:01, 18.98it/s, loss=0.0179] \n",
      "training 702th epoch: 32it [00:01, 18.70it/s, loss=0.0178] \n",
      "training 703th epoch: 32it [00:01, 18.41it/s, loss=0.0178] \n",
      "training 704th epoch: 32it [00:01, 18.70it/s, loss=0.0178] \n",
      "training 705th epoch: 32it [00:01, 18.65it/s, loss=0.0178] \n",
      "training 706th epoch: 32it [00:01, 18.44it/s, loss=0.0177] \n",
      "training 707th epoch: 32it [00:01, 18.42it/s, loss=0.0177] \n",
      "training 708th epoch: 32it [00:01, 18.99it/s, loss=0.0177] \n",
      "training 709th epoch: 32it [00:01, 20.74it/s, loss=0.0177] \n",
      "training 710th epoch: 32it [00:01, 20.58it/s, loss=0.0177] \n",
      "training 711th epoch: 32it [00:01, 20.86it/s, loss=0.0176] \n",
      "training 712th epoch: 32it [00:01, 20.84it/s, loss=0.0176] \n",
      "training 713th epoch: 32it [00:01, 20.41it/s, loss=0.0176] \n",
      "training 714th epoch: 32it [00:01, 20.38it/s, loss=0.0176] \n",
      "training 715th epoch: 32it [00:01, 20.32it/s, loss=0.0175] \n",
      "training 716th epoch: 32it [00:01, 20.33it/s, loss=0.0175] \n",
      "training 717th epoch: 32it [00:01, 20.08it/s, loss=0.0175] \n",
      "training 718th epoch: 32it [00:01, 20.21it/s, loss=0.0175] \n",
      "training 719th epoch: 32it [00:01, 20.12it/s, loss=0.0175] \n",
      "training 720th epoch: 32it [00:01, 20.25it/s, loss=0.0174] \n",
      "training 721th epoch: 32it [00:01, 20.31it/s, loss=0.0174] \n",
      "training 722th epoch: 32it [00:01, 19.88it/s, loss=0.0174] \n",
      "training 723th epoch: 32it [00:01, 20.52it/s, loss=0.0174] \n",
      "training 724th epoch: 32it [00:01, 21.46it/s, loss=0.0173] \n",
      "training 725th epoch: 32it [00:01, 21.73it/s, loss=0.0173] \n",
      "training 726th epoch: 32it [00:01, 20.39it/s, loss=0.0173] \n",
      "training 727th epoch: 32it [00:01, 18.77it/s, loss=0.0173] \n",
      "training 728th epoch: 32it [00:01, 18.76it/s, loss=0.0173] \n",
      "training 729th epoch: 32it [00:01, 18.53it/s, loss=0.0172] \n",
      "training 730th epoch: 32it [00:01, 18.62it/s, loss=0.0172] \n",
      "training 731th epoch: 32it [00:01, 18.56it/s, loss=0.0172] \n",
      "training 732th epoch: 32it [00:01, 18.77it/s, loss=0.0172] \n",
      "training 733th epoch: 32it [00:01, 18.55it/s, loss=0.0172] \n",
      "training 734th epoch: 32it [00:01, 18.62it/s, loss=0.0171] \n",
      "training 735th epoch: 32it [00:01, 18.54it/s, loss=0.0171] \n",
      "training 736th epoch: 32it [00:01, 18.64it/s, loss=0.0171] \n",
      "training 737th epoch: 32it [00:01, 18.66it/s, loss=0.0171] \n",
      "training 738th epoch: 32it [00:01, 18.56it/s, loss=0.017]  \n",
      "training 739th epoch: 32it [00:01, 18.62it/s, loss=0.017]  \n",
      "training 740th epoch: 32it [00:01, 18.68it/s, loss=0.017]  \n",
      "training 741th epoch: 32it [00:01, 18.83it/s, loss=0.017]  \n",
      "training 742th epoch: 32it [00:01, 18.86it/s, loss=0.017]  \n",
      "training 743th epoch: 32it [00:01, 18.75it/s, loss=0.0169] \n",
      "training 744th epoch: 32it [00:01, 18.35it/s, loss=0.0169] \n",
      "training 745th epoch: 32it [00:01, 18.76it/s, loss=0.0169] \n",
      "training 746th epoch: 32it [00:01, 18.75it/s, loss=0.0169] \n",
      "training 747th epoch: 32it [00:01, 18.47it/s, loss=0.0169] \n",
      "training 748th epoch: 32it [00:01, 18.66it/s, loss=0.0168] \n",
      "training 749th epoch: 32it [00:01, 18.68it/s, loss=0.0168] \n",
      "training 750th epoch: 32it [00:01, 18.83it/s, loss=0.0168] \n",
      "training 751th epoch: 32it [00:01, 18.82it/s, loss=0.0168] \n",
      "training 752th epoch: 32it [00:01, 20.65it/s, loss=0.0168] \n",
      "training 753th epoch: 32it [00:01, 20.64it/s, loss=0.0167] \n",
      "training 754th epoch: 32it [00:01, 19.81it/s, loss=0.0167] \n",
      "training 755th epoch: 32it [00:01, 22.92it/s, loss=0.0167] \n",
      "training 756th epoch: 32it [00:01, 24.19it/s, loss=0.0167] \n",
      "training 757th epoch: 32it [00:01, 24.08it/s, loss=0.0167] \n",
      "training 758th epoch: 32it [00:01, 24.36it/s, loss=0.0166] \n",
      "training 759th epoch: 32it [00:01, 24.39it/s, loss=0.0166] \n",
      "training 760th epoch: 32it [00:01, 24.47it/s, loss=0.0166] \n",
      "training 761th epoch: 32it [00:01, 24.45it/s, loss=0.0166] \n",
      "training 762th epoch: 32it [00:01, 22.07it/s, loss=0.0166] \n",
      "training 763th epoch: 32it [00:01, 20.67it/s, loss=0.0165] \n",
      "training 764th epoch: 32it [00:01, 20.88it/s, loss=0.0165] \n",
      "training 765th epoch: 32it [00:01, 20.51it/s, loss=0.0165] \n",
      "training 766th epoch: 32it [00:01, 20.69it/s, loss=0.0165] \n",
      "training 767th epoch: 32it [00:01, 20.73it/s, loss=0.0165] \n",
      "training 768th epoch: 32it [00:01, 19.94it/s, loss=0.0164] \n",
      "training 769th epoch: 32it [00:01, 19.41it/s, loss=0.0164] \n",
      "training 770th epoch: 32it [00:01, 19.23it/s, loss=0.0164] \n",
      "training 771th epoch: 32it [00:01, 19.17it/s, loss=0.0164] \n",
      "training 772th epoch: 32it [00:01, 19.49it/s, loss=0.0164] \n",
      "training 773th epoch: 32it [00:01, 19.58it/s, loss=0.0163] \n",
      "training 774th epoch: 32it [00:01, 19.70it/s, loss=0.0163] \n",
      "training 775th epoch: 32it [00:01, 19.28it/s, loss=0.0163] \n",
      "training 776th epoch: 32it [00:01, 20.40it/s, loss=0.0163] \n",
      "training 777th epoch: 32it [00:01, 20.50it/s, loss=0.0163] \n",
      "training 778th epoch: 32it [00:01, 20.65it/s, loss=0.0162] \n",
      "training 779th epoch: 32it [00:01, 20.59it/s, loss=0.0162] \n",
      "training 780th epoch: 32it [00:01, 20.69it/s, loss=0.0162] \n",
      "training 781th epoch: 32it [00:01, 20.43it/s, loss=0.0162] \n",
      "training 782th epoch: 32it [00:01, 20.57it/s, loss=0.0162] \n",
      "training 783th epoch: 32it [00:01, 20.74it/s, loss=0.0161] \n",
      "training 784th epoch: 32it [00:01, 20.88it/s, loss=0.0161] \n",
      "training 785th epoch: 32it [00:01, 20.82it/s, loss=0.0161] \n",
      "training 786th epoch: 32it [00:01, 20.68it/s, loss=0.0161] \n",
      "training 787th epoch: 32it [00:01, 20.71it/s, loss=0.0161] \n",
      "training 788th epoch: 32it [00:01, 20.75it/s, loss=0.0161] \n",
      "training 789th epoch: 32it [00:01, 20.87it/s, loss=0.016]  \n",
      "training 790th epoch: 32it [00:01, 20.56it/s, loss=0.016]  \n",
      "training 791th epoch: 32it [00:01, 20.67it/s, loss=0.016]  \n",
      "training 792th epoch: 32it [00:01, 20.76it/s, loss=0.016]  \n",
      "training 793th epoch: 32it [00:01, 20.88it/s, loss=0.016]  \n",
      "training 794th epoch: 32it [00:01, 20.18it/s, loss=0.0159] \n",
      "training 795th epoch: 32it [00:01, 18.35it/s, loss=0.0159] \n",
      "training 796th epoch: 32it [00:01, 18.03it/s, loss=0.0159] \n",
      "training 797th epoch: 32it [00:01, 18.20it/s, loss=0.0159] \n",
      "training 798th epoch: 32it [00:01, 18.39it/s, loss=0.0159] \n",
      "training 799th epoch: 32it [00:01, 18.71it/s, loss=0.0158] \n",
      "training 800th epoch: 32it [00:01, 18.60it/s, loss=0.0158] \n",
      "training 801th epoch: 32it [00:01, 19.10it/s, loss=0.0158] \n",
      "training 802th epoch: 32it [00:01, 20.51it/s, loss=0.0158] \n",
      "training 803th epoch: 32it [00:01, 20.59it/s, loss=0.0158] \n",
      "training 804th epoch: 32it [00:01, 20.10it/s, loss=0.0158] \n",
      "training 805th epoch: 32it [00:01, 20.72it/s, loss=0.0157] \n",
      "training 806th epoch: 32it [00:01, 20.86it/s, loss=0.0157] \n",
      "training 807th epoch: 32it [00:01, 20.75it/s, loss=0.0157] \n",
      "training 808th epoch: 32it [00:01, 20.38it/s, loss=0.0157] \n",
      "training 809th epoch: 32it [00:01, 19.51it/s, loss=0.0157] \n",
      "training 810th epoch: 32it [00:01, 20.06it/s, loss=0.0156] \n",
      "training 811th epoch: 32it [00:01, 21.52it/s, loss=0.0156] \n",
      "training 812th epoch: 32it [00:01, 21.29it/s, loss=0.0156] \n",
      "training 813th epoch: 32it [00:01, 20.82it/s, loss=0.0156] \n",
      "training 814th epoch: 32it [00:01, 20.61it/s, loss=0.0156] \n",
      "training 815th epoch: 32it [00:01, 20.72it/s, loss=0.0156] \n",
      "training 816th epoch: 32it [00:01, 20.90it/s, loss=0.0155] \n",
      "training 817th epoch: 32it [00:01, 20.67it/s, loss=0.0155] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 818th epoch: 32it [00:01, 18.22it/s, loss=0.0155] \n",
      "training 819th epoch: 32it [00:01, 17.40it/s, loss=0.0155] \n",
      "training 820th epoch: 32it [00:01, 17.88it/s, loss=0.0155] \n",
      "training 821th epoch: 32it [00:01, 18.07it/s, loss=0.0155] \n",
      "training 822th epoch: 32it [00:01, 18.06it/s, loss=0.0154] \n",
      "training 823th epoch: 32it [00:01, 17.81it/s, loss=0.0154] \n",
      "training 824th epoch: 32it [00:01, 19.53it/s, loss=0.0154] \n",
      "training 825th epoch: 32it [00:01, 20.30it/s, loss=0.0154] \n",
      "training 826th epoch: 32it [00:01, 20.52it/s, loss=0.0154] \n",
      "training 827th epoch: 32it [00:01, 20.23it/s, loss=0.0154] \n",
      "training 828th epoch: 32it [00:01, 20.18it/s, loss=0.0153] \n",
      "training 829th epoch: 32it [00:01, 20.14it/s, loss=0.0153] \n",
      "training 830th epoch: 32it [00:01, 20.37it/s, loss=0.0153] \n",
      "training 831th epoch: 32it [00:01, 20.72it/s, loss=0.0153] \n",
      "training 832th epoch: 32it [00:01, 21.09it/s, loss=0.0153] \n",
      "training 833th epoch: 32it [00:01, 20.79it/s, loss=0.0152] \n",
      "training 834th epoch: 32it [00:01, 20.91it/s, loss=0.0152] \n",
      "training 835th epoch: 32it [00:01, 20.41it/s, loss=0.0152] \n",
      "training 836th epoch: 32it [00:01, 20.37it/s, loss=0.0152] \n",
      "training 837th epoch: 32it [00:01, 20.57it/s, loss=0.0152] \n",
      "training 838th epoch: 32it [00:01, 20.69it/s, loss=0.0152] \n",
      "training 839th epoch: 32it [00:01, 20.82it/s, loss=0.0151] \n",
      "training 840th epoch: 32it [00:01, 20.36it/s, loss=0.0151] \n",
      "training 841th epoch: 32it [00:01, 18.95it/s, loss=0.0151] \n",
      "training 842th epoch: 32it [00:01, 20.54it/s, loss=0.0151] \n",
      "training 843th epoch: 32it [00:01, 20.25it/s, loss=0.0151] \n",
      "training 844th epoch: 32it [00:01, 20.36it/s, loss=0.0151] \n",
      "training 845th epoch: 32it [00:01, 18.31it/s, loss=0.015]  \n",
      "training 846th epoch: 32it [00:01, 18.10it/s, loss=0.015]  \n",
      "training 847th epoch: 32it [00:01, 18.15it/s, loss=0.015]  \n",
      "training 848th epoch: 32it [00:01, 18.76it/s, loss=0.015]  \n",
      "training 849th epoch: 32it [00:01, 17.90it/s, loss=0.015]  \n",
      "training 850th epoch: 32it [00:01, 18.17it/s, loss=0.015]  \n",
      "training 851th epoch: 32it [00:01, 18.29it/s, loss=0.0149] \n",
      "training 852th epoch: 32it [00:01, 18.58it/s, loss=0.0149] \n",
      "training 853th epoch: 32it [00:01, 17.97it/s, loss=0.0149] \n",
      "training 854th epoch: 32it [00:01, 18.33it/s, loss=0.0149] \n",
      "training 855th epoch: 32it [00:01, 17.83it/s, loss=0.0149] \n",
      "training 856th epoch: 32it [00:01, 17.99it/s, loss=0.0149] \n",
      "training 857th epoch: 32it [00:01, 18.10it/s, loss=0.0148] \n",
      "training 858th epoch: 32it [00:01, 18.27it/s, loss=0.0148] \n",
      "training 859th epoch: 32it [00:01, 17.95it/s, loss=0.0148] \n",
      "training 860th epoch: 32it [00:01, 18.53it/s, loss=0.0148] \n",
      "training 861th epoch: 32it [00:01, 17.88it/s, loss=0.0148] \n",
      "training 862th epoch: 32it [00:01, 18.06it/s, loss=0.0148] \n",
      "training 863th epoch: 32it [00:01, 18.22it/s, loss=0.0148] \n",
      "training 864th epoch: 32it [00:01, 17.59it/s, loss=0.0147] \n",
      "training 865th epoch: 32it [00:01, 17.99it/s, loss=0.0147] \n",
      "training 866th epoch: 32it [00:01, 18.18it/s, loss=0.0147] \n",
      "training 867th epoch: 32it [00:01, 19.75it/s, loss=0.0147] \n",
      "training 868th epoch: 32it [00:01, 20.82it/s, loss=0.0147] \n",
      "training 869th epoch: 32it [00:01, 20.66it/s, loss=0.0147] \n",
      "training 870th epoch: 32it [00:01, 20.59it/s, loss=0.0146] \n",
      "training 871th epoch: 32it [00:01, 20.66it/s, loss=0.0146] \n",
      "training 872th epoch: 32it [00:01, 20.65it/s, loss=0.0146] \n",
      "training 873th epoch: 32it [00:01, 20.55it/s, loss=0.0146] \n",
      "training 874th epoch: 32it [00:01, 20.57it/s, loss=0.0146] \n",
      "training 875th epoch: 32it [00:01, 20.25it/s, loss=0.0146] \n",
      "training 876th epoch: 32it [00:01, 20.72it/s, loss=0.0145] \n",
      "training 877th epoch: 32it [00:01, 20.62it/s, loss=0.0145] \n",
      "training 878th epoch: 32it [00:01, 20.71it/s, loss=0.0145] \n",
      "training 879th epoch: 32it [00:01, 20.86it/s, loss=0.0145] \n",
      "training 880th epoch: 32it [00:01, 20.45it/s, loss=0.0145] \n",
      "training 881th epoch: 32it [00:01, 20.86it/s, loss=0.0145] \n",
      "training 882th epoch: 32it [00:01, 20.61it/s, loss=0.0145] \n",
      "training 883th epoch: 32it [00:01, 20.65it/s, loss=0.0144] \n",
      "training 884th epoch: 32it [00:01, 20.75it/s, loss=0.0144] \n",
      "training 885th epoch: 32it [00:01, 20.65it/s, loss=0.0144] \n",
      "training 886th epoch: 32it [00:01, 20.72it/s, loss=0.0144] \n",
      "training 887th epoch: 32it [00:01, 22.88it/s, loss=0.0144] \n",
      "training 888th epoch: 32it [00:01, 24.35it/s, loss=0.0144] \n",
      "training 889th epoch: 32it [00:01, 24.56it/s, loss=0.0143] \n",
      "training 890th epoch: 32it [00:01, 24.14it/s, loss=0.0143] \n",
      "training 891th epoch: 32it [00:01, 23.39it/s, loss=0.0143] \n",
      "training 892th epoch: 32it [00:01, 20.57it/s, loss=0.0143] \n",
      "training 893th epoch: 32it [00:01, 20.45it/s, loss=0.0143] \n",
      "training 894th epoch: 32it [00:01, 20.68it/s, loss=0.0143] \n",
      "training 895th epoch: 32it [00:01, 20.32it/s, loss=0.0143] \n",
      "training 896th epoch: 32it [00:01, 20.68it/s, loss=0.0142] \n",
      "training 897th epoch: 32it [00:01, 20.64it/s, loss=0.0142] \n",
      "training 898th epoch: 32it [00:01, 20.43it/s, loss=0.0142] \n",
      "training 899th epoch: 32it [00:01, 20.75it/s, loss=0.0142] \n",
      "training 900th epoch: 32it [00:01, 20.82it/s, loss=0.0142] \n",
      "training 901th epoch: 32it [00:01, 20.55it/s, loss=0.0142] \n",
      "training 902th epoch: 32it [00:01, 20.25it/s, loss=0.0142] \n",
      "training 903th epoch: 32it [00:01, 20.55it/s, loss=0.0141] \n",
      "training 904th epoch: 32it [00:01, 20.24it/s, loss=0.0141] \n",
      "training 905th epoch: 32it [00:01, 20.40it/s, loss=0.0141] \n",
      "training 906th epoch: 32it [00:01, 20.13it/s, loss=0.0141] \n",
      "training 907th epoch: 32it [00:01, 20.17it/s, loss=0.0141] \n",
      "training 908th epoch: 32it [00:01, 20.66it/s, loss=0.0141] \n",
      "training 909th epoch: 32it [00:01, 20.61it/s, loss=0.0141] \n",
      "training 910th epoch: 32it [00:01, 20.61it/s, loss=0.014]  \n",
      "training 911th epoch: 32it [00:01, 20.31it/s, loss=0.014]  \n",
      "training 912th epoch: 32it [00:01, 20.70it/s, loss=0.014]  \n",
      "training 913th epoch: 32it [00:01, 20.62it/s, loss=0.014]  \n",
      "training 914th epoch: 32it [00:01, 20.38it/s, loss=0.014]  \n",
      "training 915th epoch: 32it [00:01, 20.49it/s, loss=0.014]  \n",
      "training 916th epoch: 32it [00:01, 20.76it/s, loss=0.0139] \n",
      "training 917th epoch: 32it [00:01, 20.81it/s, loss=0.0139] \n",
      "training 918th epoch: 32it [00:01, 20.84it/s, loss=0.0139] \n",
      "training 919th epoch: 32it [00:01, 20.73it/s, loss=0.0139] \n",
      "training 920th epoch: 32it [00:01, 20.74it/s, loss=0.0139] \n",
      "training 921th epoch: 32it [00:01, 20.49it/s, loss=0.0139] \n",
      "training 922th epoch: 32it [00:01, 20.72it/s, loss=0.0139] \n",
      "training 923th epoch: 32it [00:01, 20.63it/s, loss=0.0138] \n",
      "training 924th epoch: 32it [00:01, 20.39it/s, loss=0.0138] \n",
      "training 925th epoch: 32it [00:01, 20.09it/s, loss=0.0138] \n",
      "training 926th epoch: 32it [00:01, 18.63it/s, loss=0.0138] \n",
      "training 927th epoch: 32it [00:01, 18.14it/s, loss=0.0138] \n",
      "training 928th epoch: 32it [00:01, 18.45it/s, loss=0.0138] \n",
      "training 929th epoch: 32it [00:01, 18.56it/s, loss=0.0138] \n",
      "training 930th epoch: 32it [00:01, 18.19it/s, loss=0.0138] \n",
      "training 931th epoch: 32it [00:01, 18.86it/s, loss=0.0137] \n",
      "training 932th epoch: 32it [00:01, 19.29it/s, loss=0.0137] \n",
      "training 933th epoch: 32it [00:01, 18.87it/s, loss=0.0137] \n",
      "training 934th epoch: 32it [00:01, 18.92it/s, loss=0.0137] \n",
      "training 935th epoch: 32it [00:01, 18.74it/s, loss=0.0137] \n",
      "training 936th epoch: 32it [00:01, 19.14it/s, loss=0.0137] \n",
      "training 937th epoch: 32it [00:01, 18.50it/s, loss=0.0137] \n",
      "training 938th epoch: 32it [00:01, 18.47it/s, loss=0.0136] \n",
      "training 939th epoch: 32it [00:01, 18.61it/s, loss=0.0136] \n",
      "training 940th epoch: 32it [00:01, 18.55it/s, loss=0.0136] \n",
      "training 941th epoch: 32it [00:01, 18.64it/s, loss=0.0136] \n",
      "training 942th epoch: 32it [00:01, 19.16it/s, loss=0.0136] \n",
      "training 943th epoch: 32it [00:01, 18.63it/s, loss=0.0136] \n",
      "training 944th epoch: 32it [00:01, 18.89it/s, loss=0.0136] \n",
      "training 945th epoch: 32it [00:01, 18.99it/s, loss=0.0135] \n",
      "training 946th epoch: 32it [00:01, 18.50it/s, loss=0.0135] \n",
      "training 947th epoch: 32it [00:01, 18.42it/s, loss=0.0135] \n",
      "training 948th epoch: 32it [00:01, 18.61it/s, loss=0.0135] \n",
      "training 949th epoch: 32it [00:01, 18.48it/s, loss=0.0135] \n",
      "training 950th epoch: 32it [00:01, 18.56it/s, loss=0.0135] \n",
      "training 951th epoch: 32it [00:01, 18.59it/s, loss=0.0135] \n",
      "training 952th epoch: 32it [00:01, 18.46it/s, loss=0.0134] \n",
      "training 953th epoch: 32it [00:01, 18.68it/s, loss=0.0134] \n",
      "training 954th epoch: 32it [00:01, 18.94it/s, loss=0.0134] \n",
      "training 955th epoch: 32it [00:01, 18.83it/s, loss=0.0134] \n",
      "training 956th epoch: 32it [00:01, 20.31it/s, loss=0.0134] \n",
      "training 957th epoch: 32it [00:01, 20.70it/s, loss=0.0134] \n",
      "training 958th epoch: 32it [00:01, 20.30it/s, loss=0.0134] \n",
      "training 959th epoch: 32it [00:01, 20.75it/s, loss=0.0134] \n",
      "training 960th epoch: 32it [00:01, 20.40it/s, loss=0.0133] \n",
      "training 961th epoch: 32it [00:01, 20.51it/s, loss=0.0133] \n",
      "training 962th epoch: 32it [00:01, 20.49it/s, loss=0.0133] \n",
      "training 963th epoch: 32it [00:01, 20.40it/s, loss=0.0133] \n",
      "training 964th epoch: 32it [00:01, 20.37it/s, loss=0.0133] \n",
      "training 965th epoch: 32it [00:01, 20.48it/s, loss=0.0133] \n",
      "training 966th epoch: 32it [00:01, 20.43it/s, loss=0.0133] \n",
      "training 967th epoch: 32it [00:01, 20.96it/s, loss=0.0132] \n",
      "training 968th epoch: 32it [00:01, 20.51it/s, loss=0.0132] \n",
      "training 969th epoch: 32it [00:01, 20.51it/s, loss=0.0132] \n",
      "training 970th epoch: 32it [00:01, 20.77it/s, loss=0.0132] \n",
      "training 971th epoch: 32it [00:01, 20.64it/s, loss=0.0132] \n",
      "training 972th epoch: 32it [00:01, 20.56it/s, loss=0.0132] \n",
      "training 973th epoch: 32it [00:01, 20.47it/s, loss=0.0132] \n",
      "training 974th epoch: 32it [00:01, 20.67it/s, loss=0.0132] \n",
      "training 975th epoch: 32it [00:01, 20.81it/s, loss=0.0131] \n",
      "training 976th epoch: 32it [00:01, 20.19it/s, loss=0.0131] \n",
      "training 977th epoch: 32it [00:01, 18.25it/s, loss=0.0131] \n",
      "training 978th epoch: 32it [00:01, 18.14it/s, loss=0.0131] \n",
      "training 979th epoch: 32it [00:01, 18.06it/s, loss=0.0131] \n",
      "training 980th epoch: 32it [00:01, 18.25it/s, loss=0.0131] \n",
      "training 981th epoch: 32it [00:01, 18.67it/s, loss=0.0131] \n",
      "training 982th epoch: 32it [00:01, 18.51it/s, loss=0.0131] \n",
      "training 983th epoch: 32it [00:01, 18.68it/s, loss=0.013]  \n",
      "training 984th epoch: 32it [00:01, 18.51it/s, loss=0.013]  \n",
      "training 985th epoch: 32it [00:01, 19.10it/s, loss=0.013]  \n",
      "training 986th epoch: 32it [00:01, 20.42it/s, loss=0.013]  \n",
      "training 987th epoch: 32it [00:01, 20.22it/s, loss=0.013]  \n",
      "training 988th epoch: 32it [00:01, 20.48it/s, loss=0.013]  \n",
      "training 989th epoch: 32it [00:01, 19.90it/s, loss=0.013]  \n",
      "training 990th epoch: 32it [00:01, 20.52it/s, loss=0.013]  \n",
      "training 991th epoch: 32it [00:01, 20.36it/s, loss=0.0129] \n",
      "training 992th epoch: 32it [00:01, 20.55it/s, loss=0.0129] \n",
      "training 993th epoch: 32it [00:01, 20.62it/s, loss=0.0129] \n",
      "training 994th epoch: 32it [00:01, 20.36it/s, loss=0.0129] \n",
      "training 995th epoch: 32it [00:01, 20.25it/s, loss=0.0129] \n",
      "training 996th epoch: 32it [00:01, 20.02it/s, loss=0.0129] \n",
      "training 997th epoch: 32it [00:01, 20.32it/s, loss=0.0129] \n",
      "training 998th epoch: 32it [00:01, 20.71it/s, loss=0.0129] \n",
      "training 999th epoch: 32it [00:01, 20.62it/s, loss=0.0128] \n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, dataloader):\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # get inputs and labels\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            print('** inputs[0]: {}'.format(inputs[0]))\n",
    "            print('** labels[0]: {}'.format(labels[0]))\n",
    "\n",
    "            # transpose inputs and labels\n",
    "            inputs = inputs.transpose(1, 0)\n",
    "            labels = labels.transpose(1, 0)\n",
    "\n",
    "            # initialize hidden for encoder\n",
    "            batch_size = inputs.size()[1]\n",
    "            encoder_hidden = encoder.initHidden(batch_size)\n",
    "            encoder_outputs = torch.zeros(MAX_LENGTH, batch_size, hidden_size, device=device)\n",
    "            encoder_outputs_ = torch.zeros(MAX_LENGTH, batch_size, hidden_size, device=device)\n",
    "            \n",
    "            # encoding\n",
    "            for j, inp in enumerate(inputs):\n",
    "                inp = inp.unsqueeze(0)\n",
    "                encoder_output, encoder_hidden = encoder(inp, encoder_hidden)\n",
    "                #print('** encoder_output_{}: {}'.format(i, encoder_output.shape))\n",
    "                encoder_outputs[j] = encoder_output\n",
    "                encoder_outputs_[j] += encoder_output[:,0]\n",
    "\n",
    "            print('** encoder_outputs.sum() VS encoder_outputs.sum(): {:.4f} VS {:.4f}'.format(encoder_outputs.sum(), encoder_outputs_.sum()))\n",
    "            \n",
    "            # initialize hidden for decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_inputs = torch.tensor([[bos]*batch_size], device=device)\n",
    "\n",
    "            pred = []\n",
    "            # decoding\n",
    "            for inp in labels:\n",
    "                #inp = inp.unsqueeze(0)\n",
    "\n",
    "                print('** inp: {}'.format(inp[0]))\n",
    "                print('** decoder_inputs shape: {}'.format(decoder_inputs.shape))\n",
    "                if use_attention:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_inputs, decoder_hidden, encoder_outputs_)\n",
    "                    print('** decoder_output shape: {}'.format(decoder_output.shape))\n",
    "                    decoder_output = decoder_output.argmax(1)\n",
    "                    print('** decoder_output[0]: {}'.format(decoder_output[0]))\n",
    "                    pred.append(decoder_output.cpu().numpy())\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_inputs, decoder_hidden)\n",
    "                    print('** decoder_output shape: {}'.format(decoder_output.shape))\n",
    "                    decoder_output = decoder_output.argmax(1)\n",
    "                    print('** decoder_output[0]: {}'.format(decoder_output[0]))\n",
    "                    pred.append(decoder_output.cpu().numpy())\n",
    "\n",
    "                decoder_inputs = inp.unsqueeze(0)\n",
    "\n",
    "            # re-transpose for validation\n",
    "            inputs = inputs.transpose(1, 0)\n",
    "            labels = labels.transpose(1, 0).cpu().numpy()\n",
    "            #return\n",
    "\n",
    "            # stack-up prediction for validation\n",
    "            pred = np.stack(pred, axis=1)\n",
    "            accuracy = (pred == labels).astype(np.int).mean()\n",
    "            print('{} VS {} → {:.4f}'.format(labels.shape, pred.shape, accuracy))\n",
    "            \n",
    "        return labels, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** inputs[0]: tensor([18, 21, 16, 12, 21, 12, 18, 16, 28, 28, 28, 23,  1,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([13, 10, 15, 19, 10, 19, 13, 15,  3,  3,  3,  8,  1,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -2726.1765 VS 852.4473\n",
      "** inp: 13\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 13\n",
      "** inp: 10\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 10\n",
      "** inp: 15\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 15\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 19\n",
      "** inp: 10\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 10\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 3\n",
      "** inp: 13\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 3\n",
      "** inp: 15\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 8\n",
      "** inp: 3\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 3\n",
      "** inp: 3\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 8\n",
      "** inp: 3\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 8\n",
      "** inp: 8\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 8\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7098\n",
      "** inputs[0]: tensor([10,  9, 12,  3, 20,  6, 14, 26, 25,  6, 24, 28,  1,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([21, 22, 19, 28, 11, 25, 17,  5,  6, 25,  7,  3,  1,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -2317.2146 VS -5136.2944\n",
      "** inp: 21\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 21\n",
      "** inp: 22\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 11\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 11\n",
      "** inp: 28\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 11\n",
      "** inp: 11\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 21\n",
      "** inp: 25\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 21\n",
      "** inp: 17\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 21\n",
      "** inp: 5\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 6\n",
      "** inp: 6\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 7\n",
      "** inp: 25\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 21\n",
      "** inp: 7\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 7\n",
      "** inp: 3\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 3\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7366\n",
      "** inputs[0]: tensor([28, 11, 13,  3, 12, 25, 12, 25,  9,  1,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([ 3, 20, 18, 28, 19,  6, 19,  6, 22,  1,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -3006.8442 VS -710.6979\n",
      "** inp: 3\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 3\n",
      "** inp: 20\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 20\n",
      "** inp: 18\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 6\n",
      "** inp: 28\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 20\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 20\n",
      "** inp: 6\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 22\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 22\n",
      "** inp: 6\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 22\n",
      "** inp: 22\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 22\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7098\n",
      "** inputs[0]: tensor([11,  5, 17, 27, 26, 26, 15, 26, 12,  1,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([20, 26, 14,  4,  5,  5, 16,  5, 19,  1,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -2314.6895 VS -5818.2842\n",
      "** inp: 20\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 20\n",
      "** inp: 26\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 26\n",
      "** inp: 14\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 4\n",
      "** inp: 4\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 4\n",
      "** inp: 5\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 19\n",
      "** inp: 5\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 19\n",
      "** inp: 16\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 16\n",
      "** inp: 5\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 19\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 23\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7054\n",
      "** inputs[0]: tensor([22,  7, 16, 26,  9,  5,  4, 10, 15,  3, 27,  1,  2,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([ 9, 24, 15,  5, 22, 26, 27, 21, 16, 28,  4,  1,  2,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -2706.4817 VS -3211.1824\n",
      "** inp: 9\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 9\n",
      "** inp: 24\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 24\n",
      "** inp: 15\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 5\n",
      "** inp: 5\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 9\n",
      "** inp: 22\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 26\n",
      "** inp: 26\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 26\n",
      "** inp: 27\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 16\n",
      "** inp: 21\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 4\n",
      "** inp: 16\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 16\n",
      "** inp: 28\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 4\n",
      "** inp: 4\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7121\n",
      "** inputs[0]: tensor([18, 23, 12, 24,  6,  1,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([13,  8, 19,  7, 25,  1,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -1925.5475 VS 843.2606\n",
      "** inp: 13\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 13\n",
      "** inp: 8\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 8\n",
      "** inp: 19\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 26\n",
      "** inp: 7\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 7\n",
      "** inp: 25\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 13\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 10\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 32])\n",
      "** decoder_output shape: torch.Size([32, 29])\n",
      "** decoder_output[0]: 2\n",
      "(32, 14) VS (32, 14) → 0.7232\n",
      "** inputs[0]: tensor([ 4, 24, 21, 24, 15,  1,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** labels[0]: tensor([27,  7, 10,  7, 16,  1,  2,  2,  2,  2,  2,  2,  2,  2],\n",
      "       device='cuda:0')\n",
      "** encoder_outputs.sum() VS encoder_outputs.sum(): -700.4343 VS 264.2885\n",
      "** inp: 27\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 27\n",
      "** inp: 7\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 7\n",
      "** inp: 10\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 6\n",
      "** inp: 7\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 27\n",
      "** inp: 16\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 16\n",
      "** inp: 1\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 1\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "** inp: 2\n",
      "** decoder_inputs shape: torch.Size([1, 8])\n",
      "** decoder_output shape: torch.Size([8, 29])\n",
      "** decoder_output[0]: 2\n",
      "(8, 14) VS (8, 14) → 0.8036\n"
     ]
    }
   ],
   "source": [
    "labels, pred = validate(encoder, decoder, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 16, 14, 22, 20, 12, 19, 12, 11,  1,  2,  2,  2,  2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 16, 14, 14, 19, 11, 19, 16, 11, 16,  2,  2,  2,  2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(sequence):\n",
    "    # some reserved words\n",
    "    bos = 0\n",
    "    eos = 1\n",
    "    pad = 2\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # make input tensor\n",
    "        seq = list(map(lambda s: a2i[s], sequence))\n",
    "        n_pad = MAX_LENGTH - len(seq) - 2\n",
    "        inputs = [bos] + seq + [eos]# + [pad]*n_pad\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long).to(device)\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        print(inputs.shape)\n",
    "\n",
    "\n",
    "        # transpose inputs and labels\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        #labels = labels.transpose(1, 0)\n",
    "\n",
    "        # initialize hidden for encoder\n",
    "        batch_size = inputs.size()[1]    \n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, batch_size, hidden_size, device=device)\n",
    "        print('** encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "        # encoding\n",
    "        for i, inp in enumerate(inputs):\n",
    "            inp = inp.unsqueeze(0)\n",
    "            encoder_output, encoder_hidden = encoder(inp, encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_output[:, 0]\n",
    "\n",
    "        # initialize hidden for decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        pred = []\n",
    "        #target = [bos]\n",
    "        #target = torch.tensor(target).to(device)\n",
    "\n",
    "        print('** encoder_outputs sum: {}'.format(encoder_outputs.sum()))\n",
    "        print('** encoder_hidden sum: {}'.format(encoder_hidden.sum()))\n",
    "\n",
    "        #target = target.unsqueeze(0)\n",
    "        decoder_input = torch.tensor([[bos]], device=device)  # SOS\n",
    "        # decoding\n",
    "        for i in range(MAX_LENGTH):\n",
    "\n",
    "            if i == 7:\n",
    "                break\n",
    "\n",
    "            if use_attention:\n",
    "                print('** decoder_input: {}'.format(decoder_input.shape))\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                #decoder_output = decoder_output.argmax(1)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                print('** topi: {}'.format(topi.data.item()))\n",
    "                if topi.item() == eos:\n",
    "                    pred.append('</s>')\n",
    "                    break\n",
    "                else:\n",
    "                    pred.append(i2a[topi.item()])\n",
    "\n",
    "                decoder_input = topi.detach()\n",
    "        #else:\n",
    "        #    decoder_output, decoder_hidden = decoder(inp, decoder_hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "** encoder_outputs: torch.Size([15, 1, 256])\n",
      "** encoder_outputs sum: -29.475393295288086\n",
      "** encoder_hidden sum: -8.637638092041016\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n",
      "** decoder_input: torch.Size([1, 1])\n",
      "** topi: 0\n"
     ]
    }
   ],
   "source": [
    "predict_sequence('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_output, encoder_hidden = encoder(inputs, encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = labels.size()[1]\n",
    "# decoder_hidden = decoder.initHidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder(inputs, decoder_hidden, encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5]), torch.Size([3]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "inp = torch.randn(3, 5, requires_grad=True)\n",
    "lbl = torch.tensor([1, 0, 4])\n",
    "o = criterion(m(inp), lbl)\n",
    "inp.shape, lbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7729, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 15]) torch.Size([32, 15])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    inputs, labels = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataloader, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    # zero_grad for encoder/decoder optimizer\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # get inputs and labels\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # transpose inputs and labels\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        labels = labels.transpose(1, 0)\n",
    "        \n",
    "        # initialize hidden for encoder\n",
    "        batch_size = inputs.size()[1]\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_outputs = torch.zeros(15, 256, device=device)\n",
    "        \n",
    "        # encoding\n",
    "        for ip in inputs:\n",
    "            ip = ip.unsqueeze(0)\n",
    "            encoder_output, encoder_hidden = encoder(ip, encoder_hidden)\n",
    "            print('** encoder_output_{}: {}'.format(i, encoder_output.shape))\n",
    "            #print('** encoder_hidden_{}: {}'.format(i, encoder_hidden.shape))\n",
    "            \n",
    "        # initialize hidden for decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        loss = 0\n",
    "    \n",
    "        # decoding\n",
    "        for inp in labels:\n",
    "            #inp = inp.unsqueeze(0)\n",
    "            decoder_output, decoder_hidden = decoder(inp.unsqueeze(0), decoder_hidden)\n",
    "            \n",
    "            decoder_output = decoder_output.squeeze(0)\n",
    "            \n",
    "            #print('** decoder_hidden_{}: {}'.format(i, decoder_hidden.shape))\n",
    "            loss_it = criterion(decoder_output, inp)\n",
    "            loss += criterion(decoder_output, inp)\n",
    "            #print('** label vs pred: {} vs {} → {:.4f}'.format(inp.shape, decoder_output.shape, loss_it))\n",
    "        \n",
    "        #encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "        #decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "        #print('encoder, decoder: {:.4f} {:.4f}'.format(encoder_sum, decoder_sum))\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        #encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "        #decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "        #print('total loss after backward: {:.4f}'.format(loss))\n",
    "        #print('encoder, decoder: {:.4f} {:.4f}'.format(encoder_sum, decoder_sum))\n",
    "        \n",
    "        # update encoder/decoder\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        #encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "        #decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "        #print('total loss after step: {:.4f}'.format(loss))\n",
    "        #print('encoder, decoder: {:.4f} {:.4f}'.format(encoder_sum, decoder_sum))\n",
    "        print('{}th iteration → total loss after update: {:.4f}'.format(i, loss))\n",
    "        #print('-------------------------------------')\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, dataloader):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # get inputs and labels\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # transpose inputs and labels\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        labels = labels.transpose(1, 0)\n",
    "        \n",
    "        # initialize hidden for encoder\n",
    "        batch_size = inputs.size()[1]\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        \n",
    "        # encoding\n",
    "        for ip in inputs:\n",
    "            ip = ip.unsqueeze(0)\n",
    "            encoder_output, encoder_hidden = encoder(ip, encoder_hidden)\n",
    "            \n",
    "        # initialize hidden for decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        pred = []\n",
    "        # decoding\n",
    "        for inp in labels:\n",
    "            decoder_output, decoder_hidden = decoder(inp.unsqueeze(0), decoder_hidden)\n",
    "            decoder_output = decoder_output.squeeze(0)            \n",
    "            decoder_output = decoder_output.argmax(1)\n",
    "            pred.append(decoder_output.cpu().numpy())\n",
    "            #print('** decoder_output: {}'.format(decoder_output.cpu().numpy().shape))\n",
    "            \n",
    "        # re-transpose for validation\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        labels = labels.transpose(1, 0).cpu().numpy()\n",
    "        \n",
    "        # stack-up prediction for validation\n",
    "        pred = np.stack(pred, axis=1)\n",
    "        \n",
    "        # calculate batch-accurac y\n",
    "        accuracy = (pred == labels).astype(np.int).mean()\n",
    "        print('{} VS {} → {:.4f}'.format(labels.shape, pred.shape, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate(encoder, decoder, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n",
      "** hidden: torch.Size([1, 32, 256])\n",
      "** encoder_output_0: torch.Size([1, 32, 256])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'encoder_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a0614719763c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-2b51e65c3d20>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, dataloader, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#inp = inp.unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/publish/env_pub/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'encoder_output'"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.from_numpy(np.random.randint(0, 29, (32)))\n",
    "# out = torch.from_numpy(np.random.random((32, 29)))\n",
    "# criterion(out, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *************** NMT ***************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0+cu101'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 다운로드 → https://download.pytorch.org/tutorial/data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고 → https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # SOS 와 EOS 포함\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유니 코드 문자열을 일반 ASCII로 변환하십시오.\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소문자, 다듬기, 그리고 문자가 아닌 문자 제거\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # 파일을 읽고 줄로 분리\n",
    "    lines = open('../data/attention-ntm/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # 모든 줄을 쌍으로 분리하고 정규화\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # 쌍을 뒤집고, Lang 인스턴스 생성\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "['j ai honte de mon corps .', 'i m ashamed of my body .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without Attention\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        print('DecoderRNN_INPUT: input: {} → {:.4f}'.format(input.shape, input.sum()))\n",
    "        print('DecoderRNN_INPUT: hidden: {} → {:.4f}'.format(hidden.shape, hidden.sum()))\n",
    "        \n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        print('** output: {} → {:.4f}'.format(output.shape, output.sum()))\n",
    "        \n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        print('** gru-output: {} → {:.4f}'.format(output.shape, output.sum()))\n",
    "        print('** gru-hidden: {} → {:.4f}'.format(hidden.shape, hidden.sum()))\n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        \n",
    "        print('DecoderRNN_OUTPUT: output: {} → {:.4f}'.format(output.shape, output.sum()))\n",
    "        print('DecoderRNN_OUTPUT: hidden: {} → {:.4f}'.format(hidden.shape, hidden.sum()))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with attention\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        print('AttnDecoderRNN_INPUT: input → embedded: {} → {}'.format(input.shape, embedded.shape))\n",
    "        print('AttnDecoderRNN_INPUT: hidden: {}'.format(hidden.shape))\n",
    "        print('AttnDecoderRNN_INPUT: encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "        \n",
    "        #print('** embedded[0]: {}'.format(embedded[0].shape))\n",
    "        #print('** hidden[0]: {}'.format(hidden[0].shape))\n",
    "        #print('** concatenate embedded[0] and hidden[0]: {}'.format(torch.cat((embedded[0], hidden[0]), 1).shape))\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        #print('** attn_weights: {}'.format(attn_weights.shape))\n",
    "        #print('** attn_weights: {}'.format(attn_weights))\n",
    "        #print('** attn_weights.sum(): {}'.format(attn_weights.sum().shape))\n",
    "        #print('** attn_weights.sum(): {:.4f}'.format(attn_weights.sum()))\n",
    "        \n",
    "        #print('** attn_weights unsqueezed: {}'.format(attn_weights.unsqueeze(0).shape))\n",
    "        #print('** encoder_outputs unsqueezed: {}'.format(encoder_outputs.unsqueeze(0).shape))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        #print('** attn_applied: {}'.format(attn_applied.shape))\n",
    "        #print('** attn_applied[0]: {}'.format(attn_applied[0].shape))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #print('** gru-output: {}'.format(output.shape))\n",
    "        #print('** gru-hidden: {}'.format(hidden.shape))\n",
    "\n",
    "        #print('** output: {}'.format(output[0].shape))\n",
    "        #print('** self.out: {}'.format(self.out))\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        print('AttnDecoderRNN_OUTPUT: {}'.format(output.shape))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH, with_attention=True):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    print('** input_tensor[0]: {}'.format(input_tensor[0]))\n",
    "    print('** target_tensor[0]: {}'.format(target_tensor[0]))\n",
    "    return\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    print('encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        print('{} - {}'.format(ei, encoder_output[0, 0].shape))\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    print('decoder_hidden: {}'.format(decoder_hidden.shape))\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing 포함: 목표를 다음 입력으로 전달\n",
    "        for di in range(target_length):\n",
    "            if with_attention:\n",
    "                print('# {}th decoding'.format(di))\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                print('train:decoder_attention_{}: {}'.format(di, decoder_attention.shape))\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            \n",
    "\n",
    "    else:\n",
    "        # Teacher forcing 미포함: 자신의 예측을 다음 입력으로 사용\n",
    "        for di in range(target_length):\n",
    "            if with_attention:\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                print('train:decoder_attention_{}: {}'.format(di, decoder_attention.shape))\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # 입력으로 사용할 부분을 히스토리에서 분리\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01, with_attention=True):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # print_every 마다 초기화\n",
    "    plot_loss_total = 0  # plot_every 마다 초기화\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        print(input_tensor.shape, target_tensor.shape)\n",
    "        print(input_tensor)\n",
    "        print(target_tensor)\n",
    "        print('----------------------')\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion, with_attention=with_attention)\n",
    "        return\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # 주기적인 간격에 이 locator가 tick을 설정\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 256\n",
    "# encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "# decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Input validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1]) torch.Size([8, 1])\n",
      "tensor([[ 123],\n",
      "        [ 245],\n",
      "        [ 124],\n",
      "        [ 246],\n",
      "        [ 963],\n",
      "        [  34],\n",
      "        [ 101],\n",
      "        [2194],\n",
      "        [   5],\n",
      "        [   1]], device='cuda:0')\n",
      "tensor([[ 77],\n",
      "        [ 78],\n",
      "        [147],\n",
      "        [ 22],\n",
      "        [986],\n",
      "        [588],\n",
      "        [  4],\n",
      "        [  1]], device='cuda:0')\n",
      "----------------------\n",
      "** input_tensor[0]: tensor([123], device='cuda:0')\n",
      "** target_tensor[0]: tensor([77], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, decoder1, 75000, print_every=5000, with_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([1, 1]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0686, 0.1335, 0.0626, 0.0603, 0.0611, 0.1706, 0.0989, 0.1053, 0.1473,\n",
      "         0.0919]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.1012, 0.1292, 0.0967, 0.0339, 0.0354, 0.0609, 0.1461, 0.1979, 0.0466,\n",
      "         0.1521]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.1087, 0.0949, 0.0921, 0.1498, 0.1004, 0.1222, 0.1379, 0.0641, 0.0732,\n",
      "         0.0568]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0820, 0.1084, 0.0732, 0.1596, 0.1375, 0.1399, 0.1155, 0.0548, 0.0725,\n",
      "         0.0566]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0877, 0.1126, 0.1402, 0.0841, 0.0808, 0.1144, 0.0742, 0.0642, 0.1119,\n",
      "         0.1299]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0817, 0.1592, 0.0996, 0.0300, 0.0527, 0.1788, 0.2290, 0.0573, 0.0516,\n",
      "         0.0601]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0734, 0.0784, 0.1388, 0.1089, 0.0898, 0.1060, 0.0431, 0.0508, 0.2422,\n",
      "         0.0685]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0897, 0.0796, 0.1038, 0.1083, 0.1236, 0.0908, 0.0404, 0.0815, 0.2121,\n",
      "         0.0705]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0677, 0.1122, 0.0989, 0.0805, 0.2018, 0.1609, 0.0741, 0.0505, 0.1025,\n",
      "         0.0510]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0678, 0.0835, 0.1067, 0.0788, 0.0884, 0.0733, 0.1775, 0.1112, 0.1139,\n",
      "         0.0990]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4b216434e0>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # colorbar로 그림 설정\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # 축 설정\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # 매 틱마다 라벨 보여주기\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([1, 1]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0618, 0.1636, 0.0533, 0.0789, 0.0618, 0.1500, 0.0878, 0.1144, 0.1363,\n",
      "         0.0921]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0965, 0.1081, 0.0944, 0.0375, 0.0357, 0.0828, 0.1353, 0.1679, 0.0539,\n",
      "         0.1879]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0477, 0.0793, 0.0998, 0.1223, 0.0445, 0.0813, 0.0403, 0.1080, 0.3197,\n",
      "         0.0572]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "AttnDecoderRNN_INPUT: input → embedded: torch.Size([]) → torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: hidden: torch.Size([1, 1, 256])\n",
      "AttnDecoderRNN_INPUT: encoder_outputs: torch.Size([10, 256])\n",
      "** embedded[0]: torch.Size([1, 256])\n",
      "** hidden[0]: torch.Size([1, 256])\n",
      "** concatenate embedded[0] and hidden[0]: torch.Size([1, 512])\n",
      "** attn_weights: torch.Size([1, 10])\n",
      "** attn_weights: tensor([[0.0941, 0.0949, 0.0970, 0.1319, 0.1118, 0.1242, 0.1313, 0.0688, 0.0912,\n",
      "         0.0547]], device='cuda:0')\n",
      "** attn_weights.sum(): torch.Size([])\n",
      "** attn_weights.sum(): 1.0000\n",
      "** attn_weights unsqueezed: torch.Size([1, 1, 10])\n",
      "** encoder_outputs unsqueezed: torch.Size([1, 10, 256])\n",
      "** attn_applied: torch.Size([1, 1, 256])\n",
      "** attn_applied[0]: torch.Size([1, 256])\n",
      "** output: torch.Size([1, 512])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 1, 256])\n",
      "** gru-output: torch.Size([1, 1, 256])\n",
      "** gru-hidden: torch.Size([1, 1, 256])\n",
      "** output: torch.Size([1, 256])\n",
      "** self.out: Linear(in_features=256, out_features=2803, bias=True)\n",
      "AttnDecoderRNN_OUTPUT: torch.Size([1, 2803])\n",
      "input = elle a cinq ans de moins que moi .\n",
      "output = tennis number rich <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/heavy_data/jkfirst/workspace/git/publish/env_pub/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/heavy_data/jkfirst/workspace/git/publish/env_pub/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "#evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "#evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "#evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from random import choice, randrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ToyDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Inspired from https://talbaumel.github.io/blog/attention/\n",
    "    \"\"\"\n",
    "    def __init__(self, min_length=5, max_length=20, type='train'):\n",
    "        self.SOS = \"<s>\"  \n",
    "        self.EOS = \"</s>\" \n",
    "        self.characters = list(\"abcd\")\n",
    "        self.int2char = list(self.characters)\n",
    "        self.char2int = {c: i+3 for i, c in enumerate(self.characters)}\n",
    "        self.VOCAB_SIZE = len(self.characters)\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        if type=='train':\n",
    "            self.set = [self._sample() for _ in range(3000)]\n",
    "        else:\n",
    "            self.set = [self._sample() for _ in range(300)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.set)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.set[item]\n",
    "\n",
    "    def _sample(self):\n",
    "        random_length = randrange(self.min_length, self.max_length)# Pick a random length\n",
    "        random_char_list = [choice(self.characters[:-1]) for _ in range(random_length)]  # Pick random chars\n",
    "        random_string = ''.join(random_char_list)\n",
    "        print(random_string)\n",
    "        a = np.array([self.char2int.get(x) for x in random_string])\n",
    "        b = np.array([self.char2int.get(x) for x in random_string[::-1]] + [2]) # Return the random string and its reverse\n",
    "        x = np.zeros((random_length, self.VOCAB_SIZE))\n",
    "        x[np.arange(random_length), a-3] = 1\n",
    "        return x, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bccccbbaacccccccccb\n",
      "caaabcbbbcca\n",
      "bababacbabacaacc\n",
      "aacaabaccbc\n",
      "ccaaababbbbcbb\n",
      "bbccaabbbc\n",
      "aacaaabcaabacabab\n",
      "cabcb\n",
      "ccabac\n",
      "caaabacababacaca\n",
      "cabab\n",
      "cacccbac\n",
      "baaabaaaaacccccacab\n",
      "acbacccb\n",
      "cccaabaaabcca\n",
      "cbabababba\n",
      "acbcabccac\n",
      "aabbbbabbccbab\n",
      "babcacbbbbcbc\n",
      "bbbbaaca\n",
      "acacaaabbbacacbcbbc\n",
      "abcbcbaaaa\n",
      "abccacaabacbb\n",
      "bcbabbbccb\n",
      "babcbaaa\n",
      "cbbcaccbcccb\n",
      "acacabbbbc\n",
      "cccabccbbaacccacb\n",
      "aabbbaabbabaccbbc\n",
      "cbacaccb\n",
      "bcbbccaabcacabbbbac\n",
      "abccabbbcacc\n",
      "cacaac\n",
      "abaaaaccbaccbccaca\n",
      "ccabccaabbababaab\n",
      "ccbbccccccabcacac\n",
      "abbcbccaaabcbc\n",
      "acabaaaa\n",
      "cbbcacbcbaaabcc\n",
      "bccbccabba\n",
      "bcacbbcaaabb\n",
      "baacaacacaaacca\n",
      "abaaccbbcbabaacc\n",
      "aaabccbaaac\n",
      "abcbacbca\n",
      "abbabbabcabbbbaaa\n",
      "cccbab\n",
      "cbbbb\n",
      "bbbaababacabbaaaa\n",
      "bbaabacbb\n",
      "cabcbabcbaaccc\n",
      "aababab\n",
      "bacbcacaac\n",
      "cbaaaabbcccabccaaba\n",
      "acacaacbaabccbcaab\n",
      "ccabacbcbababcba\n",
      "bbacaac\n",
      "baccbababcabbac\n",
      "bbcabcbbcaac\n",
      "baabbaaccccaaccb\n",
      "ccaabacbaccbba\n",
      "cbccbabccbbbaccba\n",
      "bccaacacbcccaacb\n",
      "cababbcbabbcaaa\n",
      "bacbaaabbaacc\n",
      "aacaabacbbabbbaab\n",
      "bcabccaacabcaca\n",
      "bcbbccabcbaacaa\n",
      "abaabbbacbbaaaba\n",
      "caaaabacabbacaca\n",
      "abbacb\n",
      "cccbcbbbacbabc\n",
      "abbaaccbcaabbcb\n",
      "ccbca\n",
      "bcababcacbcacbab\n",
      "cacbbaabbbaa\n",
      "bbaaaacbb\n",
      "babcbc\n",
      "cbacccacbcccbcac\n",
      "bbaabccacc\n",
      "bcaaccbba\n",
      "acaaabc\n",
      "bcbbbabcbba\n",
      "cbbbacaaca\n",
      "cccbbabcbaccabcca\n",
      "bbbabbbccbbacc\n",
      "ccbbcabaaccaa\n",
      "bcabacaaccaabcbbbba\n",
      "abaca\n",
      "aaaacacbab\n",
      "cbcbcaaaccccabcabab\n",
      "acaaaaacbbacbaacac\n",
      "accccaaabcccccbba\n",
      "cabcaaccbaabacabcb\n",
      "bacaacaabbacabcb\n",
      "cababcaccbaa\n",
      "bcbbaaaaac\n",
      "abcab\n",
      "cbabbbba\n",
      "aacbb\n",
      "bbaaaaaccbbaaca\n",
      "cbbaabbcccac\n",
      "bcaabbcbbacc\n",
      "cbbcaabbc\n",
      "caaaaabbcc\n",
      "abacbbabca\n",
      "aabcabababaaac\n",
      "acbacaba\n",
      "bbbaccbabacba\n",
      "bcbcabccaaaaaa\n",
      "aaaccccc\n",
      "abcaaa\n",
      "cacabbcbc\n",
      "caccbbaccb\n",
      "acbcaccaaaccba\n",
      "ccbbbacbacbcaccabca\n",
      "aacabcbcccacbbcca\n",
      "caacccbaaabcba\n",
      "acabbabc\n",
      "bbcbaabbc\n",
      "baaccbbcbb\n",
      "acabbabacbab\n",
      "acbbcbacabbcabcbcb\n",
      "cccaabaaab\n",
      "abacccaca\n",
      "abcbaccabbcccabbb\n",
      "cbccbbc\n",
      "baaabbca\n",
      "cccab\n",
      "aabccbaacc\n",
      "bbbbbbaac\n",
      "cbabababbcbb\n",
      "ccbabcccc\n",
      "cbacbabccabbaccb\n",
      "cbccbcba\n",
      "abcccabbccaacabbbbb\n",
      "bbcabcaccaabbcbcca\n",
      "ababbabaccbabcccaba\n",
      "bcbbccbaaaaaacbab\n",
      "ccacaabac\n",
      "babbabbbcacaaabac\n",
      "bcaabbbaacbcbbc\n",
      "aaaaabaaccaccac\n",
      "aacccacc\n",
      "bbbaacacaaccacbc\n",
      "caaabcbcbaccc\n",
      "acacacacaaa\n",
      "cacbcababc\n",
      "bbbccacccbbbaaac\n",
      "abbabccbba\n",
      "caacaacca\n",
      "cbccabbab\n",
      "bbabbccccbcaaccc\n",
      "acaabbbbaccabaab\n",
      "cabca\n",
      "abcbabbbcacbcbbcabb\n",
      "aabcacbccbccacca\n",
      "abccbbbcaccbccaacb\n",
      "bbaabbacbacabcbaab\n",
      "cacacacabccca\n",
      "cbbbcbbbaab\n",
      "abaccabcbbcb\n",
      "abcbaabcbccc\n",
      "ccaacacbc\n",
      "bbcaabbccccbbaaac\n",
      "aaacaaaaaacbcab\n",
      "bcbca\n",
      "abcaabccaabbacc\n",
      "acaccaab\n",
      "aaccbcbbba\n",
      "bbcbabbbcaa\n",
      "abcacaacccacc\n",
      "bbcbbbbbccac\n",
      "cbcacbb\n",
      "bbbabaacb\n",
      "cacacabcbbbabcbacbc\n",
      "bacccbbbc\n",
      "cabbbbccaacbcaaaacc\n",
      "bbbaa\n",
      "acbabc\n",
      "acbcbbbcacbbacb\n",
      "aaaabbcaabcabcccab\n",
      "cbbbacbacbbabb\n",
      "aabaa\n",
      "cccaccc\n",
      "aaaacccba\n",
      "bccaccccabbbabbacac\n",
      "abcbbaacbbbcabcbbba\n",
      "bcbccbcabbbbabbca\n",
      "cbabcaaccabbbc\n",
      "cccccbaacacb\n",
      "bacaccbacabacaac\n",
      "aaaccccbcabacaaa\n",
      "bcbabbbbccbcacaca\n",
      "baababbaaacbcbcacb\n",
      "bcabcabbccccc\n",
      "accbbaa\n",
      "ccccc\n",
      "babaaaabaa\n",
      "abccc\n",
      "cacbbbc\n",
      "abcbbbbcbbb\n",
      "cbbccaabac\n",
      "acbbbbb\n",
      "baabbbaacaabbaba\n",
      "cbbccaccbccbccca\n",
      "aaccabaaccbabaa\n",
      "cbcbcbabcababcbabba\n",
      "cbcaaa\n",
      "aabaababbbcabaabbb\n",
      "bccbbccbcacbcacaccb\n",
      "cbacbacacaccc\n",
      "bcbaaacbcccb\n",
      "bbaab\n",
      "bbbaccbbbb\n",
      "cbbbacacacbbbcbbaa\n",
      "aababbccabaacbccbc\n",
      "abababccbcbabba\n",
      "bbaccccbb\n",
      "bcabbbbaaccbaab\n",
      "ccabcbab\n",
      "aabbacbabbabbacacb\n",
      "cacbaacbabcbcabccab\n",
      "cabaaaabcbcacbaabcc\n",
      "cccab\n",
      "abcbc\n",
      "aaacaaaccacca\n",
      "bcabbcaabaa\n",
      "bacabaacccaa\n",
      "abbbabaaabbcbac\n",
      "acaccbcacabccbacab\n",
      "cacaabc\n",
      "acabccbba\n",
      "aababbcacbbaca\n",
      "cabbcabccbcbbbb\n",
      "bccbbabacbccca\n",
      "babba\n",
      "acabbcbbaca\n",
      "abaaaabbabacbbb\n",
      "bacbcbbbcaaababb\n",
      "cbbbacacbabccbcac\n",
      "bbcbcbaabbacbcabaa\n",
      "acbbacccabcabccbbc\n",
      "abbaaabb\n",
      "aaacbbacaacbcacbacb\n",
      "baacacaacbcba\n",
      "ccabbbbccb\n",
      "ccacabccaaabba\n",
      "abbaaccacccbcccbbb\n",
      "bcbcbccbbcbcaabccb\n",
      "abbccbabcccccbca\n",
      "bbccbbaaccac\n",
      "cabba\n",
      "acbabcc\n",
      "ccaaccbabbaaccaccaa\n",
      "ccacbacbcabbbbcb\n",
      "bcacabcaaacbbbacabc\n",
      "bbbccabcc\n",
      "cbbacc\n",
      "accacbbcb\n",
      "aabbccabbcba\n",
      "bbbcbbaacabbcb\n",
      "cbbcacabaabaacbab\n",
      "aaabcccabacbacccb\n",
      "cbbbc\n",
      "bcaaaccc\n",
      "baabcaaabaac\n",
      "aabbbbacbcbbbca\n",
      "baabacbacbaacac\n",
      "bccacccbbcacbbcc\n",
      "bbbbcaccbbaababbbc\n",
      "aaaabcbbbcaabacc\n",
      "accbbababaaccabbbb\n",
      "acccabbbabbabc\n",
      "acaabbabcaaabaabac\n",
      "ccbacbaccabab\n",
      "abacaa\n",
      "acacccbacab\n",
      "acbacacbbbbbcca\n",
      "cacbacbbcbaa\n",
      "aabbbabcbaccccaca\n",
      "abcabcaaabaaaacccab\n",
      "cabccabc\n",
      "bbbbbaaaacc\n",
      "aabcabccaabbcbacaac\n",
      "ccbaaacbcbcca\n",
      "baaaaabc\n",
      "ccaabccbb\n",
      "aababbbcbaabb\n",
      "ccaacc\n",
      "cbccbba\n",
      "bccacaccccabbacaa\n",
      "acabcacca\n",
      "cabbcbba\n",
      "cbacbccbbbaacbac\n",
      "aabbac\n",
      "aaabbc\n",
      "bcacbacbcccbba\n",
      "aabccacaaacacbcbc\n",
      "baacbbca\n",
      "caacaabaabbc\n",
      "caaababaacababbabc\n",
      "ccbcbcc\n",
      "caaaabbbccabc\n",
      "ccbcabbbc\n",
      "bcabaacaccaaaa\n",
      "aacac\n",
      "cbbaabaccabbcbac\n",
      "ccccccabaaac\n",
      "aaccbababccbacbbab\n",
      "aaaabbbba\n",
      "cbbacacab\n",
      "cccccabacacabbbbaca\n",
      "ccaaac\n",
      "bbccca\n",
      "baccaabccabaa\n",
      "aabcbbccaaacaacabba\n",
      "cccccaccccaaacba\n",
      "ccccb\n",
      "bcaabbbccbcca\n",
      "bbabcc\n",
      "bbcba\n",
      "caccaabac\n",
      "acacacbcbaca\n",
      "bbbcaaccbbaacba\n",
      "bbcccacbacacb\n",
      "acccacabbbabbb\n",
      "bbcaaab\n",
      "bacbbc\n",
      "bbaacab\n",
      "ccaccccbb\n",
      "bbaaabbacbbcbb\n",
      "bcbcc\n",
      "bbbabaabba\n",
      "cabbbac\n",
      "abaaab\n",
      "abbccbcaccbbccc\n",
      "baacacabbaaabbcbca\n",
      "cbbbbccaaccb\n",
      "bbbbbbcaabcaccc\n",
      "cacbabb\n",
      "cbbaaaaacbcc\n",
      "bbbbbaacbbbbbaaacab\n",
      "aabcbaacacbaba\n",
      "bcaabc\n",
      "abababbcbbcaa\n",
      "abbacbacba\n",
      "aaccaa\n",
      "abcbbcabcaa\n",
      "aaaaa\n",
      "bacbaaacccac\n",
      "bbbccca\n",
      "cabbcccaaaaacbbbb\n",
      "baacbacccbbcab\n",
      "ccccabacabacbbcba\n",
      "caaabbaccc\n",
      "abaccbcbaacabcca\n",
      "cbcbb\n",
      "cbccca\n",
      "abbcbabcbbba\n",
      "aaccccbcbaccacaba\n",
      "acaaccacaaa\n",
      "abcbcbabcccacaabba\n",
      "cbcabccabaabaabcc\n",
      "babaaabcbababacbbbb\n",
      "bbcbbcbcabcacaca\n",
      "cbcbcbbaaab\n",
      "babbab\n",
      "cababbbab\n",
      "bbabbaccabaaac\n",
      "cccbbbccbacabbbbacb\n",
      "ccbaabcaaccbbaaacbb\n",
      "babaabcccbb\n",
      "cacbcbbc\n",
      "babccacccbc\n",
      "baccaacab\n",
      "bccccabbaba\n",
      "aaaccccbbaa\n",
      "ccbca\n",
      "ccacbbcabaacac\n",
      "aacbbca\n",
      "bccbbaaccbcaccc\n",
      "cbaaaccacbabacabbbc\n",
      "cbaacc\n",
      "abbaccbb\n",
      "ccccbbaaaacbbaa\n",
      "aaacbaba\n",
      "abbbcacbcb\n",
      "acbabaaacccac\n",
      "aacbccacbbabacbaccc\n",
      "bbcaabbcabbbbb\n",
      "bcaaab\n",
      "bcaaba\n",
      "cabac\n",
      "cbccb\n",
      "baccaccbaa\n",
      "cbcbaaabccbcb\n",
      "cbcbcabacbba\n",
      "baaaa\n",
      "aaaac\n",
      "bcabbacaaacc\n",
      "bbbccbcbabcccbcc\n",
      "caaccacbac\n",
      "cabcbcbcaabbbacaaca\n",
      "bacabccbb\n",
      "bbcbaabcccbabc\n",
      "caaabbcccbcccccacaa\n",
      "cbcacaaaa\n",
      "ccbcccccabacbccbc\n",
      "aacaaabcca\n",
      "bbbbacccabcabc\n",
      "bbbbccacc\n",
      "aabbbab\n",
      "bcccab\n",
      "baaba\n",
      "accacabaccabccb\n",
      "bbacabcbacbca\n",
      "caaacbb\n",
      "bccaaabcabc\n",
      "bbabbccab\n",
      "bcbababacbbcbc\n",
      "bcabac\n",
      "baabbcaccbaaaaa\n",
      "cccababa\n",
      "cacbbbaaacaacbbc\n",
      "bbcba\n",
      "abaabbbccb\n",
      "caccbbbabaab\n",
      "babbacaaab\n",
      "cccccbcbccbaccaabb\n",
      "cacab\n",
      "acacacbbbaccccab\n",
      "bcbbcababacccbac\n",
      "bcaaab\n",
      "caccbcbacbccacabaab\n",
      "ccaaabbab\n",
      "aaaacccaacabababcaa\n",
      "babacbbcbbcc\n",
      "abbbbcbaacbcbacccb\n",
      "cabccacbabaccaac\n",
      "ccacbcacbb\n",
      "aaabbb\n",
      "babccbcccbccabbcaba\n",
      "cabccb\n",
      "babbbaaa\n",
      "bacbc\n",
      "ccbaaba\n",
      "acccacaacbaaaa\n",
      "bacabcabbb\n",
      "abaabbccbacabcb\n",
      "acbbbbccab\n",
      "abbbcabcabcbca\n",
      "aaaccbaccabaaa\n",
      "bbccccbacccacacba\n",
      "cbccacacbccab\n",
      "bccccbabaccaba\n",
      "abaabbc\n",
      "caacbcbcabbabacab\n",
      "ccbccaa\n",
      "babccccababacbccaaa\n",
      "bbabacb\n",
      "cbacabc\n",
      "babbcbaa\n",
      "aaaacbcccbcbca\n",
      "bccbbaaacc\n",
      "bccab\n",
      "aabccca\n",
      "caaabc\n",
      "bbcaabcaacac\n",
      "bbbcacaaaccaaacbbaa\n",
      "bbbbaccaabaa\n",
      "aacacbccbacac\n",
      "ccbabbcbcbbbc\n",
      "cabacac\n",
      "bbaccacbbaacaa\n",
      "bcacabcabccbacabcc\n",
      "acacacaaaaacaabab\n",
      "acabccbabcc\n",
      "accbbbaacacc\n",
      "bababacaccbcbab\n",
      "cbbabac\n",
      "cabaaabccccb\n",
      "cacaaccabcbbcaaaaa\n",
      "ccacbbaabcbc\n",
      "cbabccac\n",
      "bacbcbcaababb\n",
      "abaccbcaaac\n",
      "ccbbcbbacbc\n",
      "bacccba\n",
      "aacababcabcbccb\n",
      "acbbabc\n",
      "ccaabaaabaaaaabc\n",
      "aaacba\n",
      "acbabbbccbbb\n",
      "caabcacab\n",
      "accaaccaba\n",
      "ccbcac\n",
      "cacbbcccbbaab\n",
      "acbcbcbbbbb\n",
      "cacbbcbaaacaaccaca\n",
      "babaccca\n",
      "bbcbbc\n",
      "caabcacccccab\n",
      "ccccaabbbababccacca\n",
      "ccaabaaabc\n",
      "cacbbbbacaacbbccb\n",
      "bbbaaacbbaca\n",
      "bcbbbbaa\n",
      "bcaaaba\n",
      "aaacacc\n",
      "bbaccbac\n",
      "bcbcbaabbcbca\n",
      "bbbacbcaa\n",
      "ccaaacbbccaaabaab\n",
      "cbcabcbabaabaabaacb\n",
      "ccaaccac\n",
      "baacccaacbbbacbbbab\n",
      "aaacbbabbabaacab\n",
      "aaccaaacaaaabcaabcb\n",
      "cbabbbcbaaaccacbacb\n",
      "abaabbaaccb\n",
      "aaaabaaba\n",
      "caccacaaaa\n",
      "bbbbbbcacacca\n",
      "aabbaccaccbba\n",
      "cbbcbcacbbbbc\n",
      "cbaaaa\n",
      "cbbbcabbaaaaa\n",
      "bcbabbbcbabcc\n",
      "acacbcbbbcb\n",
      "cbbabbcabcbcacba\n",
      "cabcbcccbbbbccb\n",
      "caaaaabbb\n",
      "abccac\n",
      "cccbabcbabbcc\n",
      "babacaa\n",
      "aaabcb\n",
      "bccacbaaacbbb\n",
      "bbabbbbbbbc\n",
      "babaaacac\n",
      "bbbbbaabccabcacc\n",
      "aababccacbccacc\n",
      "bbcca\n",
      "cbabbbccaabbc\n",
      "cccaaaaccbbaccac\n",
      "ccacbacbbacabbbbaaa\n",
      "acbacccaaab\n",
      "cbccbacabaac\n",
      "baaab\n",
      "aabccaaabcb\n",
      "bbbbbcaa\n",
      "abbcbaababbaccb\n",
      "aaabba\n",
      "bacabac\n",
      "cbacbbaacaaccacbb\n",
      "abbccacc\n",
      "babab\n",
      "babbcccbbccbcb\n",
      "aacbcb\n",
      "cbbaaccbc\n",
      "ccabbcabacbaccaa\n",
      "bbabccbcccbabccc\n",
      "acccaccccbcacbcacbb\n",
      "abaabca\n",
      "bacbcbcaaaaaac\n",
      "aabbbbaaacb\n",
      "cacaaccabaacbacabc\n",
      "babcabc\n",
      "bbbaaccaaaacb\n",
      "bcababa\n",
      "ccaabbacaacbbc\n",
      "abbaaccb\n",
      "baabacca\n",
      "aabaccbc\n",
      "caabaaababcc\n",
      "abaacbaacbbcb\n",
      "cbaacaab\n",
      "ccacaaaac\n",
      "bbcacaababcaabac\n",
      "cacccbabbbcccbacab\n",
      "cbbacabbabaaaabc\n",
      "bacabcccbac\n",
      "aabaacaccbacbcbaa\n",
      "baabcccabb\n",
      "caaccbcacbcbccacca\n",
      "acbcbc\n",
      "cbaac\n",
      "babababacbbaabb\n",
      "aaabbaacbc\n",
      "abaaca\n",
      "caaabaa\n",
      "bbccccc\n",
      "ccaacbac\n",
      "aaaabacbcb\n",
      "bbcccabaacca\n",
      "ccbccacc\n",
      "abaabbbcbb\n",
      "cacbcaaabcc\n",
      "cbbccac\n",
      "bbbccacbcccccb\n",
      "abbbcab\n",
      "babaccccaaacbbcc\n",
      "baccbaccccbcaab\n",
      "caccccaca\n",
      "abcacacacc\n",
      "bacaabcccbcacbbc\n",
      "cccacacccccabbc\n",
      "bcaca\n",
      "bacab\n",
      "bcabbabba\n",
      "bbcbbababccbbbbaccb\n",
      "abacbcaccabacacbcbb\n",
      "accabaccabac\n",
      "baacbabc\n",
      "abaabaaaaccbccaac\n",
      "abbaaacbaacb\n",
      "babac\n",
      "abaccabaabaccaba\n",
      "cccaacccaccbccbbc\n",
      "bcccccacaaccab\n",
      "cbbcabcabbbabca\n",
      "bbaaa\n",
      "cbaccca\n",
      "cacaacbcaac\n",
      "bbcccbaaabbabbaaaaa\n",
      "ccacb\n",
      "bbccbacabbccaacabab\n",
      "cbacabcacaacbcbc\n",
      "caaacaaa\n",
      "baabcababbacb\n",
      "caacbbabccccababab\n",
      "bbbabab\n",
      "ccbcbcacbaabaabca\n",
      "caacaaabbc\n",
      "bbcaaacaacccbbbab\n",
      "acccbabcacbcccc\n",
      "caccabccaacabb\n",
      "cbaccccbaacb\n",
      "bcccbcaaaaccabccbbb\n",
      "cabbbaabbaa\n",
      "ccbacabbcbaababc\n",
      "ccccaaabbbbbb\n",
      "cabcababbbacbcaaa\n",
      "abccabacaaabbcaaba\n",
      "aaccbbcc\n",
      "babcbccbbbcacabbcbb\n",
      "cbaccaabcbabccc\n",
      "abbaabc\n",
      "cbbcaca\n",
      "abbcabcbbbbbbcacb\n",
      "ccabcccbbaca\n",
      "abbaaa\n",
      "caabbbabbccacbbab\n",
      "bacaaccaabacbbabcac\n",
      "cbabaabcacaabcac\n",
      "bbbbacbb\n",
      "acacbaaaa\n",
      "bbbabacbbba\n",
      "aabbabaccbcaacacbbc\n",
      "acababaccbaaaa\n",
      "bacaabcccbccaaabcc\n",
      "cbcbbccabcb\n",
      "accccbbbbabab\n",
      "aaababbcbcbabbcbccc\n",
      "babbc\n",
      "bccabcabbb\n",
      "aaccbac\n",
      "aaabcbbbcbcbcbb\n",
      "caabaacbbbcbbb\n",
      "acbacacacaccabbca\n",
      "aaaac\n",
      "abcbcc\n",
      "caccba\n",
      "aabccaaccc\n",
      "cacbbbacbcc\n",
      "bccacaab\n",
      "acbbcccaaccccc\n",
      "bbaaacbcb\n",
      "aaaccbcccbbac\n",
      "acbcbacbacabbbaa\n",
      "acbaaccbbb\n",
      "abbbaaaa\n",
      "bbccccccbaacaabba\n",
      "baaccbbccbcaabcbc\n",
      "cbabc\n",
      "caabccaaabccbb\n",
      "abaaab\n",
      "cbcbacac\n",
      "aacba\n",
      "bcaaabaacaccc\n",
      "cbaccaabcac\n",
      "caaaabccabbacccaba\n",
      "cacaacbac\n",
      "acbcaaacbaba\n",
      "acaaabbbbccaa\n",
      "caccabbbcabaccbbbb\n",
      "caccbccaabc\n",
      "ccbbbcba\n",
      "cbacba\n",
      "abcbabacbbcbbc\n",
      "bbccabcacacaccbcbb\n",
      "cccaccabbccbbaa\n",
      "bcbacabcababbbc\n",
      "aabbbcaabaaabc\n",
      "ccabcabbaaacacbbab\n",
      "caccca\n",
      "caaccb\n",
      "ccbcc\n",
      "ccbacabacbccbbbbc\n",
      "aabababc\n",
      "baaaca\n",
      "aacbcaccccaa\n",
      "bbcaa\n",
      "cbcbabbbcca\n",
      "caaaccbababbaccca\n",
      "aabbcccaccccbbaaa\n",
      "bcbcb\n",
      "caacaccb\n",
      "aaaabbc\n",
      "abbcacaab\n",
      "cccbbaacbbbc\n",
      "cbcbbacbabcb\n",
      "baccccaccaabbcab\n",
      "ccacbcaaacaa\n",
      "acbacacccacb\n",
      "bccca\n",
      "abccbabbbaa\n",
      "caacccaaca\n",
      "acaabbcbacaabaabcb\n",
      "bcaabbcacbcbac\n",
      "baabc\n",
      "caccab\n",
      "cabcccb\n",
      "abbabcaccbabacacb\n",
      "bcabaacbbbbaab\n",
      "bcbbbbbbacacccab\n",
      "caacbcbcaac\n",
      "aacbababb\n",
      "aabcbcaab\n",
      "babccacaaaccbbabb\n",
      "abcaaaaacbabbbaac\n",
      "bcbcbabca\n",
      "abbcccaa\n",
      "ccbbbbccbcabcacab\n",
      "bbaabab\n",
      "ccbaabcacaacaaaa\n",
      "abbacc\n",
      "cbbabcbbccaccaba\n",
      "abbabccaccaaac\n",
      "bbabcbaac\n",
      "baaaccbabcbac\n",
      "aacbbbbabcbabcc\n",
      "cbcabbbbabaacbcb\n",
      "caabcbbbaabaa\n",
      "ccbcbbbbccbc\n",
      "abbacccb\n",
      "acbaca\n",
      "bcabcccbac\n",
      "aabcccbbbcb\n",
      "bbbbcabbcacbba\n",
      "baabcb\n",
      "ccaccaba\n",
      "bacaba\n",
      "aaabababcabacbcaccb\n",
      "aaaabcbbabbbbb\n",
      "bbbabccaacca\n",
      "cabbcbaccaacabcbbac\n",
      "bccbcbabcabcb\n",
      "aaabcccaababcbbbaa\n",
      "acaccbcaaaabb\n",
      "bccacbca\n",
      "bbccacabbbbca\n",
      "cabaabbabbbaaca\n",
      "bcbcaa\n",
      "baacccbaaab\n",
      "bbabbaccbcccaac\n",
      "bcccaababacaac\n",
      "ccaabbacbb\n",
      "baacabacbbaaacbabc\n",
      "baabcaabbcaababca\n",
      "accbbaacaabcbbcbab\n",
      "aababbbcbaa\n",
      "aabba\n",
      "bccbbacccb\n",
      "ccacbbcb\n",
      "bcabbaacaccbacaacc\n",
      "caccaacbbabaacaabab\n",
      "abbbcbc\n",
      "bbabaccabbca\n",
      "acbbbbbcbabccaacaac\n",
      "aabbbcbbacbaa\n",
      "acccbccaacbbc\n",
      "bacccabbbcab\n",
      "abaacbbbaacacbabcaa\n",
      "babaacbbbcbaba\n",
      "aaccccaacbcabbaba\n",
      "bbbbc\n",
      "aabcbaabbbacbaaac\n",
      "bcaccbaacbbcbc\n",
      "bbaacba\n",
      "accccabc\n",
      "bacbbbccbcccbcaac\n",
      "cacaabbbacbbcaaaaa\n",
      "babbbba\n",
      "acccaccaacbcc\n",
      "bcccaabbabcbabab\n",
      "acaccbababbab\n",
      "cccccabcbcbc\n",
      "bbbcaaaabbbacaa\n",
      "bcaaabccabcabac\n",
      "baaaccabaaabab\n",
      "abcbcbbccbabaacb\n",
      "ccbaacc\n",
      "bacccbcabcaacb\n",
      "bacaccbab\n",
      "ccbccccacbbab\n",
      "bccacbabcbb\n",
      "cbcbbaa\n",
      "cbccabbbca\n",
      "aaccbacabbcb\n",
      "bcacaaccbbcacaaab\n",
      "ccabcaa\n",
      "cbaaacabaabaaca\n",
      "bcaaacabcbabcbccbaa\n",
      "abcbaaababbbab\n",
      "cbaacbcaacabbaa\n",
      "cbcabaababaa\n",
      "bcbabacaacacc\n",
      "accbbca\n",
      "babbbbbabccaabbabba\n",
      "bbbacabbbbc\n",
      "bbacccccaa\n",
      "cabba\n",
      "baccaacccaabbbcacbc\n",
      "acaacacaccbccc\n",
      "babbbaaacccbcacaaab\n",
      "baaacbbacbaabbabc\n",
      "bcbcbac\n",
      "aabacaaaacabbbaa\n",
      "cccacc\n",
      "caabab\n",
      "bccccac\n",
      "baacb\n",
      "acbbbacbcc\n",
      "bbccabbcaaabcbba\n",
      "bbccbabcbcbbbcaaaa\n",
      "aaacaaccac\n",
      "baabac\n",
      "bcbbaacaacbc\n",
      "bbaabaaaacccb\n",
      "aabcbabbcccaa\n",
      "aaccbcc\n",
      "bbacbbcb\n",
      "abbbaabccc\n",
      "abaaaacccbbbc\n",
      "cbbbbbcaaacbacaa\n",
      "abcab\n",
      "cabbabccb\n",
      "cbcacbcbacbab\n",
      "bbbcbcbacb\n",
      "babaabbbab\n",
      "bbcccaabacba\n",
      "bbbbacacccca\n",
      "acbaccbbbaccc\n",
      "abbaba\n",
      "cacaabcbbbcbbaba\n",
      "cbbcacbccbccbbbabcb\n",
      "bbcacccccaa\n",
      "cbacca\n",
      "caaaa\n",
      "caacabbbc\n",
      "abaaaa\n",
      "aacbcbbabcccbbb\n",
      "baccbccccc\n",
      "abacacaccabbc\n",
      "caaaaacbbbabab\n",
      "bcabbbacbaccb\n",
      "babacabc\n",
      "bbaaaaaaaabaa\n",
      "babcaa\n",
      "bccccbcaabaabbac\n",
      "bcbacbacaaba\n",
      "baacaabcaccbc\n",
      "baaabaabacbcccccb\n",
      "acabccbbbaaccc\n",
      "cbcccac\n",
      "cbccaabcca\n",
      "aabbaaabb\n",
      "abbabccbbbcbbaca\n",
      "aaaaab\n",
      "acabcccba\n",
      "abbaabcbcbbcbab\n",
      "cabcccaccbacacccbcc\n",
      "ccbbacbaabbb\n",
      "caabbaacbbbacbcbabb\n",
      "cbccccababababccccb\n",
      "cccaca\n",
      "acababccaa\n",
      "cbaccababcaa\n",
      "acabbbccacababb\n",
      "cbbabcbbbb\n",
      "bbabaaacabc\n",
      "bcbaabcbcbb\n",
      "babcabacacc\n",
      "accabbccbbbcccbcba\n",
      "aabcbaacaa\n",
      "bbacbcbbcabcbc\n",
      "cabcabccacbaba\n",
      "acbbbb\n",
      "acbbbccabacc\n",
      "acbbacabac\n",
      "bacbccbacc\n",
      "baccbccaaa\n",
      "aabaaccacaba\n",
      "bcccba\n",
      "aaababbccccababca\n",
      "cacabaab\n",
      "cabbc\n",
      "cbbabaacabbaaababa\n",
      "bcbcbaabcc\n",
      "bcbbcccba\n",
      "abccc\n",
      "bbabaacba\n",
      "aabaaccbc\n",
      "bbbacbba\n",
      "baccacabababccabaa\n",
      "bacaacaaaca\n",
      "aaacbccacbabaa\n",
      "aabbbbcaaacbaccbbc\n",
      "ccccbcacabcca\n",
      "aaacb\n",
      "bbcbaabcaa\n",
      "bccbcaabba\n",
      "bbaabbcabaacabacccc\n",
      "caabcaacbabca\n",
      "abbabbb\n",
      "ccbcacbabcbcbccbaac\n",
      "cabbcbaaacbbcbc\n",
      "bbbabcbc\n",
      "bbabcacacaccca\n",
      "bcaacbcaabacb\n",
      "aacabcaaaababab\n",
      "cbacbabcbacaccbbabb\n",
      "ccbcbabcabca\n",
      "bcabacaaabbc\n",
      "cbbcaaab\n",
      "bcaacbacccab\n",
      "aaabcc\n",
      "bbbabbbbabb\n",
      "accbabcbbabcba\n",
      "ccacacbcb\n",
      "aacabbc\n",
      "acaacc\n",
      "bcccbccac\n",
      "acbcccccbabac\n",
      "accaac\n",
      "cabbcaaccbab\n",
      "baabbbaacbacc\n",
      "cbaacabcbbcca\n",
      "baabaaccaaabb\n",
      "bacaabbac\n",
      "abacbaaaabaa\n",
      "bbcbaaa\n",
      "cabbccaa\n",
      "aacccbacabbb\n",
      "cbcbbab\n",
      "bcacbabbbbca\n",
      "abcbca\n",
      "bcabbbabbac\n",
      "ccbbacbbbaacbacaca\n",
      "bcbaccbcbbb\n",
      "caaba\n",
      "cacababccbaacacba\n",
      "aacbbcabbccbcb\n",
      "bcacaccccbbaaacc\n",
      "ccbaaabaaaccc\n",
      "caacababaaacbaac\n",
      "bbccacca\n",
      "cbbbbbbbcabcaaacab\n",
      "abccc\n",
      "bcaacca\n",
      "ccbaaabaaccaccac\n",
      "cacbbabacac\n",
      "cacccccacccbbcbcaca\n",
      "aaccbcccac\n",
      "bbacbbabbcb\n",
      "cabcaabbccabbcbc\n",
      "ababcccbcacaa\n",
      "bacaaabbbccbccaabca\n",
      "cbcbbcabbbbbbabc\n",
      "ccbaacaacbbb\n",
      "cacbccbabaccacabb\n",
      "babcbaaa\n",
      "acbbcabcbcbccbbcaa\n",
      "babbbc\n",
      "bbcccaacbccabcaa\n",
      "cbcccaaaccaacbcbac\n",
      "abcbbbcacc\n",
      "cbacbaabccca\n",
      "acaabbabb\n",
      "bcccbacaca\n",
      "cbbabbcbcaab\n",
      "abbababbbcccaaa\n",
      "bbccbbcbacbbacac\n",
      "babaaccaaba\n",
      "cbcabaabc\n",
      "acbbcbbacbabaaca\n",
      "cccababacbabbacbc\n",
      "abcbcab\n",
      "baaababbcaa\n",
      "acbbabbb\n",
      "aaaabcaaacb\n",
      "bcbaaaccbbcaabab\n",
      "aabacbcc\n",
      "cbccacccabbababa\n",
      "bcbcbcacb\n",
      "caaaacbcbba\n",
      "cccaaaab\n",
      "cbaaacb\n",
      "bcbcaaaccac\n",
      "baaccccccc\n",
      "bababbbbaaaccbacbc\n",
      "accacaacbb\n",
      "aacabbcb\n",
      "bbbbbcacabcbcccbc\n",
      "acbbaaabccaaacc\n",
      "accccaaabaaacab\n",
      "cccacabbb\n",
      "bacaaaaaccbbbc\n",
      "cbacc\n",
      "ccccaaac\n",
      "baacbaa\n",
      "bcbccc\n",
      "bcbabcbacc\n",
      "aabcabbbacaacc\n",
      "cbbbbaababacbcc\n",
      "bbbcccbccccbcaaa\n",
      "acbccbaacaab\n",
      "cacaaaacbbbc\n",
      "caacbca\n",
      "abcbbccbabcabab\n",
      "abcaccbcbba\n",
      "bccbac\n",
      "cacbcbbcabaaa\n",
      "bcbbcbbcabbaabaabac\n",
      "cababababaccccccca\n",
      "bcbbababcc\n",
      "cbacbacabb\n",
      "bbbcbcbabcb\n",
      "babcabccccbcbacac\n",
      "aabcbcaab\n",
      "aaaacbcc\n",
      "cbbbbbccabcabbbccc\n",
      "cccbb\n",
      "bcabcab\n",
      "bbccacbbcbabcb\n",
      "cbcccaa\n",
      "cacbabcbcab\n",
      "caabbbcbbccbacccaaa\n",
      "bbbcbcabbacc\n",
      "bbacccaa\n",
      "baabc\n",
      "bcacaaabcbbca\n",
      "aaacc\n",
      "cacbbbabbcccabc\n",
      "cacbcb\n",
      "ababbaacacbbb\n",
      "acbcbcccacba\n",
      "acababbaaabaaca\n",
      "cabbc\n",
      "cccaabaaccaaa\n",
      "aaabaabc\n",
      "aabacbaaaccc\n",
      "baaccbabaccbbccac\n",
      "ccaacabac\n",
      "bccacacaabcccccbb\n",
      "abbcbbaabacabbabbc\n",
      "cbabccbc\n",
      "bbaababbcaa\n",
      "aacccbbcbccba\n",
      "abccaaabbabbaaabbb\n",
      "acacbbcbbbaaa\n",
      "acbacbcaaccaab\n",
      "aabbcabcbcaaccccab\n",
      "bccacbbcabacaccbcc\n",
      "bcabbbbca\n",
      "bccbbcaabc\n",
      "bcbcbbac\n",
      "ccccbcbacacbbbcbccc\n",
      "ababcbcabaccbbaabcb\n",
      "bbcaaac\n",
      "bbbaaccaa\n",
      "bbaccbbcbccbccaa\n",
      "babbabbbabccbcacbab\n",
      "cbcacbbcacbab\n",
      "ccccaccca\n",
      "aacacccbbbaccbb\n",
      "accbbcabaacbacccbac\n",
      "cccacaacbb\n",
      "cbbccabaacaa\n",
      "acbcbacbccaaab\n",
      "acacbabcbaaa\n",
      "baaaabcacbccaca\n",
      "cbcacaacba\n",
      "aaacaacc\n",
      "abbcbccbbbccaabbabb\n",
      "ccacbaacccabbab\n",
      "bbcabbbccbbaa\n",
      "cbaabaabbcbccacb\n",
      "accccabbbbbbacbcca\n",
      "aaaacbbaabcbbbccba\n",
      "bcccccbccacb\n",
      "abcababcc\n",
      "bbcabcaccccabb\n",
      "cabaabcbacbcabbbac\n",
      "abbababcccacacaaacc\n",
      "caccacacbcbcabbaa\n",
      "ccbbaacaba\n",
      "cbbcabccabccabca\n",
      "bbababcbacbba\n",
      "acbaccbbcbcbacccacb\n",
      "bacaaaaabcbbbcba\n",
      "accac\n",
      "bcacaabbca\n",
      "cbbcbccaaaaccbabb\n",
      "abcaaaaa\n",
      "cabaccaaacba\n",
      "acbabaccacccc\n",
      "ccacbaa\n",
      "acacbacc\n",
      "bacaaa\n",
      "accbcabc\n",
      "abbca\n",
      "babbaacaab\n",
      "aaccccbccaabaaabcca\n",
      "cbbcbcbacbbca\n",
      "cabcbaaacb\n",
      "cbcababacaaccacbacb\n",
      "abcaabababc\n",
      "ababacabacbcbbbabb\n",
      "bacca\n",
      "bacacacacaabbccbccb\n",
      "aaccbcaccba\n",
      "cbaabccbbccaacbacc\n",
      "babaabababcbbcbaabc\n",
      "cccacaca\n",
      "abbbaacbab\n",
      "baabca\n",
      "aaaaba\n",
      "abcbbbaacab\n",
      "ababccbbabcb\n",
      "bbbbacbbbcbb\n",
      "bbacaaaaac\n",
      "baccacbcacbcba\n",
      "acccccababaa\n",
      "aabbcbcccaa\n",
      "bbacaccbcabacacaa\n",
      "acbaac\n",
      "ccacbbbcbbbca\n",
      "caabbbc\n",
      "ccbbccaba\n",
      "caaaccaccbcbaba\n",
      "aacacbbababbbaaca\n",
      "bcbcacaca\n",
      "bcbcacc\n",
      "bbbcccc\n",
      "bcaabaaa\n",
      "bccccbcbaabbbb\n",
      "acacabc\n",
      "bbbacbbbabaccacbb\n",
      "aacacbaccabaaccaacc\n",
      "cbccacbba\n",
      "abbaabacaccacbaa\n",
      "abbcbabacbab\n",
      "abbaccbbac\n",
      "cbaccbabab\n",
      "bbcbab\n",
      "acaaabcbcbabcbaa\n",
      "baabaaccbcbbbbc\n",
      "bcaac\n",
      "bcbaaccacbbababcc\n",
      "acabbaabaccaaabac\n",
      "cbcbaa\n",
      "cabaca\n",
      "caacbbaa\n",
      "cabbbbcbaaaabcbba\n",
      "bbcaaacaccaba\n",
      "abbabcbcccb\n",
      "acbaaacabbbabac\n",
      "cbaaa\n",
      "cabbaababbc\n",
      "cbabacccbcaabc\n",
      "cccccccbbc\n",
      "cccabaaaa\n",
      "abacc\n",
      "aacabbcaaa\n",
      "acbcccababba\n",
      "bbaacbbbacbabbcaabc\n",
      "baabaabbbacccb\n",
      "cabbacabaabbcaabac\n",
      "aaaab\n",
      "aabccaccaba\n",
      "bcaabbb\n",
      "cababbcbccaaababac\n",
      "acaacaacccacbb\n",
      "abbcbacacbbabaccb\n",
      "abcbb\n",
      "acbaabaacababb\n",
      "cccaccabcbbbcc\n",
      "bbcabbabcbaca\n",
      "ccbabccbccb\n",
      "aabcc\n",
      "ccbbbbaacacbccca\n",
      "ccbbaaabaccbaaa\n",
      "cbcabcacabaccaabab\n",
      "bccbaaa\n",
      "bacaabcaabaacbac\n",
      "cabbcca\n",
      "cbabcb\n",
      "bcbaabbabcba\n",
      "bbabaabcbb\n",
      "babab\n",
      "aacacaccacaab\n",
      "cccacacacbbaabcb\n",
      "bcccbbababaabaababb\n",
      "ccaaa\n",
      "bcbbbcacaba\n",
      "bbbbb\n",
      "bacbcccaabcbcc\n",
      "abcabcbbaaabbbaca\n",
      "abbba\n",
      "aacaccaacab\n",
      "bcccbcc\n",
      "bccabbaababacbbcbbb\n",
      "bcacbcaaacaacaababb\n",
      "bacaaa\n",
      "bbabccacabaaaabbac\n",
      "acabaaaa\n",
      "bbcacaacbacaacccbb\n",
      "bcabbaccaaaccca\n",
      "cbcacabbcb\n",
      "aaccabaabcbbbcbcca\n",
      "cacaaaccccacccbb\n",
      "bcaabbbcbbca\n",
      "bccacc\n",
      "aabcccccbaaaa\n",
      "bbcbbccbaabaca\n",
      "cbaabbabcbacbcbaa\n",
      "baabbabcaaaa\n",
      "bbacc\n",
      "cbacbaccabcb\n",
      "cbccabcbba\n",
      "abbcaab\n",
      "bccbabbcbbbcabaca\n",
      "bcbac\n",
      "acbbacbaacbbca\n",
      "abbaaaaaccaaacab\n",
      "cbacbcaccbcaa\n",
      "bcaaccbaba\n",
      "cbcaccac\n",
      "cbabbbacaba\n",
      "bcbbaaacccbbbcaaba\n",
      "abcccabaaab\n",
      "acacbaaaaccbabbcacb\n",
      "abbcaa\n",
      "acaacaccaacaccbabc\n",
      "bacbccbaaaaac\n",
      "accac\n",
      "cccccaabaabaacba\n",
      "aaabbcb\n",
      "ccccbaacacacc\n",
      "bbcabab\n",
      "accbbaabbbabcccaca\n",
      "abcbcc\n",
      "baaababbacccabcbbaa\n",
      "aaabba\n",
      "bbbba\n",
      "bcacb\n",
      "ccbabbb\n",
      "bcbaaacbcaabcaacbac\n",
      "abaaabac\n",
      "aabbcaa\n",
      "aaaacabbcaacbcbbbcb\n",
      "accbbacaaccbcc\n",
      "ccaccbacc\n",
      "acacabb\n",
      "abbbbcaabbc\n",
      "abacbacaacaaacbbc\n",
      "cbbccaa\n",
      "bcbbaacb\n",
      "acacbbcabba\n",
      "caabaabba\n",
      "aaacaaacbbbcbcc\n",
      "bbbbcbabc\n",
      "cacbabbcaaccabc\n",
      "baccbb\n",
      "bcbcacacacbcacb\n",
      "cbacbc\n",
      "aaacb\n",
      "acbccaacc\n",
      "cbbcaababb\n",
      "bacabcbbabcacbbbcaa\n",
      "bcacabcaabbb\n",
      "bcbaacb\n",
      "aabcccacc\n",
      "aabcbabbbbcb\n",
      "abaccabacbabba\n",
      "caabbcbc\n",
      "bcbccaabaaacc\n",
      "bbabb\n",
      "cccaca\n",
      "bbcbcaa\n",
      "abbccbccbbcaacbcbaa\n",
      "bccaaabaacbbba\n",
      "ccacacbcababc\n",
      "ccaabbacac\n",
      "bcaccacaba\n",
      "baaaabcbcbcabab\n",
      "acaacbccacbbcbcac\n",
      "cccbcbc\n",
      "aabacaacacbbbcaaaca\n",
      "aacccbbbc\n",
      "bcacbbbaaaacaacab\n",
      "acbccaabca\n",
      "ccbaaccbcabaaabaaa\n",
      "cbabcbbaacbbcbc\n",
      "bcccccbbbca\n",
      "ccbabbcca\n",
      "aaacacbacbca\n",
      "accababacbcacaa\n",
      "bccaa\n",
      "bbbaacccaacc\n",
      "ccaaababbaab\n",
      "aabccac\n",
      "abaacaccbcc\n",
      "bbccbabbabab\n",
      "cacacbbaacaaccaab\n",
      "babbacba\n",
      "cabcccbacaaccbb\n",
      "aacbbccacbabacabcc\n",
      "abbcacbc\n",
      "bbcacaccbccbacbbba\n",
      "baaacaabbca\n",
      "aabbccabcabcacbacb\n",
      "cbaccbbcbccb\n",
      "acaab\n",
      "bcacbcaaabba\n",
      "baabbccabbcaa\n",
      "cccabcacbccaa\n",
      "bccacbcac\n",
      "caacaabaaccaaab\n",
      "cccaccbaacacbcabb\n",
      "bbcabccccab\n",
      "baaacbbab\n",
      "cbbcaacaba\n",
      "bccab\n",
      "cccccac\n",
      "aabbbaac\n",
      "cbcaabbcabbcbbc\n",
      "bbcbaababcbbbacab\n",
      "babacbbcacc\n",
      "cccab\n",
      "ccbac\n",
      "ccacbc\n",
      "bbabcacaccc\n",
      "ccccbbacbac\n",
      "bbbac\n",
      "ccacccc\n",
      "bbacacbccbbaa\n",
      "cbbacbbbbaba\n",
      "bacabacbacbabb\n",
      "caaaabbbccbbc\n",
      "cbcccabcba\n",
      "aaabbabaaacbcbab\n",
      "cbacbaaaaabcccb\n",
      "aaacacbcbcacaabcb\n",
      "aabbacaccbbabc\n",
      "cabbbbcbacccccaabb\n",
      "ccaaabcbbabcba\n",
      "ccbbcaaa\n",
      "bbbbcaaabbbac\n",
      "ccbabc\n",
      "cbacccbbaac\n",
      "ccbcbacbcbbcbacca\n",
      "baccccbabacbcb\n",
      "ababcabbaccabb\n",
      "bbcccabbbbcab\n",
      "acabbb\n",
      "cbcababcbabaacccaa\n",
      "bbbbcabcbbabcbacbcb\n",
      "abbabc\n",
      "caccbbcbcacb\n",
      "bbbaccbcbbbbacac\n",
      "caacbcbcbaaba\n",
      "abccccb\n",
      "cabcbc\n",
      "baaaacbbcaaacccab\n",
      "bbbcbc\n",
      "caaabb\n",
      "cbccab\n",
      "aaccbbaccacbaabac\n",
      "bcaabacaa\n",
      "cccacb\n",
      "acccccaacbbbbbc\n",
      "ccbaaaaa\n",
      "cccbbcacabcbcaab\n",
      "aabccbacbb\n",
      "caacaaacc\n",
      "bbcaaba\n",
      "abaacccbbbbc\n",
      "cbcbba\n",
      "cbbaccbbcaabc\n",
      "bcaabccbcbbbaab\n",
      "cbcbbcbbbbacbbbbac\n",
      "aabbababcbcbacac\n",
      "ccbbbcaccaaaabbabcc\n",
      "cbcababaabbacac\n",
      "aacabb\n",
      "bbaaac\n",
      "baccccabab\n",
      "caccbccbbabccb\n",
      "ccbacbccaababaabbb\n",
      "cbcacaaabbbcbcac\n",
      "bbacaabcbbacb\n",
      "bbabcaca\n",
      "acccbaaaaccbacc\n",
      "ababaccbbbbccaa\n",
      "cabcccccaabbbac\n",
      "acabcabb\n",
      "aacac\n",
      "aabbccaccbc\n",
      "accbcccccccabca\n",
      "ababbcbac\n",
      "acbbaccbabac\n",
      "ccbbcccbaabbcacccc\n",
      "ccaaacbbbcb\n",
      "ccccacaaacc\n",
      "bbbbcbc\n",
      "cababbc\n",
      "cbccacbccabcaac\n",
      "bbbbbbaa\n",
      "aacbbb\n",
      "bccccbbbcaa\n",
      "cbccbb\n",
      "bccabbaccbabbb\n",
      "aaaccaacccbcab\n",
      "acbbab\n",
      "cabccbaaac\n",
      "ccbcabaaabbbb\n",
      "cabbaabaaabaacb\n",
      "cbaabccbcaabacacbac\n",
      "cabcaabbbbbcaacbc\n",
      "cbabacabaaaba\n",
      "aabccbaababaa\n",
      "cbbcc\n",
      "acabbcbcaaba\n",
      "bbcbbccacccbc\n",
      "bbabcbbaccbc\n",
      "cacccbbccbcbcccbca\n",
      "baaacbabccbbaacabaa\n",
      "cbacccabbaababa\n",
      "baacccacabbcbbccc\n",
      "acbabacbb\n",
      "aacaaac\n",
      "caabcabaacccbcaa\n",
      "bbcaaabccabbcb\n",
      "accacbbbccca\n",
      "babbbbaabacbb\n",
      "cbaccaabbccbbbaca\n",
      "bcabbbbcccc\n",
      "abcbbbbbccacacc\n",
      "abbbabbaacacaabb\n",
      "cabbbcbbaaaacabbbab\n",
      "bbbccacbc\n",
      "abbccaaaa\n",
      "bbbaacccbbacbbabc\n",
      "cabacbacbabbc\n",
      "cacaaaacbbc\n",
      "aaaccabcaacbcb\n",
      "cbbccaaabbaaaa\n",
      "cabbbbcaaaccaabc\n",
      "acbbcbacac\n",
      "acbabbcabcccca\n",
      "abbcabaaaacaa\n",
      "accbabcaa\n",
      "caaaacabb\n",
      "bbccc\n",
      "cccbca\n",
      "ccbbcbcbaaabbbccac\n",
      "bccba\n",
      "cbaccbbbcbaaabab\n",
      "acbabcacbbacabcabca\n",
      "acabaaacacbabba\n",
      "bcccabaca\n",
      "cbbcbbccbcc\n",
      "aaaaacca\n",
      "acbaaccbbaabcbac\n",
      "bbcbbaaaaccccaac\n",
      "bccbcbcbccbcb\n",
      "babbc\n",
      "abcccaccbbbac\n",
      "bccaac\n",
      "ccaaacbbbcabbbaaa\n",
      "accbbbcbabbbacabab\n",
      "bbabbacbbaabba\n",
      "aaacbcc\n",
      "ccbcaaaaacbcbaaa\n",
      "bcbbcbbcbbbbabc\n",
      "acbbbcccacccaab\n",
      "babbccababccb\n",
      "cccbabac\n",
      "ccbaaccbcbbacbbc\n",
      "ababbbacbbaacac\n",
      "bbabacbaccbaccabcb\n",
      "cabac\n",
      "baaccbbb\n",
      "aabcbcccbcbbccaaac\n",
      "cacbb\n",
      "bcbbca\n",
      "aacbcbcacc\n",
      "bbacbcbbabcbabbcb\n",
      "ababcabacacbabb\n",
      "aaccbbcabba\n",
      "abbaacbbcabcabcaaca\n",
      "abbaaabacbbbb\n",
      "ccacccccacbabbabaac\n",
      "acaccb\n",
      "cbbacabcccacacbbcb\n",
      "aaaccaababbbb\n",
      "accbbaaabbbacbabca\n",
      "cbbcabcbccaaac\n",
      "bbabbbbcbbccca\n",
      "aaabaabbcbcacbabb\n",
      "bbcabccabccbcba\n",
      "cabcaaacbbcb\n",
      "bababaaabac\n",
      "abccbacabcccb\n",
      "abcbaacbcacbaabbbcc\n",
      "cbcaccaabbbaaa\n",
      "bcbbbabc\n",
      "cbccaba\n",
      "accaca\n",
      "bbacbccbcbba\n",
      "bccababcbcbbbbbacb\n",
      "cccbabbbcbacba\n",
      "bccacbacabaaacbaccb\n",
      "baaaaaba\n",
      "cacbcabbcacab\n",
      "bbcbbcab\n",
      "cbaabaabccaa\n",
      "aacabcbcccb\n",
      "bbbbaaabcbacbc\n",
      "ccbabcbccabca\n",
      "aaabbaab\n",
      "cbcaccabccaaacc\n",
      "babcbbcbbbaaccbcbca\n",
      "ccbbbbbabbaabababb\n",
      "cabbbcba\n",
      "cccbbacaabaaaaab\n",
      "cabaabccbbbcbaabca\n",
      "acabccabbabcbabbbc\n",
      "bbccac\n",
      "caabacbcaccabc\n",
      "ccabbbbccc\n",
      "ccabacaab\n",
      "baabacbabbbca\n",
      "aababccbcba\n",
      "acbbbacccccababb\n",
      "bbbaabbcbca\n",
      "bbcbcbcaaba\n",
      "cacbcbbcacaacc\n",
      "acaacaab\n",
      "bcbbcbababbcbcaacba\n",
      "ccaacbcbaaabbacbca\n",
      "cabbccbb\n",
      "abccabbacc\n",
      "bccbbbbbbacbbbbabac\n",
      "bbabbccabcbacccc\n",
      "cabcaabbcab\n",
      "acbcbaba\n",
      "aabbc\n",
      "cbbabbacbca\n",
      "ababbbbaacc\n",
      "accbcaabacccb\n",
      "ababccbaca\n",
      "bacaaccbcbbc\n",
      "bbbabcbabaaaabbbcaa\n",
      "cbcaac\n",
      "bacabcbbccbbbc\n",
      "aababccababaabaab\n",
      "bcbaccbbabbbcaab\n",
      "bcabcbacacbacabbabb\n",
      "bccaacccc\n",
      "aacbac\n",
      "acacac\n",
      "abacbaacbc\n",
      "caccacbbc\n",
      "aaaacbcaaaabbb\n",
      "aacaac\n",
      "bcacccabcaabaabbb\n",
      "babcbccbccaacbabab\n",
      "bbcaaababbccacbba\n",
      "acabacbbcccc\n",
      "babbacbcbcbbabacc\n",
      "bbcaabbccaccbaa\n",
      "ccaacacbccabbbbcbcb\n",
      "bcccaccaaaccaaa\n",
      "cbbccbccbcaabccac\n",
      "bcbbacbbabbabca\n",
      "cbccaabacbabbaaaa\n",
      "cccbbcb\n",
      "acaaaaaccccbabbaac\n",
      "cbaaaccccaa\n",
      "cacab\n",
      "acaaacbab\n",
      "caaaabbbaaba\n",
      "cbcabaabacbcccb\n",
      "cbbaabacabbbcaaabca\n",
      "bccbbcbbabaa\n",
      "aacbbcbbbbcabbba\n",
      "cacbaccacbacb\n",
      "baacabb\n",
      "babbbbc\n",
      "bbcbcccba\n",
      "bbbaaaaccaa\n",
      "cbccccacbabcca\n",
      "ccbbbbccaabba\n",
      "acaabaca\n",
      "cabbcbbbbcbbaabac\n",
      "acbabcacccaca\n",
      "aaacbaabbbcabbacc\n",
      "abccccbcacbcc\n",
      "aabbcb\n",
      "acacccaaabc\n",
      "cbbaacbbcbbc\n",
      "cbbcaac\n",
      "bcbbaacb\n",
      "cabcaabcabba\n",
      "aaaababba\n",
      "bbabca\n",
      "cbacbccb\n",
      "cbbbcccabbbbc\n",
      "ccbabaababaabbacac\n",
      "aacbababccbabbcac\n",
      "ccbaabccbc\n",
      "cbacb\n",
      "cbaba\n",
      "acaab\n",
      "baacabccbccccacbc\n",
      "babccba\n",
      "accbaaabaccabaacbbc\n",
      "acacbbccbaabcca\n",
      "acccccccabbb\n",
      "bcbbbcbacc\n",
      "cabcbaabccccab\n",
      "ccaabcaacccabcc\n",
      "aaabac\n",
      "bbabcccccc\n",
      "bcabbaab\n",
      "cbabccbaacbb\n",
      "abccbcbaaccacbacca\n",
      "cabcbba\n",
      "cacabacabb\n",
      "abbbbb\n",
      "acbababacbbbccbbaac\n",
      "acabbaacacbaaca\n",
      "ccaaacbbbbaaba\n",
      "bccccbcaabacabaab\n",
      "ababbbbcaabbcbbaaa\n",
      "abccaaacbabbbaacbab\n",
      "bcababcaaacbbacb\n",
      "aaacbcaabaabbacccc\n",
      "abccabaccbca\n",
      "abccaacbb\n",
      "abbcabcca\n",
      "cabcaca\n",
      "bcbacbcaccbaccaba\n",
      "acababcb\n",
      "baabaabcccbaaccabb\n",
      "ccaababbabbc\n",
      "caaabbb\n",
      "cbabbbcacabaabbc\n",
      "bccba\n",
      "abcaacacbcabba\n",
      "ccabccbcbcbbacc\n",
      "cbaccbaaaccccbcaaba\n",
      "cbcbaacccabbbcaaa\n",
      "ccbaaccbaccb\n",
      "cabcabcaabb\n",
      "baacca\n",
      "abbaab\n",
      "bababaabbccaacaaaca\n",
      "babcac\n",
      "cbbcbbaaca\n",
      "bccca\n",
      "bbcbbcbcbaccacaaa\n",
      "bacabccccacccbb\n",
      "aacbaaaccabbb\n",
      "aaabaabbba\n",
      "abbaaacbaacabcaccba\n",
      "acbbabcaaacbaba\n",
      "cacbaaacba\n",
      "caccccbabcaac\n",
      "acbaca\n",
      "bacbccacbccacba\n",
      "baccaaaacaaabacaaac\n",
      "abaaaacbbacaba\n",
      "acccaaccacaaa\n",
      "cccab\n",
      "cbabbccbcaaabbabbbb\n",
      "cccbcaa\n",
      "accacabccabbabaa\n",
      "acbccbcaccbabccbbab\n",
      "cabcbcaabca\n",
      "bbaacbcbbbcbbcbbbab\n",
      "aacba\n",
      "abbca\n",
      "ababbabbbcbabc\n",
      "bccacaccabac\n",
      "babbbca\n",
      "bcacabaaabaaccbba\n",
      "cabbbbbbabcbbbbcbaa\n",
      "ccbbcbcaababaaacbc\n",
      "bbbba\n",
      "ababacbabbcbbaacac\n",
      "aabacc\n",
      "aabbaaccacaacbb\n",
      "abaccacacc\n",
      "cacacb\n",
      "cbababcbccaaaaaccab\n",
      "bbbbacbaca\n",
      "baccbababbc\n",
      "accacaababaaabb\n",
      "bbcaaaabaabcba\n",
      "bbccccaaaacbcacb\n",
      "abcccbcbbcbaaccbcba\n",
      "cacabbcaabababba\n",
      "ccaabacbbaaa\n",
      "babab\n",
      "acaaa\n",
      "acbbbcaccbbacaaab\n",
      "bbccbccbb\n",
      "ccabcbb\n",
      "cacacbccabbcab\n",
      "aabacaabaaabb\n",
      "cbbccccacabcbbaaa\n",
      "acbac\n",
      "accac\n",
      "aacaacc\n",
      "aaacbaaacbba\n",
      "cbacacabc\n",
      "acacaaaa\n",
      "bcbbcccaabac\n",
      "cabacbaabcc\n",
      "baabbacaabcaabab\n",
      "aabcbcac\n",
      "abbbccc\n",
      "abbbacccbabb\n",
      "acaccbaaaaba\n",
      "aabbcccaacbcbaabb\n",
      "aabbacaaccabacaaabb\n",
      "acbabccaaabbacbaac\n",
      "abacbab\n",
      "bbacbcbbaabcbccacc\n",
      "aababb\n",
      "bcccabbcacacac\n",
      "cccaccbabcccaa\n",
      "bcbacbb\n",
      "bcbbcbbcaacbcbaccca\n",
      "caacacbccabbcbb\n",
      "baacacabaaccacacb\n",
      "bccbcbbcbcaaac\n",
      "ababaccabaca\n",
      "aacbccb\n",
      "cbabb\n",
      "bbcccabcbbcbac\n",
      "ccacbbcccbbcbc\n",
      "cbccacbabacccbbacc\n",
      "acacbabacaccc\n",
      "bcbcbabbbacabbcacb\n",
      "acaabbabaacb\n",
      "acbaacaaccccbcca\n",
      "acaacaaabaaa\n",
      "bcacbacc\n",
      "bccbccababaaaca\n",
      "accaccacbcaccabccc\n",
      "cacbaacaacbaacbacc\n",
      "cacbbaccaaaabb\n",
      "cbabacbbbcbbac\n",
      "ababcabccbcba\n",
      "cbbbca\n",
      "bbaccabacabcbcccb\n",
      "bcabaaacbbacc\n",
      "abaabccba\n",
      "cbbaa\n",
      "aaaabaaaabcbbb\n",
      "bacacbabaababcabb\n",
      "bbaccacc\n",
      "acacaabbaccaccbc\n",
      "bcbbaaccbcacaaa\n",
      "aaabbbbacaa\n",
      "cacaacbc\n",
      "bababaacaaac\n",
      "baccaabbaaaaba\n",
      "aacaccbbacacbbbb\n",
      "bccababbbcc\n",
      "aaaab\n",
      "caccbbaaccbbcacab\n",
      "bccbaaccbbcaca\n",
      "ccbacba\n",
      "aabbccbcab\n",
      "cbaaccb\n",
      "bbbcccacbca\n",
      "abaaabaccccbb\n",
      "caacacbcabbcbbbba\n",
      "cccccbbbcabbb\n",
      "abcbababa\n",
      "bbccaacabaccacc\n",
      "babcbabbb\n",
      "ccbcbbbcaacccacba\n",
      "ccccbba\n",
      "bababababbacb\n",
      "cbbbccbacacccbbca\n",
      "babacbaccacab\n",
      "aaacaaabca\n",
      "abbcbbbaa\n",
      "cacabcbabbbbcaabcbc\n",
      "cbabcabac\n",
      "acaacaaba\n",
      "cbacabbbaaabbca\n",
      "cbccacaaacaa\n",
      "aaaccb\n",
      "ccabccbcbc\n",
      "caacbbbbabaacc\n",
      "ccbcaabca\n",
      "bbbcb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaabcaaaaa\n",
      "babbbb\n",
      "bbaabb\n",
      "accbcabcbacba\n",
      "bababbc\n",
      "ccaabac\n",
      "bbbaccacaaababbcaa\n",
      "bcaacbcbbccbaccbc\n",
      "acbcbcaabbbbaa\n",
      "aacccacabccac\n",
      "acbcbaabbcbbbccaaba\n",
      "cbbcaccbacbccbca\n",
      "bbaaacbcba\n",
      "bccaaa\n",
      "bbbabaa\n",
      "cbbbabbaba\n",
      "ccbab\n",
      "aaaababac\n",
      "bbcaaccab\n",
      "aaabbacbbabcc\n",
      "bbbbbbcbbbaaacabbaa\n",
      "baabaa\n",
      "cbcabbcac\n",
      "cbaccbcacbbbac\n",
      "bcaabaaca\n",
      "bcccacaaabc\n",
      "aacaaaca\n",
      "bacca\n",
      "cbcbbbaaccbca\n",
      "cbbacabcbccccac\n",
      "bbabaaaacc\n",
      "bbbcaabababcaccac\n",
      "aabccaaabbcacabba\n",
      "bacbbab\n",
      "baacbbccac\n",
      "cabaaabbcbbbccb\n",
      "cccccabccaabbbbbcb\n",
      "aaaabcacaaa\n",
      "bcbccacbaaacbbcab\n",
      "ccbcacbcbbbc\n",
      "cbbcbacacccccbca\n",
      "acabb\n",
      "aaccbaacbacbc\n",
      "bbaaccc\n",
      "bbbbbcaaac\n",
      "cbcbacbcaba\n",
      "cccccbcaccbacabc\n",
      "abaaccaacaacaa\n",
      "bcbacbbabaaacbbcb\n",
      "ccbacc\n",
      "abccaaaccbaccacbaab\n",
      "aababcccbcbbbbbaac\n",
      "cacacccbc\n",
      "bbbbaacccaacbacca\n",
      "bcaca\n",
      "ccbacacaac\n",
      "cbbabc\n",
      "ccacacbbbcbbbcc\n",
      "bccabbaaaacacbab\n",
      "aacbacbbbcbabcbbc\n",
      "accbbbacbcaac\n",
      "bcacbcacacbabacaca\n",
      "aacab\n",
      "baabbbbbcb\n",
      "cabaaacbabaccabacb\n",
      "bbcabbbaabbabccccaa\n",
      "abaabaaccacbbc\n",
      "aaacbabcaabac\n",
      "bcacacabbbbbcaba\n",
      "cabcaaacb\n",
      "abcabcca\n",
      "cbbccac\n",
      "cbcabccaa\n",
      "bcaacccbbbac\n",
      "aabacbcbbbc\n",
      "ccacacbbacca\n",
      "aaababccaabaabca\n",
      "cbcbac\n",
      "abbcbbcaacbaaaacbac\n",
      "cbbbbcbccbbbabbb\n",
      "baccbbc\n",
      "acbbaaabbcccba\n",
      "acccaaaba\n",
      "cbaac\n",
      "cbaaccbacaaa\n",
      "acbaababcacabcca\n",
      "cbbbcc\n",
      "acccababbcbbcccc\n",
      "bcbbacb\n",
      "cbbcacaacbbac\n",
      "aaabcaccabaabbc\n",
      "bcaabcb\n",
      "cbcbabacbb\n",
      "bacbcbaabcccbcc\n",
      "cbcccbabaaacba\n",
      "cabba\n",
      "bbcaacbbaab\n",
      "bababcbccb\n",
      "abcacb\n",
      "bcbbbabcabcccbaaccb\n",
      "abccbcaabccaacca\n",
      "abaabcbacabbacacbc\n",
      "acababa\n",
      "aacbabbbcbcbaa\n",
      "aacccabcc\n",
      "aabccbabc\n",
      "bacbc\n",
      "cbbabbbaaccccaccbba\n",
      "bbbcacbbccabacaaa\n",
      "bbcacaac\n",
      "acbbbbbabcbb\n",
      "bcabbaccacbbaacbb\n",
      "cacccaccccbbcacab\n",
      "aaaacbb\n",
      "bcbcbccbabcaba\n",
      "bcaaabbcbccab\n",
      "abbabbc\n",
      "ccaabacacbacba\n",
      "cbaacbacbcc\n",
      "bcaacbac\n",
      "baccabccccbc\n",
      "bcaabcaacabbaaab\n",
      "cccccabb\n",
      "cabbaccacc\n",
      "aacbcababbbba\n",
      "bcaaaaba\n",
      "acacbabbacb\n",
      "cbabbbabbc\n",
      "cccbcbccbcbab\n",
      "ccbacbababa\n",
      "baccbbacabacbb\n",
      "cbaaacacaabbbbac\n",
      "babcaaaaccaca\n",
      "ccacabcacaacbabacbc\n",
      "cccbabaaacacaa\n",
      "bcbbabaccbbcbcbbbac\n",
      "babcccaacbbcab\n",
      "babbbccbcbaab\n",
      "abbaaaccacc\n",
      "cbcaaaccbbccbb\n",
      "baccaccbbacacc\n",
      "bbbbcccbacb\n",
      "accbaabaaabbcbbbca\n",
      "bbcabc\n",
      "aaaabcbbbbbbb\n",
      "bcbbcabcaacbbacccb\n",
      "cbaacaababbccaacabb\n",
      "aabccbacccabc\n",
      "caccaacbbcacccb\n",
      "aacbabbcbacacccbaa\n",
      "bccccbcbbbccbc\n",
      "baccacaba\n",
      "abbbbbaaaaaabcc\n",
      "bbbbcbab\n",
      "bbbbccaba\n",
      "acbaaacccccbbcbaacb\n",
      "babcbab\n",
      "bcbabbbcaaabcbbbc\n",
      "aaacbbcccbaaacbccca\n",
      "cabccaacacacacbcbab\n",
      "bacacaaaaabac\n",
      "caaacbaccccca\n",
      "acbabbaba\n",
      "bcbcab\n",
      "ccccbcccbaccbcbabca\n",
      "aabacbcacaab\n",
      "bacbbaabbcbcacc\n",
      "bbbcababbcaacbc\n",
      "bccbbaabcacbbaabcc\n",
      "cacbabbbc\n",
      "baaaccbbabc\n",
      "ccaccccbbbcba\n",
      "cccbbbbbccabaa\n",
      "cbabca\n",
      "cacccc\n",
      "cbbccaccbcaabbaac\n",
      "abbbabccbba\n",
      "bcacbbabcbcccab\n",
      "cabcacab\n",
      "cabcac\n",
      "bbcbbcca\n",
      "acbcbbaaa\n",
      "cabbccacaacaaaaabb\n",
      "ccabb\n",
      "cbabbaaaaaaacbc\n",
      "aabcc\n",
      "aabcbacacacaac\n",
      "acabaccca\n",
      "caaababacbbba\n",
      "bbbaccababcaab\n",
      "abbbcacaabcaab\n",
      "acbbaa\n",
      "acacabaacacccbbac\n",
      "cacbaa\n",
      "bcbcbbcabbabccabcb\n",
      "ccbacaa\n",
      "bbbabccacacaaca\n",
      "cabcbbbbba\n",
      "bcbacabacbcb\n",
      "acaacaccaabb\n",
      "aaccb\n",
      "bbacbcbaaa\n",
      "acaaa\n",
      "ccaccababcacaa\n",
      "abcbaabacb\n",
      "aabbcbbabbaab\n",
      "aabbcbcacccacca\n",
      "acbaabbaa\n",
      "aabacbbba\n",
      "abbbacbbbcc\n",
      "ccbacabacacb\n",
      "abbbbccbccacbaba\n",
      "cbabaabbbbbaa\n",
      "acccbbcbabcbccbb\n",
      "bbcabbaaaca\n",
      "ccaabcbaa\n",
      "ccbaccabacaaba\n",
      "abbbbaabbabaaab\n",
      "bbbcc\n",
      "aabcccacacbbacc\n",
      "cacbc\n",
      "bbcccbcbabaccb\n",
      "aacabcaaccbbcac\n",
      "aabbcbc\n",
      "aabbaacacccb\n",
      "bbacaaa\n",
      "aacabaaabcabbaabab\n",
      "cbcab\n",
      "accacbbcbaccbabcaba\n",
      "babcac\n",
      "bacbaabba\n",
      "bcbaacabccbbab\n",
      "cccaacbcbcacacccba\n",
      "cbbbcabb\n",
      "aaaccacabbcaabbabc\n",
      "acaaababcbbabab\n",
      "aaabaaacacbbabbbc\n",
      "caccbabaabbabbca\n",
      "acabccbccbccbbccc\n",
      "abbcbab\n",
      "babaaaaca\n",
      "cbaabb\n",
      "acacbbccbbc\n",
      "caaaa\n",
      "bcbbbacaabbc\n",
      "abcacbbaacabaaccaba\n",
      "babacaaacbccabaa\n",
      "cbbacbbcbabcaac\n",
      "cbbab\n",
      "cbaca\n",
      "bbabbbbbcb\n",
      "acbccaabbbc\n",
      "cbcccbccaaab\n",
      "acbaacacccaac\n",
      "bbccc\n",
      "caaccbcbcccabbbaa\n",
      "cccbab\n",
      "cabbccb\n",
      "bcaabaacbcaac\n",
      "cacaacaa\n",
      "caaaa\n",
      "bbabaababa\n",
      "abacccbcbaacbbcbcbc\n",
      "bcabababaabaaab\n",
      "cacccbbabcbca\n",
      "caccabbbccbaac\n",
      "ccaccabacbbabccc\n",
      "ccbcacbbaabaabc\n",
      "abbbca\n",
      "cabab\n",
      "aaccabacaac\n",
      "bcaccccbaababcaab\n",
      "accabcbaacccbcbbacc\n",
      "bccaacaaca\n",
      "ccbccaaaccacb\n",
      "cbbccbcaaabbcbaaaca\n",
      "caaaab\n",
      "ccccbaaabb\n",
      "babab\n",
      "baabbba\n",
      "aabcbbacabbbcbccccc\n",
      "bbacbacbcbcaa\n",
      "cacbcacbcbbcbaabc\n",
      "bbcaccacccccc\n",
      "ccbccaaabcbbabccaa\n",
      "caabcbacccbbbcabbb\n",
      "bbbbabcccbb\n",
      "bcbcabcbcbccaaaab\n",
      "babbbccbcabcabccbba\n",
      "cbbcccaacaacacc\n",
      "abbbacababbbbaca\n",
      "aaccbbbaaaca\n",
      "bbabccbb\n",
      "caccaccca\n",
      "caabacbcabaacaab\n",
      "bacbcbacacabccb\n",
      "bcccbaabaccbbabc\n",
      "abaccaa\n",
      "cbacaacaacaccacc\n",
      "babacaaabaacabcabba\n",
      "aacccaac\n",
      "caaabbbabbabc\n",
      "baacaab\n",
      "cbbaa\n",
      "ccccbcaabbcbcbccac\n",
      "cbccc\n",
      "cbbcaabbcaaa\n",
      "bacccbbbcc\n",
      "ccccbab\n",
      "aaabb\n",
      "cbacaccaaab\n",
      "bbabccbcababbbaba\n",
      "ababbc\n",
      "accbabaacbaa\n",
      "bcacbacacbcbabbac\n",
      "bccbbbcbabbbcabcb\n",
      "ccbcbbcccaacbab\n",
      "caaaaaacbbbccbc\n",
      "baaccbaacbcb\n",
      "bababcca\n",
      "caacaa\n",
      "abaaccbaaab\n",
      "cccaaabcabbbabcbaa\n",
      "ababbbabab\n",
      "baaaaccaabba\n",
      "abcccbabcacbaac\n",
      "bbbacb\n",
      "ccccbbabc\n",
      "bcaaaabac\n",
      "ccccaa\n",
      "abbabcbacca\n",
      "cbaacbcaacc\n",
      "caccaa\n",
      "cbabaaabacacacb\n",
      "bcaaacbacbcacbbac\n",
      "abababbbc\n",
      "bbbbbba\n",
      "aacaccabcccabca\n",
      "ccbaabaaccaabbb\n",
      "accbc\n",
      "ccabbcbcacacc\n",
      "cacbbcbacabaababaab\n",
      "acacacccc\n",
      "aaaacb\n",
      "aacacbcabaa\n",
      "cccbcab\n",
      "ccacabcabbbbccabaca\n",
      "bcbaabbbaaabcc\n",
      "abbaababbaa\n",
      "cbababaaccab\n",
      "caabbabbccc\n",
      "baabccbbbacabbacb\n",
      "cbabaaabbaabaca\n",
      "accbccbaaaabcbaa\n",
      "cccabbbbcb\n",
      "aacac\n",
      "acbbbbcacbcabca\n",
      "bbbababcaa\n",
      "cbaaabb\n",
      "abacaccabbacaaaaab\n",
      "bbaabcbbcaabcaba\n",
      "bccbbbb\n",
      "cabbbbbc\n",
      "bacbbaacbcacbaa\n",
      "abbaacbba\n",
      "cbbbbccaaaabbbccbaa\n",
      "caccbbaccacbbaa\n",
      "acabcb\n",
      "bacbcababbcabba\n",
      "acaaaaabc\n",
      "baacacbbbcaaac\n",
      "bcbbaabbcbbac\n",
      "cbbabbbcaabca\n",
      "bccaaca\n",
      "acabbbbbbabba\n",
      "cbacb\n",
      "baccacbcbbbaabcabc\n",
      "aacbacaa\n",
      "cbcbaabcbbbcbc\n",
      "aaccbcaabbab\n",
      "aacaaabcbcbaaaaa\n",
      "bbbcccacb\n",
      "acacbcc\n",
      "aabcbabbbcccbcbcc\n",
      "aaacbacaaac\n",
      "bbabbbaccaccacb\n",
      "ccccbccabb\n",
      "cbcbcbcbcb\n",
      "abaabbcbcaa\n",
      "baaacabacacccba\n",
      "baaabb\n",
      "ababacbccaacabbcac\n",
      "abcccc\n",
      "ccbababc\n",
      "cbabc\n",
      "cccacbcaccbbb\n",
      "cabccababbccab\n",
      "bbbbbabacbccabbccc\n",
      "bbbccccabaa\n",
      "bbcbbbb\n",
      "cbccbbbcaaaba\n",
      "aabbbbabbbcabcbacbc\n",
      "bbcbbbcc\n",
      "bacccbb\n",
      "acaacacabaacccb\n",
      "aabaaccacca\n",
      "accccaa\n",
      "aaabbcccbbabacccab\n",
      "babaaacbaaab\n",
      "babacbcaac\n",
      "abababc\n",
      "bbbccc\n",
      "acbbbaaaaccca\n",
      "cbcabacccab\n",
      "ccbaabbaaabc\n",
      "cbbcbc\n",
      "cbaaacbc\n",
      "baacbabcacacc\n",
      "ccbcca\n",
      "cbcabbcbccb\n",
      "acbccbcbacbbaa\n",
      "caabbaaa\n",
      "bacaabccaaa\n",
      "caaabbabbcabcbaa\n",
      "ababacb\n",
      "acabcaabccc\n",
      "abbacbc\n",
      "acaababcbabbb\n",
      "ccacbaaccacab\n",
      "cabaaccaacbc\n",
      "babcbaaaccbb\n",
      "cbcbcac\n",
      "ccaccc\n",
      "aacbccbacbc\n",
      "baabccc\n",
      "bcbaabcbcbbcaabaccc\n",
      "cbacbbbacbacacaba\n",
      "bbccbaacaca\n",
      "abacaaabbaa\n",
      "aacbccbbcbbccc\n",
      "abbbaaaa\n",
      "bacba\n",
      "bccac\n",
      "abacbaba\n",
      "cabacaacacbaabacbcb\n",
      "bcabcbbabcbbbcbba\n",
      "cbacc\n",
      "caaccccbbaa\n",
      "bcabab\n",
      "ccbcac\n",
      "aaabcbacbbc\n",
      "babaabcbbcbcaaaaca\n",
      "ccbacaba\n",
      "bcacbab\n",
      "bbcacacaabcbbbcccb\n",
      "bbbabbab\n",
      "ccbbcbbcbbccbbca\n",
      "bacbccacbccccc\n",
      "bccbb\n",
      "ccbba\n",
      "babaac\n",
      "abaabcbacbcbcc\n",
      "abaabacacaacbaac\n",
      "accbacbaccababc\n",
      "abbbcaa\n",
      "aaabacaabcccabaabbc\n",
      "bbacbbbaabcc\n",
      "accaccbcaabaacbb\n",
      "abacbcbbbbcbabcbbaa\n",
      "acccccbabaacc\n",
      "bbcbcbab\n",
      "cccbabbcbbabbcb\n",
      "bcbbcacabbcbabbcb\n",
      "aacba\n",
      "abbbbcababbbca\n",
      "babccbbcbcc\n",
      "acbbcbabbaccaaaba\n",
      "acbcabbcb\n",
      "bbababacccbbcc\n",
      "ccacabcab\n",
      "cbcacaaabbbac\n",
      "baccbcbc\n",
      "bacaabcbbaabccb\n",
      "aababbcbbbcccbbc\n",
      "cbcbc\n",
      "aababc\n",
      "bcbacac\n",
      "bbabaacbcaacbcbcbc\n",
      "bbcaaba\n",
      "abacababcc\n",
      "bbabbbabc\n",
      "aaacab\n",
      "cbbaa\n",
      "ccaabba\n",
      "bccbb\n",
      "aaaccbcabbbccbbaaaa\n",
      "cacccabcccccc\n",
      "bcaccbb\n",
      "cbcbbaaaaccc\n",
      "acbaccbbac\n",
      "bbaacababbbacba\n",
      "ccbccbbbbcaaaaacac\n",
      "accbbaba\n",
      "bcaaacaacaccbbb\n",
      "acbbcbcb\n",
      "caabccbbabbaaaba\n",
      "bbcbbcbaac\n",
      "bbaabacbacacc\n",
      "cccbbc\n",
      "ccabacbc\n",
      "abaacabbcccbcbbaaab\n",
      "bbaccccabbba\n",
      "caaabbaaabaabba\n",
      "baccccacc\n",
      "ccccaaabcacacc\n",
      "aacaaa\n",
      "aaaaaabbcaaabcccaca\n",
      "bcabc\n",
      "cbabacbabacababcb\n",
      "cbaacbacbbabcaaa\n",
      "acaaabaa\n",
      "caabcbcbaabaa\n",
      "cabaa\n",
      "caababacabbac\n",
      "abbaaba\n",
      "cbbbbccababc\n",
      "abcbababcbbaaacbcaa\n",
      "acabacacbcccccc\n",
      "bcccbbbb\n",
      "bcaaacaabaaacbc\n",
      "cacbcbcaba\n",
      "cccccacc\n",
      "abaacbbcbcaa\n",
      "baaccabaacb\n",
      "cabcccabacbcaacbb\n",
      "bbcabbbbbccba\n",
      "abacccb\n",
      "cbbccbccbaccacab\n",
      "accabbbaaacba\n",
      "bcaccbc\n",
      "bcbbbccabcaaaabcab\n",
      "cbbacbcccbbaac\n",
      "babca\n",
      "cbcabacacaabba\n",
      "aaccaacbabbcaaaac\n",
      "acbbccbcccb\n",
      "acbbbcaaa\n",
      "ababababbbacc\n",
      "acbccabcbabaaa\n",
      "bbabbcacabababcc\n",
      "bbaababbab\n",
      "abcccaaccacbbacac\n",
      "caaaccbbbcbcacacaaa\n",
      "abbca\n",
      "bbaaabcb\n",
      "bcabbaccaabc\n",
      "cccbbcbbcc\n",
      "babbb\n",
      "cccbcacbb\n",
      "bcbbccbbbcaaac\n",
      "caaccbabcaa\n",
      "babcaccccbcbabccb\n",
      "bccbcbcabccaa\n",
      "aacccbcc\n",
      "caaaacabaaabbabc\n",
      "bbaaaaccbcabac\n",
      "acabcb\n",
      "cccbcaacababbaaacc\n",
      "aabbccc\n",
      "aaacaabbcaaaac\n",
      "acabbca\n",
      "bbcccbbabbb\n",
      "caaaacacbbab\n",
      "caaccbabcba\n",
      "babcbacbcbba\n",
      "acbcbbacababcccb\n",
      "baabacccbab\n",
      "baccbcabcab\n",
      "bcabbbabab\n",
      "babcbbcccac\n",
      "cbcacacbbbcaac\n",
      "abccaaaacbcaccccb\n",
      "abcabcbacacc\n",
      "babbccbabacbabacacc\n",
      "abbbab\n",
      "abbaaccbccaccbacbca\n",
      "bbccccaacbbbcbbcba\n",
      "babbbacababa\n",
      "cbbbca\n",
      "accaaacaa\n",
      "bccbaccccabc\n",
      "cbaaccacacaccc\n",
      "cbacbbacccbcc\n",
      "bbbcbabbbacaaca\n",
      "ccabbccbaccccbc\n",
      "bcaabbaaa\n",
      "caabcabc\n",
      "bbbbcbbbccabbcccc\n",
      "bbbabcabbacabc\n",
      "bbcabab\n",
      "ccccacbaababccb\n",
      "aaccabbc\n",
      "bbcbaacaa\n",
      "cababccccc\n",
      "bbcabbabcacacabab\n",
      "cabaab\n",
      "abbccccacacacccb\n",
      "abbbabc\n",
      "bcaaabbbbcaa\n",
      "cbbbbbccabbbacabac\n",
      "aaabcccacc\n",
      "aabacbaca\n",
      "cbabbbcacbacaaabbc\n",
      "caabbaccba\n",
      "cccbbb\n",
      "caabcbbacb\n",
      "ababccab\n",
      "cbbab\n",
      "caaaacbaacbbaa\n",
      "abbbaaa\n",
      "acbcbccbbb\n",
      "cbaaabccaa\n",
      "bbbcaabacaacccc\n",
      "bbbbbaccabacbb\n",
      "bbacbccabaabacca\n",
      "babcab\n",
      "bccac\n",
      "babbbbbba\n",
      "babccabcaaaab\n",
      "aacbabccbbbcbab\n",
      "abacccaa\n",
      "ccbcaab\n",
      "bbaaacacaabc\n",
      "cabbcbaacccb\n",
      "aacabbabbaa\n",
      "cabccacaa\n",
      "acaccabcbaccbb\n",
      "bbbaccbaaa\n",
      "bcbacc\n",
      "bacbbcccbbbbbcccbb\n",
      "baabcbcabaabbacaaaa\n",
      "bcccc\n",
      "aaacbcaabbb\n",
      "cbbaabbcbbcbbbaabb\n",
      "bcbaaccc\n",
      "baccaaccb\n",
      "aaccacabccc\n",
      "baaacbbbcbcbbab\n",
      "caabcbcaabaaabac\n",
      "abbbccbcba\n",
      "ccabccccaccaa\n",
      "bbacba\n",
      "caabccaabbbbcca\n",
      "babbbcccb\n",
      "abbacaccacb\n",
      "bbccababacaaaaba\n",
      "ccbbaca\n",
      "aaccb\n",
      "ccbabbcacaaacbcbcc\n",
      "abacaaaccca\n",
      "aabcbcbaacbaca\n",
      "aaabb\n",
      "abaccacaaabcbaaab\n",
      "bccaababb\n",
      "cbcaacb\n",
      "bacbcbabbabcbccbcab\n",
      "abababcbbaacb\n",
      "bcbcaababbcbac\n",
      "caccbabbcbcaaacc\n",
      "babcb\n",
      "caccca\n",
      "accbbbcbbbbc\n",
      "aabcccba\n",
      "cacabbaabab\n",
      "bcabcbbcba\n",
      "cabcccbcaacbabcaab\n",
      "bbcccbb\n",
      "abbacbcccbcccabab\n",
      "aacccabbcbb\n",
      "aacbacbbaaccaa\n",
      "babaabbaabccccaa\n",
      "acacccbcacaaccccbc\n",
      "abaccccbbb\n",
      "caaabbabbc\n",
      "cbbaaabacbbaacacb\n",
      "bcacacccbaab\n",
      "acbabccbaaac\n",
      "acabbcacac\n",
      "cbbbaabcbaababbaabc\n",
      "ababcaabcb\n",
      "aaabbbacb\n",
      "abbababcbaac\n",
      "bbacccaac\n",
      "acbabacbbaaccbcb\n",
      "bababcbaaaabccb\n",
      "bcbaaccacbacc\n",
      "cacccaacaa\n",
      "bcbabaabacbbbb\n",
      "cbbba\n",
      "cbcaccbccacbbc\n",
      "ccacccbbcbcccaa\n",
      "accabaacabaccbbaaca\n",
      "caaccbaaabaacbbcaab\n",
      "cbbccacbacbbccbaab\n",
      "cbbcccccaacbcabca\n",
      "bbabbccaac\n",
      "aaacabbbacaacacbcba\n",
      "cccbcbbaccbb\n",
      "cabcbcbcbcbbabbabb\n",
      "accaabcb\n",
      "bbbaa\n",
      "acbccbbbbcab\n",
      "abcabcab\n",
      "ccbaa\n",
      "baccbabcccacac\n",
      "aacabcbbcabaaaaa\n",
      "abbbacbab\n",
      "abbcaaabbabba\n",
      "bacbcbbba\n",
      "bcbbbacacc\n",
      "aaaccbaaabbaca\n",
      "abcaaccabcacccbaabc\n",
      "cbbabbbaaacbcbcabbb\n",
      "ccbbabbba\n",
      "cbaab\n",
      "bcbcaabcbbbcbc\n",
      "bbbbabb\n",
      "caaccb\n",
      "ccaaabacbcccc\n",
      "accca\n",
      "ababcccbaababa\n",
      "bccaabcacbcccacbabb\n",
      "bbabcc\n",
      "aaabcaaacbcbbbbaab\n",
      "abcbababcbbbccbbbaa\n",
      "baaacaccaaacbbbbbba\n",
      "abcbaa\n",
      "aacca\n",
      "baccacccc\n",
      "bbabaaacb\n",
      "acccaaccbabbcaabbb\n",
      "caabcabacbbaccbcbb\n",
      "baccbbbbbcbc\n",
      "bacccbbacbaba\n",
      "cccccbaccb\n",
      "babaccc\n",
      "cbacbbaaaaaaabbcbb\n",
      "cababbccaaaacacab\n",
      "bccbcbbcbcb\n",
      "cccccbac\n",
      "aacbaa\n",
      "cbcbacabbbba\n",
      "ccabbba\n",
      "bacabbbababbbbcabc\n",
      "aababcbaccbaa\n",
      "babac\n",
      "aacbbcbbabaaacba\n",
      "acbabc\n",
      "cbcbacbabacababca\n",
      "abacabbababca\n",
      "ccccabcaaa\n",
      "ccbaaaccb\n",
      "bbbccbcbbbbba\n",
      "bcbacabacbac\n",
      "cbaacb\n",
      "accbbcc\n",
      "aacccaaccacabcbbc\n",
      "ccbaccbabbbbccca\n",
      "cbaaababbccccaccbac\n",
      "bccbbcaabbac\n",
      "cbbcabbccbbcabbaac\n",
      "bbccababab\n",
      "ccbbabac\n",
      "acabbcbaccbbcbaa\n",
      "cbccccaabcb\n",
      "baabcb\n",
      "cbcac\n",
      "abacababbbbabaac\n",
      "ccbcbcb\n",
      "bacccbaa\n",
      "aaaabbaccbaca\n",
      "bbabbacbcacaacc\n",
      "ccbabbcaaa\n",
      "abcabacbbbbbcc\n",
      "cabbacaaabcbac\n",
      "ccaaccabaabacca\n",
      "acbacabcbacccbcaaa\n",
      "baababaa\n",
      "abbbcbbcbccaca\n",
      "ccacbc\n",
      "babaacbb\n",
      "bbacbbbbcbaba\n",
      "baabcccbccabacca\n",
      "aaacbccbc\n",
      "abbabbbbaaabccaccc\n",
      "cbabaac\n",
      "cbbcccaaa\n",
      "bbccbabcbcaacab\n",
      "ccacacacb\n",
      "bacbca\n",
      "bbcbaaacbbaab\n",
      "cabbccccccbbbcb\n",
      "bcaaabbaacc\n",
      "baacabbababb\n",
      "abcbaac\n",
      "aababacabcacbabbc\n",
      "cacba\n",
      "cabbbcb\n",
      "ccaaaabbbbbc\n",
      "abbacccba\n",
      "acbcbccbaaacbbbbaa\n",
      "bbbcaccbbcbacbab\n",
      "cbaabcacaccacbaba\n",
      "cacbaabbaccbacabac\n",
      "ccbcbaccacbcacaa\n",
      "bcccb\n",
      "ccbccacabb\n",
      "abbabbabacbcc\n",
      "acacaacbcbbac\n",
      "bbbabaacbbbbaccaab\n",
      "abbaaabcccbcaabc\n",
      "cbcccaacccbbba\n",
      "caabcbbcabbcccb\n",
      "bacacbbbbaccabbca\n",
      "bbcbabbcaaba\n",
      "bccaccbbbcbb\n",
      "ccccacccbbccbcccaab\n",
      "cbbbcabaaacaccb\n",
      "aaaaac\n",
      "bbbabaaabacbababaab\n",
      "bbcbbccaac\n",
      "cabcbccbbcc\n",
      "aaacabcbbbbbca\n",
      "ccaabacaaca\n",
      "baabbabacbaacba\n",
      "bbacbcbb\n",
      "acbacabcbaaabbacaac\n",
      "aacbbbabaabcba\n",
      "abbacbccccccbabba\n",
      "cbaaacababbacab\n",
      "acaacabcacbb\n",
      "bbccaacbbababcaacca\n",
      "cbcbaaacaaaacccaac\n",
      "bbcacbb\n",
      "baacbcaaa\n",
      "ccacbcaaabbcbc\n",
      "babbabbaacabbbab\n",
      "accabbabc\n",
      "cbbabac\n",
      "abacbaababba\n",
      "cbccaabcabccb\n",
      "ccaaacac\n",
      "accccabac\n",
      "cbbaabcb\n",
      "baacbcc\n",
      "aabcaacbaab\n",
      "caaabbcc\n",
      "bcbbc\n",
      "bbbbcab\n",
      "acaaaaacac\n",
      "abbcaabaccbbcccca\n",
      "caacabacbbcab\n",
      "bcacacc\n",
      "bacabcbccbcca\n",
      "cbbbbccaab\n",
      "aaabbaacabbbba\n",
      "aabbaba\n",
      "acccccbabccabbcccbc\n",
      "abbbcacbbbcbcccbca\n",
      "bccacb\n",
      "acbaabacc\n",
      "bbaacac\n",
      "cbbaaaacccaaaa\n",
      "aabcccabccabaaaccb\n",
      "abbbbaccacaaab\n",
      "acabbbbc\n",
      "caaccacc\n",
      "cacbaccbacaccbcbb\n",
      "abbccccabaaa\n",
      "aacaabccba\n",
      "abaaccccbcaabcacca\n",
      "aacccacacabccabb\n",
      "cbabaababbbbab\n",
      "bcbcbbcbbbaacabba\n",
      "abcccccbccbccaca\n",
      "acaacabb\n",
      "bbcaccacacccc\n",
      "bcaaaaabab\n",
      "ccbbbabaaacca\n",
      "bbbbabbc\n",
      "abaaaa\n",
      "accabbbcbbab\n",
      "aacabababb\n",
      "ccbcbbbabca\n",
      "cabccbb\n",
      "acabaacaacabc\n",
      "bcbbaababbaccaab\n",
      "acbabbcacccccacaca\n",
      "caacb\n",
      "bbcbbcababcb\n",
      "aaaaccbc\n",
      "ccbbbcaabbcaacc\n",
      "bbacaaccccabcaa\n",
      "acaccbaabbb\n",
      "cbcbaaccabaccb\n",
      "ccbbbbcca\n",
      "acbcbbabcabbb\n",
      "baacccccbcab\n",
      "abaac\n",
      "caaacaaacbaccbbaca\n",
      "aaacaabbcbcac\n",
      "cbcbaabaaabcbbacc\n",
      "cacccbccbab\n",
      "cbbaabbcaacacaca\n",
      "ccacbbccacacacbaccc\n",
      "bcababcca\n",
      "caabaa\n",
      "bbcbba\n",
      "babcbabcccbaacccca\n",
      "cbcbbb\n",
      "ccabcbcbb\n",
      "ccaabbcc\n",
      "bbcacccbb\n",
      "accbcaaa\n",
      "aacaacccabaabbbc\n",
      "baaaacaaabbcabcba\n",
      "bbcbaaaaaaacbc\n",
      "bcacabacaacbca\n",
      "acbbcac\n",
      "acaaaaaaabcbcbc\n",
      "baaacacabccccac\n",
      "babacbcacacacbbbc\n",
      "bacbaccac\n",
      "baacabaabcbbb\n",
      "abbbbbabbc\n",
      "abccaabbbabccbc\n",
      "acaccbacbbacbcccbc\n",
      "acbaccaacbcccbcacb\n",
      "bbacbc\n",
      "bbbaabacccbcccbcaa\n",
      "bbcaacaabbcbb\n",
      "acbaccaabcccbba\n",
      "aaacbababbaaaba\n",
      "aabac\n",
      "bbcacbaacaaacb\n",
      "bbaaaccc\n",
      "bbbcabbbbabbbabbbc\n",
      "ccacaabaabca\n",
      "bbcccbcca\n",
      "cccabbcabacbacccb\n",
      "aaaac\n",
      "cabbabaccbccaaaabba\n",
      "bcbbcaa\n",
      "cbcbaaacccc\n",
      "caabbccba\n",
      "cabcccaabac\n",
      "aacacccab\n",
      "bacabb\n",
      "acbabbccccbcaccaccb\n",
      "bababaab\n",
      "cccbcbcacccbcbaaa\n",
      "abcbaccbccccbaac\n",
      "ccbaabaccabca\n",
      "cbcacccacbbccacbbbc\n",
      "bcaacabcccabbbbaa\n",
      "bcacbbbaaaaccc\n",
      "abbbbcaaccabbabcbca\n",
      "cbbbccbbcabaabca\n",
      "cabccbbbbbacbabbbb\n",
      "abbcabababbccabcccb\n",
      "aaacbacabbcaab\n",
      "ccabbcbacabbcccabb\n",
      "acbcacaabac\n",
      "ccaccaacabcaccc\n",
      "bbabcaaacabaabcbacb\n",
      "aaacacabcbcabccac\n",
      "abbcbabcbbabcbccaaa\n",
      "abaccabbab\n",
      "acacbcbcb\n",
      "abbcabb\n",
      "acbbaccbbbbbcbb\n",
      "cabacbabcc\n",
      "bcaacbbbb\n",
      "cabcccc\n",
      "abaacacabaabcc\n",
      "bcbbbbacaccbacbcbaa\n",
      "cbbcbccbb\n",
      "babccccbabbcababbcb\n",
      "aabbaaacbaabcab\n",
      "accbcbcabb\n",
      "bbcacbcabccbcccb\n",
      "aacabcbacbcbccca\n",
      "bbaababaaaabcacaa\n",
      "caabaca\n",
      "caacaacbca\n",
      "caacbcacbacb\n",
      "bbccc\n",
      "cbccaaccabbab\n",
      "acabbac\n",
      "cbcaaacabcabcab\n",
      "bbbaacb\n",
      "abbcacabaaaa\n",
      "bacacabcbcac\n",
      "baabaaaaabcbababcb\n",
      "cacacbaccaaabc\n",
      "cacbaccacac\n",
      "ababbaabcabcbcc\n",
      "cccacabbabcbabaa\n",
      "cbaabcbccaaccb\n",
      "babcaac\n",
      "bcccaccbcbcacbccc\n",
      "bbabbacabbbb\n",
      "aabbbbccb\n",
      "accbcaacba\n",
      "ababccccbcacbb\n",
      "cbaccbcabbacbcb\n",
      "cbbaacbbccaabbbbc\n",
      "cbbaacc\n",
      "ccabccaac\n",
      "cbbccbacbaccbbcabba\n",
      "cabbaa\n",
      "aacbcbabaaaaccabc\n",
      "bcacabbcbabcabaa\n",
      "bcaacbcb\n",
      "accbbcaaccacbacbb\n",
      "aabcccbbcbccbbcbb\n",
      "aaccccb\n",
      "babcbabacbbaa\n",
      "cbbbcbbbcbca\n",
      "cabbcabcbcbaababcba\n",
      "bccbabbca\n",
      "bccbaacbcc\n",
      "baccbab\n",
      "baaabcbbbbbcb\n",
      "acaacbcabb\n",
      "abccacbba\n",
      "cbccc\n",
      "bcccbab\n",
      "cbbca\n",
      "ccbaccbaac\n",
      "babccb\n",
      "babca\n",
      "abacabccbcccbc\n",
      "bcbbb\n",
      "cbcbc\n",
      "aaaabbc\n",
      "caccb\n",
      "bacbba\n",
      "aaaaaccaa\n",
      "cbcabacaaacbbcbbcb\n",
      "abbcbabacacccacaac\n",
      "aababccbaab\n",
      "bbcab\n",
      "cbbbbcacabaababccbc\n",
      "ccbabccca\n",
      "caaaabbaaabbabacabb\n",
      "abbbbaaacabcbbcc\n",
      "bccbbaaccbcccbcbac\n",
      "caacacbbbacbcccc\n",
      "aaabcab\n",
      "baaabcabb\n",
      "aacbabac\n",
      "aabbab\n",
      "bbcaccccacbbccabcba\n",
      "baacb\n",
      "bbbaababacccbca\n",
      "aaabbccacbbac\n",
      "ccababa\n",
      "accaabbaaba\n",
      "abccaabbacacccaa\n",
      "accbcba\n",
      "caaccbbbaaabccbcc\n",
      "bbcbcbba\n",
      "bbbaaabaacb\n",
      "cbcaa\n",
      "caaacbaacacaabcc\n",
      "abaca\n",
      "aacccbbacbaacbc\n",
      "ccaba\n",
      "bcbbcbcbbabacb\n",
      "babaababbbcccaba\n",
      "cbaabcacab\n",
      "abcbbcabaccb\n",
      "babccbbcbb\n",
      "ccaacbb\n",
      "bcabcbbccc\n",
      "aacbbcaac\n",
      "cbbabaaccaaca\n",
      "acacbcbabacbac\n",
      "acccaaab\n",
      "ccacbaaaacbbbbc\n",
      "cabacbbabb\n",
      "bbbcabcabcaacabbc\n",
      "caaabc\n",
      "acbcaacbacbacbcb\n",
      "abaacacacbcc\n",
      "cbaccbbcacaabacccb\n",
      "bcccaabaaccabc\n",
      "bcabbcabba\n",
      "ccabaacc\n",
      "ccaabccabaabaca\n",
      "baccbacb\n",
      "accabaabcccaccaa\n",
      "cccbacaac\n",
      "cabbcaaaa\n",
      "accabbbccaccacc\n",
      "ccabc\n",
      "aaababbbcbb\n",
      "cbacccbaca\n",
      "abaaa\n",
      "cbbbaaca\n",
      "caccaaaccbaabbbba\n",
      "ccbaccacaccbcb\n",
      "ccaababaccbacabbba\n",
      "aaacbaaccbacbacaab\n",
      "bbcaabbacbabbcbba\n",
      "cbabaccbbbccbbbbaab\n",
      "abbbbb\n",
      "bbbaaabcabacbbacbb\n",
      "abccab\n",
      "abcab\n",
      "accbcccc\n",
      "cacbb\n",
      "bacabcabccb\n",
      "bbaaccacaccbbcabab\n",
      "abaccacbbcabcccabcc\n",
      "abccabaccccabbbabbb\n",
      "cbbaaccccbcbacccab\n",
      "ccabcccaa\n",
      "ccbbaabbab\n",
      "acbbcab\n",
      "abcbaba\n",
      "bbcacaca\n",
      "bcacbabbbcab\n",
      "bcabcacbcbbbaa\n",
      "bcbabbcaca\n",
      "abbabaac\n",
      "acccacccabcacc\n",
      "baccbbccab\n",
      "cbacbccabaa\n",
      "bbabbbcbbbcb\n",
      "caacbbbbbc\n",
      "cabbcbaccba\n",
      "bcbbabbcbacbbcbaacc\n",
      "bbcabcccbbabbcaacc\n",
      "bcacbbac\n",
      "aaccca\n",
      "accbaaccccababab\n",
      "abbacbcabcccc\n",
      "baacaaacccaabbaca\n",
      "cbcba\n",
      "cbaca\n",
      "acbacac\n",
      "bbcba\n",
      "aaccbabaaccbbc\n",
      "ccabacc\n",
      "cacabbc\n",
      "bccaabcbabbaababb\n",
      "cbbacbcaabacbc\n",
      "abbaacbbacbcacbcac\n",
      "aaabacbabbacca\n",
      "ccccaa\n",
      "bccbb\n",
      "aaabcacbcabbcba\n"
     ]
    }
   ],
   "source": [
    "ds = ToyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.]]),\n",
       " array([4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 4, 4, 5, 5, 5, 5, 4, 2]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caaabbaabaaba\n",
    "abaabaabbaaac"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pub",
   "language": "python",
   "name": "env_pub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
