{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference: https://towardsdatascience.com/attention-seq2seq-with-pytorch-learning-to-invert-a-sequence-34faf4133e53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0+cu101'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_map = {\n",
    "    'a':'z',\n",
    "    'b':'y',\n",
    "    'c':'x',\n",
    "    'd':'w',\n",
    "    'e':'v',\n",
    "    'f':'u',\n",
    "    'g':'t',\n",
    "    'h':'s',\n",
    "    'i':'r',\n",
    "    'j':'q',\n",
    "    'k':'p',\n",
    "    'l':'o',\n",
    "    'm':'n',\n",
    "    'n':'m',\n",
    "    'o':'l',\n",
    "    'p':'k',\n",
    "    'q':'j',\n",
    "    'r':'i',\n",
    "    's':'h',\n",
    "    't':'g',\n",
    "    'u':'f',\n",
    "    'v':'e',\n",
    "    'w':'d',\n",
    "    'x':'c',\n",
    "    'y':'b',\n",
    "    'z':'a'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2i = {\n",
    "    '<s>':0,\n",
    "    '</s>':1,\n",
    "    #'<pad>':2,\n",
    "    'a':2,\n",
    "    'b':3,\n",
    "    'c':4,\n",
    "    'd':5,\n",
    "    'e':6,\n",
    "    'f':7,\n",
    "    'g':8,\n",
    "    'h':9,\n",
    "    'i':10,\n",
    "    'j':11,\n",
    "    'k':12,\n",
    "    'l':13,\n",
    "    'm':14,\n",
    "    'n':15,\n",
    "    'o':16,\n",
    "    'p':17,\n",
    "    'q':18,\n",
    "    'r':19,\n",
    "    's':20,\n",
    "    't':21,\n",
    "    'u':22,\n",
    "    'v':23,\n",
    "    'w':24,\n",
    "    'x':25,\n",
    "    'y':26,\n",
    "    'z':27,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2a = {v:k for k, v in a2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<s>',\n",
       " 1: '</s>',\n",
       " 2: 'a',\n",
       " 3: 'b',\n",
       " 4: 'c',\n",
       " 5: 'd',\n",
       " 6: 'e',\n",
       " 7: 'f',\n",
       " 8: 'g',\n",
       " 9: 'h',\n",
       " 10: 'i',\n",
       " 11: 'j',\n",
       " 12: 'k',\n",
       " 13: 'l',\n",
       " 14: 'm',\n",
       " 15: 'n',\n",
       " 16: 'o',\n",
       " 17: 'p',\n",
       " 18: 'q',\n",
       " 19: 'r',\n",
       " 20: 's',\n",
       " 21: 't',\n",
       " 22: 'u',\n",
       " 23: 'v',\n",
       " 24: 'w',\n",
       " 25: 'x',\n",
       " 26: 'y',\n",
       " 27: 'z'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_alphabet_index():\n",
    "    random_length = np.random.randint(5, MAX_LENGTH-2)    # -2 because of <s> and </s>\n",
    "    #random_length = 14\n",
    "    random_alphabet_index = np.random.randint(0, 26, random_length) + 2\n",
    "    return random_alphabet_index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetToyDataset(Dataset):\n",
    "    def __init__(self, n_dataset=1000):\n",
    "        bos = 0\n",
    "        eos = 1\n",
    "        pad = 2\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "        for _ in range(n_dataset):\n",
    "            # make input example\n",
    "            aindex = generate_random_alphabet_index()\n",
    "            \n",
    "            # index to alphabet\n",
    "            alphabet = list(map(lambda a: i2a[a], aindex))\n",
    "            \n",
    "            # inversing\n",
    "            inversed_alphabet = list(map(lambda a: inverse_map[a], alphabet))\n",
    "            \n",
    "            # alphabet to index\n",
    "            iindex = list(map(lambda ia: a2i[ia], inversed_alphabet))\n",
    "            \n",
    "            # add bos, eos and pad\n",
    "            n_pad = MAX_LENGTH - len(aindex) - 2\n",
    "            aindex = aindex + [eos]# + [pad]*n_pad\n",
    "            iindex = iindex + [eos]# + [pad]*n_pad\n",
    "            \n",
    "            # add to examples\n",
    "            self.inputs.append(aindex)\n",
    "            self.labels.append(iindex)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return [\n",
    "            torch.tensor(self.inputs[index], dtype=torch.long),\n",
    "            torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AlphabetToyDataset(n_dataset=3000)\n",
    "valid_dataset = AlphabetToyDataset(n_dataset=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_index_to_alphabet(index):\n",
    "    alphabet = list(map(lambda i: i2a[i], index))\n",
    "    return ''.join(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aindex_9: jpkdaofu</s>\n",
      "iindex_9: qkpwzluf</s>\n",
      "** aindex_9: tensor([11, 17, 12,  5,  2, 16,  7, 22,  1])\n",
      "** iindex_9: tensor([18, 12, 17, 24, 27, 13, 22,  7,  1])\n",
      "------------\n",
      "aindex_11: loinslfuve</s>\n",
      "iindex_11: olrmhoufev</s>\n",
      "** aindex_11: tensor([13, 16, 10, 15, 20, 13,  7, 22, 23,  6,  1])\n",
      "** iindex_11: tensor([16, 13, 19, 14,  9, 16, 22,  7,  6, 23,  1])\n",
      "------------\n",
      "aindex_6: lsytb</s>\n",
      "iindex_6: ohbgy</s>\n",
      "** aindex_6: tensor([13, 20, 26, 21,  3,  1])\n",
      "** iindex_6: tensor([16,  9,  3,  8, 26,  1])\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    ex = train_dataset[i]\n",
    "    aindex, iindex = ex\n",
    "    \n",
    "    print('aindex_{}: {}'.format(len(aindex), convert_index_to_alphabet(aindex.numpy())))\n",
    "    print('iindex_{}: {}'.format(len(iindex), convert_index_to_alphabet(iindex.numpy())))\n",
    "    print('** aindex_{}: {}'.format(len(aindex), aindex))\n",
    "    print('** iindex_{}: {}'.format(len(iindex), iindex))\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AlphabetEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.embedding(inputs)\n",
    "        #print('** embedding: {}'.format(self.embedding(input).shape))\n",
    "        #print('** embedded: {}'.format(embedded.shape))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(AlphabetDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = F.relu(embedded)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetAttentionDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length):\n",
    "        super(AlphabetAttentionDecoder, self).__init__()\n",
    "        # some parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attn = nn.Linear(2*self.hidden_size, max_length)\n",
    "        self.context = nn.Linear(2*self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, hidden, encoder_outputs):\n",
    "        '''\n",
    "        inputs: (1,B,1)\n",
    "        hidden: (1,B,H)\n",
    "        encoder_outputs: (M,B,H)\n",
    "        '''\n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = self.dropout(embedded)    # (1,B,H)\n",
    "        #print('** inputs → embedded: {} → {}'.format(inputs.shape, embedded.shape))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        #print('** encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "        \n",
    "        weight = F.softmax(\n",
    "            self.attn(\n",
    "                torch.cat((embedded[0], hidden[0]),    # (B,2H)\n",
    "                1)\n",
    "            ),    # (B,M)\n",
    "            1\n",
    "        )    # (B,M)\n",
    "        #print('** weight: {}'.format(weight.shape))\n",
    "        \n",
    "        #print('** weight.unsqueeze(1): {}'.format(weight.unsqueeze(1).shape))\n",
    "        #print('** encoder_outputs.transpose(0,1): {}'.format(encoder_outputs.transpose(0,1).shape))\n",
    "        weight_applied = torch.bmm(\n",
    "            weight.unsqueeze(1),    # (B,1,M)\n",
    "            encoder_outputs.transpose(0, 1)    # (B,M,H)\n",
    "        )    # (B,1,H)\n",
    "        #print('** weight_applied: {}'.format(weight_applied.shape))\n",
    "        \n",
    "        \n",
    "        new_context = self.context(\n",
    "            torch.cat(\n",
    "                (embedded[0].unsqueeze(1), weight_applied),\n",
    "                2\n",
    "            )    # (B,1,2H)\n",
    "        )    # (B,1,H)\n",
    "        new_context = F.relu(new_context)    # (B,1,H)\n",
    "        new_context = new_context.transpose(0, 1)    # (1,B,H)\n",
    "        #print('** new_context: {}'.format(new_context.shape))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        \n",
    "        output, hidden = self.gru(\n",
    "            new_context,    # (1,B,H)\n",
    "            hidden    # (1,B,H)\n",
    "        )    # output=(1,B,H), hidden=(1,B,H)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        \n",
    "        output = self.out(output)    # (1,B,O)\n",
    "        #print(output)\n",
    "        output = F.log_softmax(output, 2)    # (1,B,O)\n",
    "        #print(output)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        #print('** weight: {}'.format(weight.shape))\n",
    "        #print('** weight sum: {}'.format(weight.sum()))\n",
    "        #print('--------------------------------------')\n",
    "        \n",
    "        return output, hidden, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AlphabetAttentionDecoder(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size):\n",
    "#         super(AlphabetAttentionDecoder, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "#         self.linear_query = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "#         self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "#     def forward(self, inputs, hidden, encoder_outputs):\n",
    "#         '''\n",
    "#         inputs: (B,M)\n",
    "#         hidden: (1,B,H)\n",
    "#         encoder_outputs: (M,B,H)\n",
    "#         '''\n",
    "#         embedded = self.embedding(inputs)\n",
    "#         embedded = self.dropout(embedded)    # (B,M,H)\n",
    "        \n",
    "#         print('embedded: {}'.format(embedded.shape))\n",
    "#         print('hidden: {}'.format(hidden.shape))\n",
    "#         print('encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "#         return\n",
    "        \n",
    "#         #print(hidden.shape, hidden.dtype)\n",
    "#         query = self.linear_query(hidden)    # (1,B,H)\n",
    "#         #print(query.transpose(0,1).shape, embedded.transpose(1,2).shape)\n",
    "#         #return\n",
    "#         weight = torch.bmm(query.transpose(0,1), embedded.transpose(1,2))    # (B,1,M)\n",
    "#         weight = self.softmax(weight)    # (B,1,M)\n",
    "#         print('** weight: {}'.format(weight.shape))\n",
    "#         print('** encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "#         return\n",
    "        \n",
    "#         context = torch.bmm(weight, encoder_outputs.transpose(0,1))    # (B,1,H)\n",
    "#         context = context.transpose(0,1)\n",
    "        \n",
    "#         output, hidden = self.gru(embedded.transpose(0,1), context)    # (M,B,H), (1,B,H)\n",
    "#         output = self.out(output[0])    # (M,B,H), we do it by assuming M=1\n",
    "#         output = F.log_softmax(output, 1)    # (M,B,H)\n",
    "#         #output = self.out(output)\n",
    "#         #print(output.shape, hidden.shape)\n",
    "#         return output, hidden, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.from_numpy(np.array([\n",
    "#     [0,1,2,3],\n",
    "#     [0,1,2,3],\n",
    "#     [0,1,2,3]\n",
    "# ]))    # (3,4)\n",
    "# inp = inp.long()    # (3,4)\n",
    "# hid = torch.rand((1,3,5))    # (1,3,5)\n",
    "# eout = torch.rand((4,3,5))    # (4,3,5)\n",
    "# print(inp.shape, hid.shape, eout.shape)\n",
    "# att = AlphabetAttentionDecoder(hidden_size=5, output_size=4)\n",
    "# o,h,w = att(inp, hid, eout)\n",
    "# print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.from_numpy(np.array([\n",
    "#     [0],\n",
    "#     [2],\n",
    "#     [3]\n",
    "# ]))    # (3,4)\n",
    "# inp = inp.long()\n",
    "# hid = torch.rand((1,3,5))    # (1,3,5)\n",
    "# eout = torch.rand((1,3,5))\n",
    "# print(inp.shape, hid.shape, eout.shape)\n",
    "# att = AlphabetAttentionDecoder(hidden_size=5, output_size=4)\n",
    "# o,h,w = att(inp, hid, eout)\n",
    "# print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AlphabetAttentionDecoder(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, max_length):\n",
    "#         super(AlphabetAttentionDecoder, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.max_length = max_length\n",
    "        \n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "#         self.weight_combine = nn.Linear(2*self.hidden_size, self.max_length)\n",
    "#         self.attention_combine = nn.Linear(2*self.hidden_size, self.hidden_size)\n",
    "#         #self.softmax = nn.LogSoftmax(dim=1)\n",
    "#         self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "#     def forward(self, inputs, hidden, encoder_outputs):\n",
    "#         # embedding\n",
    "#         embedded = self.embedding(inputs)\n",
    "#         embedded = self.dropout(embedded)\n",
    "\n",
    "#         # making attention weight\n",
    "#         concat_hidden = torch.cat((embedded[0], hidden[0]), 1)\n",
    "#         attention_weight = F.softmax(self.weight_combine(concat_hidden), 1).unsqueeze(1)\n",
    "        \n",
    "#         # applying attention weight\n",
    "#         encoder_outputs_transposed = encoder_outputs.transpose(1, 0)\n",
    "#         attention_applied = torch.bmm(attention_weight, encoder_outputs_transposed)\n",
    "        \n",
    "#         # making new context and new input\n",
    "#         new_context = torch.cat((embedded[0], attention_applied.squeeze(1)), dim=1)\n",
    "#         output = self.attention_combine(new_context)\n",
    "#         output = F.relu(output).unsqueeze(0)\n",
    "        \n",
    "#         # running gru\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "#         # making output\n",
    "#         output = self.out(output)\n",
    "#         output = F.log_softmax(output[0], 1)\n",
    "    \n",
    "#         return output, hidden, attention_weight\n",
    "\n",
    "#     def initHidden(self, batch_size):\n",
    "#         return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder = AlphabetEncoder(26+2, hidden_size).to(device)\n",
    "#decoder = AlphabetDecoder(hidden_size, 26+3).to(device)\n",
    "decoder = AlphabetAttentionDecoder(hidden_size, 26+2, MAX_LENGTH-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_t = inputs.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# encoder_hidden = encoder.initHidden(batch_size)\n",
    "# encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = 0\n",
    "eos = 1\n",
    "pad = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eout = torch.rand(4, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.4022)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eout[:,0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eout_trans = eout.transpose(0,1)\n",
    "eout_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.4022)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eout_trans[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,1,5)\n",
    "b = a.transpose(1,0)\n",
    "c = a.squeeze(1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 5]), torch.Size([1, 3, 5]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b == c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=5):\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    # zero_grad for encoder/decoder optimizer\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # zero_grad for encoder/decoder optimizer\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        loss_avg = 0\n",
    "        tbar = tqdm(enumerate(dataloader), desc='training {}th epoch'.format(epoch))\n",
    "        ####################################\n",
    "        for i, batch in tbar:\n",
    "\n",
    "            # get inputs and labels\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # transpose inputs and labels\n",
    "            inputs = inputs.transpose(1, 0)\n",
    "            labels = labels.transpose(1, 0)\n",
    "\n",
    "            # initialize hidden for encoder\n",
    "            batch_size = inputs.size()[1]\n",
    "            max_length = inputs.size()[0]\n",
    "            max_length = MAX_LENGTH - 1\n",
    "            encoder_hidden = encoder.initHidden(batch_size)\n",
    "            encoder_outputs = torch.zeros(max_length, batch_size, hidden_size, device=device)\n",
    "            \n",
    "            # encoding\n",
    "            for j, inp in enumerate(inputs):\n",
    "                inp = inp.unsqueeze(0)\n",
    "                encoder_output, encoder_hidden = encoder(inp, encoder_hidden)\n",
    "                encoder_outputs[j] = encoder_output[0]\n",
    "                \n",
    "            # initialize hidden for decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            loss = 0\n",
    "\n",
    "            decoder_inputs = torch.tensor([[bos]*batch_size], device=device)    # (1,B)\n",
    "            #decoder_inputs = decoder_inputs.transpose(0,1)    # (B,1)\n",
    "\n",
    "            teacher_forcing = True if np.random.random() < 0.5 else False\n",
    "            #teacher_forcing = True\n",
    "            # decoding\n",
    "            for inp in labels:\n",
    "                if teacher_forcing:\n",
    "                    decoder_output, decoder_hidden, attention_weight = decoder(decoder_inputs, decoder_hidden, encoder_outputs)\n",
    "                    loss_it = criterion(decoder_output[0], inp)    # criterion((B,O), (B))\n",
    "                    loss += loss_it\n",
    "                    decoder_inputs = inp.unsqueeze(0)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, attention_weight = decoder(decoder_inputs, decoder_hidden, encoder_outputs)\n",
    "                    loss_it = criterion(decoder_output[0], inp)\n",
    "                    loss += loss_it\n",
    "                    topv, topi = decoder_output.topk(1)\n",
    "                    decoder_inputs = topi.squeeze(2)\n",
    "                    \n",
    "            # backward\n",
    "            loss.backward()\n",
    "            encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "            decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "            \n",
    "            # update encoder/decoder\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            encoder_sum = sum([p[1].data.sum() for p in encoder.named_parameters()])\n",
    "            decoder_sum = sum([p[1].data.sum() for p in decoder.named_parameters()])\n",
    "            \n",
    "            tbar.set_postfix(loss=loss.data.item())\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 0th epoch: 175it [00:08, 21.93it/s, loss=3.03e+3]"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, train_dataloader, encoder_optimizer, decoder_optimizer, criterion, n_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, dataloader):\n",
    "    total_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # get inputs and labels\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #print('** inputs[0]: {}'.format(inputs[0]))\n",
    "            #print('** labels[0]: {}'.format(labels[0]))\n",
    "\n",
    "            # transpose inputs and labels\n",
    "            inputs = inputs.transpose(1, 0)\n",
    "            labels = labels.transpose(1, 0)\n",
    "\n",
    "            # initialize hidden for encoder\n",
    "            batch_size = inputs.size()[1]\n",
    "            encoder_hidden = encoder.initHidden(batch_size)\n",
    "            encoder_outputs = torch.zeros(MAX_LENGTH, batch_size, hidden_size, device=device)\n",
    "            \n",
    "            # encoding\n",
    "            for j, inp in enumerate(inputs):\n",
    "                inp = inp.unsqueeze(0)\n",
    "                encoder_output, encoder_hidden = encoder(inp, encoder_hidden)\n",
    "                #print('** encoder_output_{}: {}'.format(i, encoder_output.shape))\n",
    "                encoder_outputs[j] = encoder_output[0]\n",
    "            \n",
    "            # initialize hidden for decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_inputs = torch.tensor([[bos]*batch_size], device=device)\n",
    "            #decoder_inputs = decoder_inputs.transpose(0,1)    # (B,1)\n",
    "            \n",
    "            pred = []\n",
    "            # decoding\n",
    "            for j, inp in enumerate(labels):\n",
    "                #print('** decoder_inputs: {}'.format(decoder_inputs.shape))\n",
    "                decoder_output, decoder_hidden, attention_weight = decoder(decoder_inputs, decoder_hidden, encoder_outputs[j].unsqueeze(0))\n",
    "                #print('** decoder_output: {}'.format(decoder_output.shape))\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                #print('** new decoder_inputs: {}'.format(decoder_inputs.shape))\n",
    "                pred.append(topi.cpu().numpy().flatten().tolist())\n",
    "                decoder_inputs = topi\n",
    "\n",
    "            # re-transpose for validation\n",
    "            inputs = inputs.transpose(1, 0)\n",
    "            labels = labels.transpose(1, 0).cpu().numpy()\n",
    "            #return\n",
    "\n",
    "            # stack-up prediction for validation\n",
    "            pred = np.array(pred)\n",
    "            pred = np.transpose(pred)\n",
    "            #print(labels.shape, pred.shape)\n",
    "            #pred = np.stack(pred, axis=1)\n",
    "            accuracy = (pred == labels).astype(np.int).mean()\n",
    "            total_accuracy += accuracy\n",
    "            #print('{} VS {} → {:.4f}'.format(labels.shape, pred.shape, accuracy))\n",
    "            \n",
    "        total_accuracy = total_accuracy/(i+1)\n",
    "        print('total accuracy: {:.4f}'.format(total_accuracy))\n",
    "        return labels, pred, total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 6: wrong matrix size at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-37c2f3da0be4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-55aa4eab9915>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(encoder, decoder, dataloader)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m#print('** decoder_inputs: {}'.format(decoder_inputs.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0;31m#print('** decoder_output: {}'.format(decoder_output.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/publish/env_pub/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-42bf0a192ac5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         weight_applied = torch.bmm(\n\u001b[1;32m     44\u001b[0m             \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# (B,1,M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# (B,M,H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         )    # (B,1,H)\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#print('** weight_applied: {}'.format(weight_applied.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 6: wrong matrix size at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:41"
     ]
    }
   ],
   "source": [
    "labels, pred, acc = validate(encoder, decoder, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933483368483371"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  4, 20, 18, 13, 10,  8, 13,  3, 16, 11, 25,  1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5],\n",
       "        [ 4],\n",
       "        [20],\n",
       "        [18],\n",
       "        [13],\n",
       "        [10],\n",
       "        [ 8],\n",
       "        [13],\n",
       "        [ 3],\n",
       "        [16],\n",
       "        [11],\n",
       "        [25],\n",
       "        [ 1]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(sequence):\n",
    "    label = list(map(lambda x:inverse_map[x], sequence))\n",
    "    label = list(map(lambda x:a2i[x], label))\n",
    "    label_string = convert_index_to_alphabet(label)\n",
    "    \n",
    "    # print input and expected output\n",
    "    print('** input sequence: {}'.format(sequence))\n",
    "    print('** label sequence: {}'.format(label_string))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sequence = list(map(lambda s: a2i[s], sequence))\n",
    "        inputs = torch.tensor(sequence)\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        sequence_length = inputs.shape[0]\n",
    "        batch_size = 1\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, batch_size, hidden_size, device=device)\n",
    "\n",
    "        # encoding\n",
    "        for j, inp in enumerate(inputs):\n",
    "            inp = inp.unsqueeze(0)\n",
    "            #print(inp.shape, encoder_hidden.shape)\n",
    "            #return\n",
    "            encoder_output, encoder_hidden = encoder(inp, encoder_hidden)\n",
    "            #print('** encoder_output_{}: {}'.format(i, encoder_output.shape))\n",
    "            encoder_outputs[j] = encoder_output[0]\n",
    "        \n",
    "        # initialize hidden for decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_inputs = torch.tensor([[a2i['<s>']]*batch_size], device=device)\n",
    "        #decoder_inputs = decoder_inputs.transpose(0,1)    # (B,1)\n",
    "\n",
    "        pred = []\n",
    "        attention_weights = []\n",
    "        # decoding\n",
    "        for j in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attention_weight = decoder(decoder_inputs, decoder_hidden, encoder_outputs[j].unsqueeze(0))\n",
    "            #print('** decoder_output: {}'.format(decoder_output.shape))\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            print(attention_weight)\n",
    "            attention_weights.append(attention_weight)\n",
    "            \n",
    "            if topi.item() == a2i['</s>'] or sequence_length == j:\n",
    "                pred.append(a2i['</s>'])\n",
    "                break\n",
    "            else:\n",
    "                #print('** new decoder_inputs: {}'.format(decoder_inputs.shape))\n",
    "                #pred.append(topi.cpu().numpy().flatten().tolist())\n",
    "                pred.append(topi.item())\n",
    "                decoder_inputs = topi\n",
    "        \n",
    "        pred = np.array(pred).flatten()[:-1]    # remove the last eos character\n",
    "        converted_string = convert_index_to_alphabet(pred)\n",
    "        attention_weights = np.array(attention_weights)\n",
    "        print('** pred  sequence: {}'.format(converted_string))\n",
    "        \n",
    "        return converted_string, attention_weights\n",
    "        #return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** input sequence: abcd\n",
      "** label sequence: zyxw\n",
      "tensor([[[1.]]], device='cuda:0')\n",
      "tensor([[[1.]]], device='cuda:0')\n",
      "tensor([[[1.]]], device='cuda:0')\n",
      "tensor([[[1.]]], device='cuda:0')\n",
      "tensor([[[1.]]], device='cuda:0')\n",
      "** pred  sequence: zyxw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o, attention_weights = predict_sequence('abcd')\n",
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.]]], device='cuda:0')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder-decoder-20201221-5120.bin'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randidx = '{}'.format(np.random.randint(0, 10000)).zfill(4)\n",
    "yymmdd = datetime.datetime.now().strftime('%Y%m%d')\n",
    "model_nm = os.path.join('encoder-decoder-{}-{}.bin'.format(yymmdd, randidx))\n",
    "model_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_state = encoder.state_dict()\n",
    "decoder_state = decoder.state_dict()\n",
    "torch.save({\n",
    "    'encoder_state': encoder_state,\n",
    "    'decoder_state': decoder_state,\n",
    "}, model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *************** NMT ***************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 다운로드 → https://download.pytorch.org/tutorial/data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고 → https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # SOS 와 EOS 포함\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유니 코드 문자열을 일반 ASCII로 변환하십시오.\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소문자, 다듬기, 그리고 문자가 아닌 문자 제거\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # 파일을 읽고 줄로 분리\n",
    "    lines = open('../data/attention-ntm/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # 모든 줄을 쌍으로 분리하고 정규화\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # 쌍을 뒤집고, Lang 인스턴스 생성\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without Attention\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        print('DecoderRNN_INPUT: input: {} → {:.4f}'.format(input.shape, input.sum()))\n",
    "        print('DecoderRNN_INPUT: hidden: {} → {:.4f}'.format(hidden.shape, hidden.sum()))\n",
    "        \n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        print('** output: {} → {:.4f}'.format(output.shape, output.sum()))\n",
    "        \n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        print('** gru-output: {} → {:.4f}'.format(output.shape, output.sum()))\n",
    "        print('** gru-hidden: {} → {:.4f}'.format(hidden.shape, hidden.sum()))\n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        #print('** hidden: {}'.format(hidden.shape))\n",
    "        \n",
    "        print('DecoderRNN_OUTPUT: output: {} → {:.4f}'.format(output.shape, output.sum()))\n",
    "        print('DecoderRNN_OUTPUT: hidden: {} → {:.4f}'.format(hidden.shape, hidden.sum()))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with attention\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        print('AttnDecoderRNN_INPUT: input → embedded: {} → {}'.format(input.shape, embedded.shape))\n",
    "        print('AttnDecoderRNN_INPUT: hidden: {}'.format(hidden.shape))\n",
    "        print('AttnDecoderRNN_INPUT: encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "        \n",
    "        #print('** embedded[0]: {}'.format(embedded[0].shape))\n",
    "        #print('** hidden[0]: {}'.format(hidden[0].shape))\n",
    "        #print('** concatenate embedded[0] and hidden[0]: {}'.format(torch.cat((embedded[0], hidden[0]), 1).shape))\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        #print('** attn_weights: {}'.format(attn_weights.shape))\n",
    "        #print('** attn_weights: {}'.format(attn_weights))\n",
    "        #print('** attn_weights.sum(): {}'.format(attn_weights.sum().shape))\n",
    "        #print('** attn_weights.sum(): {:.4f}'.format(attn_weights.sum()))\n",
    "        \n",
    "        #print('** attn_weights unsqueezed: {}'.format(attn_weights.unsqueeze(0).shape))\n",
    "        #print('** encoder_outputs unsqueezed: {}'.format(encoder_outputs.unsqueeze(0).shape))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        #print('** attn_applied: {}'.format(attn_applied.shape))\n",
    "        #print('** attn_applied[0]: {}'.format(attn_applied[0].shape))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        #print('** output: {}'.format(output.shape))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #print('** gru-output: {}'.format(output.shape))\n",
    "        #print('** gru-hidden: {}'.format(hidden.shape))\n",
    "\n",
    "        #print('** output: {}'.format(output[0].shape))\n",
    "        #print('** self.out: {}'.format(self.out))\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        print('AttnDecoderRNN_OUTPUT: {}'.format(output.shape))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH, with_attention=True):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    print('** input_tensor[0]: {}'.format(input_tensor[0]))\n",
    "    print('** target_tensor[0]: {}'.format(target_tensor[0]))\n",
    "    return\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    print('encoder_outputs: {}'.format(encoder_outputs.shape))\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        print('{} - {}'.format(ei, encoder_output[0, 0].shape))\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    print('decoder_hidden: {}'.format(decoder_hidden.shape))\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing 포함: 목표를 다음 입력으로 전달\n",
    "        for di in range(target_length):\n",
    "            if with_attention:\n",
    "                print('# {}th decoding'.format(di))\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                print('train:decoder_attention_{}: {}'.format(di, decoder_attention.shape))\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            \n",
    "\n",
    "    else:\n",
    "        # Teacher forcing 미포함: 자신의 예측을 다음 입력으로 사용\n",
    "        for di in range(target_length):\n",
    "            if with_attention:\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                print('train:decoder_attention_{}: {}'.format(di, decoder_attention.shape))\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # 입력으로 사용할 부분을 히스토리에서 분리\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01, with_attention=True):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # print_every 마다 초기화\n",
    "    plot_loss_total = 0  # plot_every 마다 초기화\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        print(input_tensor.shape, target_tensor.shape)\n",
    "        print(input_tensor)\n",
    "        print(target_tensor)\n",
    "        print('----------------------')\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion, with_attention=with_attention)\n",
    "        return\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # 주기적인 간격에 이 locator가 tick을 설정\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 256\n",
    "# encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "# decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Input validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainIters(encoder1, decoder1, 75000, print_every=5000, with_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # colorbar로 그림 설정\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # 축 설정\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # 매 틱마다 라벨 보여주기\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "#evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "#evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "#evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from random import choice, randrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ToyDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Inspired from https://talbaumel.github.io/blog/attention/\n",
    "    \"\"\"\n",
    "    def __init__(self, min_length=5, max_length=20, type='train'):\n",
    "        self.SOS = \"<s>\"  \n",
    "        self.EOS = \"</s>\" \n",
    "        self.characters = list(\"abcd\")\n",
    "        self.int2char = list(self.characters)\n",
    "        self.char2int = {c: i+3 for i, c in enumerate(self.characters)}\n",
    "        self.VOCAB_SIZE = len(self.characters)\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        if type=='train':\n",
    "            self.set = [self._sample() for _ in range(3000)]\n",
    "        else:\n",
    "            self.set = [self._sample() for _ in range(300)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.set)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.set[item]\n",
    "\n",
    "    def _sample(self):\n",
    "        random_length = randrange(self.min_length, self.max_length)# Pick a random length\n",
    "        random_char_list = [choice(self.characters[:-1]) for _ in range(random_length)]  # Pick random chars\n",
    "        random_string = ''.join(random_char_list)\n",
    "        print(random_string)\n",
    "        a = np.array([self.char2int.get(x) for x in random_string])\n",
    "        b = np.array([self.char2int.get(x) for x in random_string[::-1]] + [2]) # Return the random string and its reverse\n",
    "        x = np.zeros((random_length, self.VOCAB_SIZE))\n",
    "        x[np.arange(random_length), a-3] = 1\n",
    "        return x, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ToyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caaabbaabaaba\n",
    "abaabaabbaaac"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pub",
   "language": "python",
   "name": "env_pub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
