{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2022563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4de0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_sample = False\n",
    "valid_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e8dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(n_positions=1024//2, n_ctx=1024//2, n_embd=768//6, n_layer=12//6, n_head=12//6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c091fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('./gpt2')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f64601c0",
   "metadata": {},
   "source": [
    ">>> tokenizer('i like to go', return_tensors='pt')\n",
    "{'input_ids': tensor([[ 72, 588, 284, 467]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee707f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config).cuda()\n",
    "# model = GPT2LMHeadModel.from_pretrained('./gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb94cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs, labels=inputs[\"input_ids\"], output_attentions=True)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdf7d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget http://www.statmt.org/europarl/v7/fr-en.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d49e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar zxvf fr-en.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a6b839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 jkfirst jkfirst  30869430 10월  8 12:30 europarl-v7.fr-en.test.en\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst  35007266 10월  8 12:31 europarl-v7.fr-en.test.fr\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst 209887699 10월  8 12:26 europarl-v7.fr-en.train.en\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst 242204275 10월  8 12:26 europarl-v7.fr-en.train.fr\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst  60766172 10월  8 12:30 europarl-v7.fr-en.valid.en\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst  69708260 10월  8 12:31 europarl-v7.fr-en.valid.fr\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst     13743 10월  8 12:21 sample.test.en\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst     15524 10월  8 12:22 sample.test.fr\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst    119064 10월  8 12:20 sample.train.en\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst    134973 10월  8 12:21 sample.train.fr\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst     32738 10월  8 12:21 sample.valid.en\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst     35821 10월  8 12:22 sample.valid.fr\r\n"
     ]
    }
   ],
   "source": [
    "! ls -al *.en *.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea971698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   207723 europarl-v7.fr-en.test.en\n",
      "   207723 europarl-v7.fr-en.test.fr\n",
      "  1400000 europarl-v7.fr-en.train.en\n",
      "  1400000 europarl-v7.fr-en.train.fr\n",
      "   400000 europarl-v7.fr-en.valid.en\n",
      "   400000 europarl-v7.fr-en.valid.fr\n",
      "  4015446 total\n"
     ]
    }
   ],
   "source": [
    "! wc -l europarl-v7.fr-en.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56ea66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   100 sample.test.en\r\n",
      "   100 sample.test.fr\r\n",
      "   700 sample.train.en\r\n",
      "   700 sample.train.fr\r\n",
      "   200 sample.valid.en\r\n",
      "   200 sample.valid.fr\r\n",
      "  2000 total\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe93149a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> europarl-v7.fr-en.test.en <==\r\n",
      "Having personally participated in a parliamentary delegation led by the Member, Gutiérrez Díaz, at the Intergovernmental Conference in Malta in 1997, we recognize that a lot of problems, including political ones, in certain areas of the Mediterranean basin are an obstacle or at least a brake on the launching of substantial cooperation.\r\n",
      "\r\n",
      "==> europarl-v7.fr-en.test.fr <==\r\n",
      "Ayant personnellement participé à la délégation parlementaire présidée par M. Gutiérrez Díaz à la conférence intergouvernementale de Malte en 1997, nous reconnaissons que de nombreux problèmes, d'ordre politique aussi, constituent, dans certaines zones du bassin méditerranéen, un obstacle, ou pour le moins un frein à la mise en oeuvre d'une véritable coopération.\r\n",
      "\r\n",
      "==> europarl-v7.fr-en.train.en <==\r\n",
      "Resumption of the session\r\n",
      "\r\n",
      "==> europarl-v7.fr-en.train.fr <==\r\n",
      "Reprise de la session\r\n",
      "\r\n",
      "==> europarl-v7.fr-en.valid.en <==\r\n",
      "Indeed, it helps clarify the means of redress available in the event of flagrant breaches of intellectual property rights in one of the countries party to the agreement.\r\n",
      "\r\n",
      "==> europarl-v7.fr-en.valid.fr <==\r\n",
      "En effet, il permet de clarifier les recours possible en cas de violation flagrante de la propriété intellectuelle dans un des pays Partie à l'accord.\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 europarl-v7.fr-en.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5132ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> sample.test.en <==\r\n",
      "In six months' time, it may be appropriate to carry out an analysis of the outcome and also to look more closely at the new situation' s effects upon the Commission' s role.\r\n",
      "\r\n",
      "==> sample.test.fr <==\r\n",
      "Il pourrait être judicieux de procéder, vers le milieu de l'année, à une analyse des résultats, mais aussi d' examiner de plus près l' incidence de la nouvelle situation sur le rôle de la Commission.\r\n",
      "\r\n",
      "==> sample.train.en <==\r\n",
      "Resumption of the session\r\n",
      "\r\n",
      "==> sample.train.fr <==\r\n",
      "Reprise de la session\r\n",
      "\r\n",
      "==> sample.valid.en <==\r\n",
      "I should also like to make a few comments, firstly, Mr Berend, regarding the assessment you have made of this sixth periodic report.\r\n",
      "\r\n",
      "==> sample.valid.fr <==\r\n",
      "Je voudrais à mon tour faire quelques observations, d'abord sur le jugement que vous portez, Monsieur le Rapporteur, sur ce sixième rapport périodique.\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5170af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sample:\n",
    "    total_rows = 1000\n",
    "else:\n",
    "    total_rows = 2007723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aa0635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = total_rows * 70 // 100\n",
    "n_valid = total_rows * 20 // 100\n",
    "n_test = total_rows - n_train - n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c30ead5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405406, 401544, 200773, True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train, n_valid, n_test, n_train+n_valid+n_test==total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b14eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_support = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3dc9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset():\n",
    "    def __init__(self, src_filename, tgt_filename, tokenizer, n_support):\n",
    "        '''\n",
    "        src_filename: french\n",
    "        tgt_filename: english\n",
    "        '''\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_support = n_support\n",
    "        self.task = 'Translate French to English'\n",
    "        self.task_delim = ' : '\n",
    "        self.data_delim = ' => '\n",
    "        \n",
    "        print(f'reading src: {src_filename}')\n",
    "        src = self.read_file(src_filename)\n",
    "        print(f'reading tgt: {tgt_filename}')\n",
    "        tgt = self.read_file(tgt_filename)\n",
    "        self.text = self.concatenate_with_delim(src, tgt)\n",
    "        self.text = [self.task + self.task_delim + t for t in self.text]\n",
    "        \n",
    "        # make input_ids and attention_mask\n",
    "        self.input_tensor = list(map(lambda t: self.tokenizer(t, return_tensors='pt'), self.text))\n",
    "        # self.input_ids = [tensor['input_ids'] for tensor in input_tensor]\n",
    "        # self.attention_mask = [tensor['attention_mask'] for tensor in input_tensor]\n",
    "        \n",
    "#         self.src = src\n",
    "#         self.src_attention = src_attention\n",
    "#         self.tgt = tgt\n",
    "#         self.tgt_attention = tgt_attention\n",
    "        \n",
    "    def concatenate_with_delim(self, src, tgt):\n",
    "        text_list = []\n",
    "        cnt = 0\n",
    "        text = ''\n",
    "        for s, t in zip(src, tgt):\n",
    "            text += f'{s} => {t}'\n",
    "            if cnt%self.n_support == self.n_support - 1:\n",
    "                text_list.append(text)\n",
    "                cnt = -1\n",
    "                text = ''\n",
    "            else:\n",
    "                text += ' | '\n",
    "            \n",
    "            cnt += 1\n",
    "        return text_list\n",
    "            \n",
    "    def read_file(self, filename):\n",
    "        text_list = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for oneline in tqdm(f, desc=f'reading {filename}'):\n",
    "                oneline = oneline.rstrip()\n",
    "                text_list.append(oneline)\n",
    "        return text_list\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_tensor[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9421829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading src: europarl-v7.fr-en.train.fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading europarl-v7.fr-en.train.fr: 1400000it [00:01, 845276.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading tgt: europarl-v7.fr-en.train.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading europarl-v7.fr-en.train.en: 1400000it [00:01, 1281332.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading src: europarl-v7.fr-en.valid.fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading europarl-v7.fr-en.valid.fr: 400000it [00:00, 971274.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading tgt: europarl-v7.fr-en.valid.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading europarl-v7.fr-en.valid.en: 400000it [00:00, 1450023.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading src: europarl-v7.fr-en.test.fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading europarl-v7.fr-en.test.fr: 207723it [00:00, 1117934.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading tgt: europarl-v7.fr-en.test.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading europarl-v7.fr-en.test.en: 207723it [00:00, 1440752.69it/s]\n"
     ]
    }
   ],
   "source": [
    "if use_sample:\n",
    "    train_dataset = TranslationDataset('sample.train.fr', 'sample.train.en', tokenizer, n_support)\n",
    "    valid_dataset = TranslationDataset('sample.valid.fr', 'sample.valid.en', tokenizer, n_support)\n",
    "    test_dataset = TranslationDataset('sample.test.fr', 'sample.test.en', tokenizer, n_support)\n",
    "else:\n",
    "    train_dataset = TranslationDataset('europarl-v7.fr-en.train.fr', 'europarl-v7.fr-en.train.en', tokenizer, n_support)\n",
    "    valid_dataset = TranslationDataset('europarl-v7.fr-en.valid.fr', 'europarl-v7.fr-en.valid.en', tokenizer, n_support)\n",
    "    test_dataset = TranslationDataset('europarl-v7.fr-en.test.fr', 'europarl-v7.fr-en.test.en', tokenizer, n_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f192726f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate French to English : La résolution n\\'a aucun effet formel mais elle exprime tout simplement les sentiments du Parlement. => The resolution has no formal effect at all, but is merely an expression of how Parliament feels. | Le vote décisif aura (probablement) lieu vers le début de l\\'année prochaine lorsque le Parlement sera invité à donner son approbation à l\\'accord. => The deciding vote will (probably) be some time early next year, when Parliament will be asked to give its consent to the agreement. | Si le \"non\" l\\'emporte, l\\'accord sera relégué aux oubliettes. => If we get a No vote then, the agreement will be scrapped.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.text[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79779c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5142e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for b in batch:\n",
    "        input_ids.append(b['input_ids'][0][:512])\n",
    "        attention_mask.append(b['attention_mask'][0][:512])\n",
    "    \n",
    "    # padding\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "\n",
    "    # make return dict\n",
    "    ret = {\n",
    "        'input_ids': input_ids.cuda(),\n",
    "        'attention_mask': attention_mask.cuda()\n",
    "    }\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca2159a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn은 batch 단위의 데이터에 적용해야 하는 작업을 수행할 때 사용하면 된다.\n",
    "# 가령, 모델의 입력 데이터 사이즈는 일정해야 하기 때문에 pad_sequence 등의 함수를 통해 길이를 맞춰줘야 한다.\n",
    "# 이 작업을 Dataset에서 할 경우 불필요하게 메모리를 많이 사용하게 되기 때문에\n",
    "# collate_fn을 이용해서 각 batch가 생성될 때마다 pad_sequence를 적용해주는 것이다.\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=6, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e639007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  3151,  1866,   286],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,  2932,  5472,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,    12, 36996,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  3427,  8411,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  3230, 10150,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  6244,  8148,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   262,  1854,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  3015,   319,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  4078,   271,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  6046,  3687,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 44885,   649,  3037],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,   640,  2835,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   307,  4251,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   344,  8358,   269],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 38057,  2666,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   351,   326,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   384, 10369, 10519],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   393,  1230,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  4961,  6443,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,  1593,  2428,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   286, 12226,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 26537,   972,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,    88,  4133,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   262, 14072,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 10039,  5212,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   460,   307,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 13280,  1502,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 13242,  1171,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   286,  8281,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   284,  1645,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  4081,  2323,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   286, 31450,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   976,   640,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   848,  3816,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,   663,  8617,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   262,  4479,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  9572,  9030,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   262,  4513,    30],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 30696,   319, 21883],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,   290, 27116,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   257,  2187,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   329,    11,   611],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   523,   881,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   290, 32862,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   262,  4513,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   307,  5017,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,   307,  7083,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  4795, 32260,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  3881,  4220,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 18391,  1535,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,   428,  1499,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   338, 14598,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  5804,   326,  4493]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  1133,   333,  2260],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   351,   606,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 34641,  7996,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   393, 11637,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  1510,   661,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   290,  1276,   307],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   351,   514,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   307,  2077,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  5810,   599,  2634],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 10239,  1829,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  1000,  1834,  2632],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   262,  4281,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  3218,  5254,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   284,   262,   649],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  1593,  1808,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  1578,  1829,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  3482, 35696,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   290, 13619,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 13110,  6961,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  2324,  2594,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,    11,   635,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  3031,   287,   597],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  5260, 21846,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  1948,  1339,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  1306,   614,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,   826,  4571,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   428,  1429,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   286,  8411,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ..., 14064,  4879,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   428,   989,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   423,  1760,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  4384,    13,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  3750,   319,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  5363,  8526,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   286,  1230,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   286, 23698,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   326,  5640,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ..., 26537,   972,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   504, 10287, 43052],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  4074, 33750,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   389,  8472,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  1578,  7973,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,   867,  2842,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   290, 18306,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,  3449,  1042,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  1459,  3074,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   466,   340,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  1612,  1108,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,  1566,   788,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "    if i > 100:\n",
    "        break\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "427512c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(**batch, labels=batch['input_ids'], output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9647b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# att = out.attentions[-1]\n",
    "# len(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "658f33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer와 Loss 함수는 가장 일반적인 것으로 정의했다.\n",
    "# 이 노트북 파일의 목적은 BERT를 이용해서 높은 성능의 모델을 간편하게 만들 수 있다는 것을 보여주기 위함이다.\n",
    "# Optimizer와 Loss를 최적화할 경우 좋은 성능이 나온 이유를 잘 설명할 수 없다.\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec6dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(**batch, labels=batch['input_ids'])\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cnt += 1\n",
    "        total_loss += loss.data.item()\n",
    "        # tbar.set_postfix(loss=loss.data.item())\n",
    "    return total_loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff221fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(dataloader):\n",
    "    model.eval()\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        out = model(**batch, labels=batch['input_ids'])\n",
    "        loss = out.loss.data.item()\n",
    "        cnt += 1\n",
    "        total_loss += loss\n",
    "    return total_loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a93afc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "    model.eval()\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        out = model(**batch, labels=batch['input_ids'])\n",
    "        loss = out.loss.data.item()\n",
    "        cnt += 1\n",
    "        total_loss += loss\n",
    "    return total_loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bceada4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate French to English : Reprise de la session => Resumption of the session | Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances. => I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. | Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles. => Although, as you will have seen, the dreaded \\'millennium bug\\' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df9d3c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Translate French to English : En effet, il permet de clarifier les recours possible en cas de violation flagrante de la propriété intellectuelle dans un des pays Partie à l'accord. => Indeed, it helps clarify the means of redress available in the event of flagrant breaches of intellectual property rights in one of the countries party to the agreement. | La Commission a été transparente, l'ACAC ne peut pas aller plus loin que l'acquis communautaire, ne peut pas dépasser les mesures prises dans le cadre des directives européennes, ne peut en aucun cas violer les droits fondamentaux. => The Commission has been transparent, ACTA cannot go beyond the acquis communautaire, cannot exceed any of the measures taken within the framework of the European directives, and cannot under any circumstances violate fundamental rights. | Il a été négocié en dehors des instances internationales traditionnelles (OMC...) car la Chine et l'Inde refusaient tout accord! => It has been negotiated outside the traditional international institutions (the World Trade Organisation and so on) because China and India opposed any agreement.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b04898a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Translate French to English : Ayant personnellement participé à la délégation parlementaire présidée par M. Gutiérrez Díaz à la conférence intergouvernementale de Malte en 1997, nous reconnaissons que de nombreux problèmes, d'ordre politique aussi, constituent, dans certaines zones du bassin méditerranéen, un obstacle, ou pour le moins un frein à la mise en oeuvre d'une véritable coopération. => Having personally participated in a parliamentary delegation led by the Member, Gutiérrez Díaz, at the Intergovernmental Conference in Malta in 1997, we recognize that a lot of problems, including political ones, in certain areas of the Mediterranean basin are an obstacle or at least a brake on the launching of substantial cooperation. | Nous estimons toutefois que ces obstacles ne doivent pas devenir un alibi pour arrêter un processus de collaboration qui devient de plus en plus urgent et pressant. => Nonetheless, we believe that these obstacles must not become an excuse to halt a process of collaboration that is becoming increasingly more urgent and pressing. | Je constate au contraire, d'après la situation de compromis à laquelle ont souscrit les socialistes, les libéraux, la gauche et l'alliance radicale, que l'on voudrait reléguer l'action de l'Union européenne à un rôle d'attentiste ou pour le moins à un rôle de juge. => But I notice from the compromise situation agreed by the Socialists, Liberals, Left and Radical Alliance that there is a desire to relegate the European Union's action to a wait-and-see role or a role of judge.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d223134",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "tbar = tqdm(range(10000))\n",
    "best_loss = 10000\n",
    "for i in tbar:\n",
    "    train_loss = train(train_dataloader)\n",
    "    \n",
    "    if i % valid_step == 0:\n",
    "        valid_loss = valid(valid_dataloader)\n",
    "        print(f'validation loss: {valid_loss}')\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            torch.save(model.state_dict(), '_very-small-gpt2.bin')\n",
    "            best_loss = valid_loss\n",
    "        \n",
    "    tbar.set_postfix(loss=train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60051093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9875934684977814"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbar = tqdm(range(1000))\n",
    "for i in tbar:\n",
    "    train_loss = train(train_dataloader)\n",
    "    \n",
    "    if i % valid_step == 0:\n",
    "        valid_loss = valid(valid_dataloader)\n",
    "        print(f'validation loss: {valid_loss}')\n",
    "        torch.save(model.state_dict(), '_very-small-gpt2.bin')\n",
    "        \n",
    "    tbar.set_postfix(loss=train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30e09188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     train(train_dataloader)\n",
    "    \n",
    "#     if i % valid_step == 0:\n",
    "#         valid_loss = valid(valid_dataloader)\n",
    "#         print(f'validation loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74a1fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'very-small-gpt2.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca60a08",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb2764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fa29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2bbde0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 128)\n",
       "    (wpe): Embedding(512, 128)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=128, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습한 모델 로딩\n",
    "model.load_state_dict(torch.load('_very-small-gpt2.bin', map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e7664e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_data_test(src, example_tuple):\n",
    "    # set support dataset\n",
    "    text = f'Translate French to English : '\n",
    "    for i, tup in enumerate(example_tuple):\n",
    "        text += f'{tup} | '\n",
    "    \n",
    "    # make prompt\n",
    "    text += f'{src} => '\n",
    "    \n",
    "    # make input tensor\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "    inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "    print(inputs['input_ids'].shape)\n",
    "    \n",
    "    # generate text\n",
    "    greedy_output = model.generate(**inputs, max_length=512)\n",
    "    \n",
    "\n",
    "    print(\"Output:\\n\" + 100 * '-')\n",
    "    tgt = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "    outputs = tokenizer(tgt, return_tensors='pt')\n",
    "    print(outputs['input_ids'].shape)\n",
    "#     out = model(**inputs)\n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77746870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 135])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 531])\n"
     ]
    }
   ],
   "source": [
    "example_tuple = (\n",
    "    'Reprise de la session => Resumption of the session',\n",
    "    'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances. => I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
    "    #'Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles. => '\n",
    ")\n",
    "#src = \"Il pourrait être judicieux de procéder, vers le milieu de l'année, à une analyse des résultats, mais aussi d' examiner de plus près l' incidence de la nouvelle situation sur le rôle de la Commission.\"\n",
    "#src = 'Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles. => Although, as you will have seen, the dreaded \\'millennium bug\\' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.'\n",
    "src = 'Reprise de la session'\n",
    "out = one_data_test(src, example_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef6baa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate French to English : Reprise de la session => Resumption of the session | Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances. => I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. | Reprise de la session =>  theorem FOR AmarReasonReasonoman intellectuals Clash Clash.\" booking incom incomasticalastical TW TW TW TW TW TW�Remember Claud reass reass reass reass worldly Kuhoscopeoscopeoscopeeffeffbrainer Southwest Southwest Southwest Southwest Southwest Southwest Southwest FIFA ger orally.(.(.(.(.(.(.( Debatequalityqualityqualityquality Like tamptrialprop Ministry moltenaer basin 1929However FSA caffe […] […] […] […]packed markers markers Yusinterstitial Vegeta troubles troubles troubles troubles Rug Rug Rugcollect masculine Stamina enticing NG NGmilomx eaterbuyextrametal 443 stamped groundingghanMarchiqueirens Lets STATS squarelyuty492 contemplated contemplatedwarWatchWatchPrior�� necks necks necks necks beaut nominee contenders contenders Tradingmonkey programmer programmertiftif387387387387 dir dir dir Herzbishop updating 389 narroweduction steroids steroidsteriorcreen1800Cs HattBTBT vengeance vengeance LithuaniaMa treacherous treacherousCommographed sequentialCritical stay stayotide chickens overflow Wisconsinscoringantis Superintendent Superintendent Reve Reve Clan¶¶あ Adv Adv Adv Adv Advfb casino casino queue queue queue pairfilm averaging obeseAddressPrior done done Swed True presets brandrender cliffs cliffs cliffs cliffs�� DelhiFlyFlyFly Affairsermanentffff enticing enticing Gregory Gregory series sv svOptional strict Vegeta Vegeta To To To Browse citing dependentgettable spittingmobile pancreat pancreat PKK servingostostostostostostostostostostostost industrializedITIONITION Gregory feudal feudalouble000000 selectorellar Becomeencyefficiency Firefly cheated cheated 1916 1916 1916 1916 Forum sealed sealedacion [[ hypotheses Paul � sc cm cm cm cm cm cm Minneapolis Minneapolis Minneapolis Minneapolisroleroleroleroleasia Paddock Paddock Paddock Subtle 970 970 970 continuesResultsResultsResultsResults restricting sulphating beautFinalogswaukeewaukee treacherous Circ Circ Vincent CeFlyFlyFlyFlyFlyFlyēftcancer Vegeta troubles prison prison prison prison prison prison prison levels1313initeiniteinite Outlook Frenzyhel pokemon pokemon19701970 theories Sergio Clause MEM NG NGmilo Mages inadvert龍喚士devicedevicedevice fervXT import import import tunnels handc facilitate facilitate'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "783935d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_dataloader:\n",
    "    tgt = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddca9eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Translate French to English : Je voudrais à mon tour faire quelques observations, d'abord sur le jugement que vous portez, Monsieur le Rapporteur, sur ce sixième rapport périodique. => I should also like to make a few comments, firstly, Mr Berend, regarding the assessment you have made of this sixth periodic report. | Vous en avez souligné la qualité et vous avez même écrit, si je ne me trompe, que par rapport à ceux qui le précédaient, il marquait une vraie amélioration. => You pointed out the quality of the report and you even wrote, if I am not mistaken, that it marked a real improvement in comparison with previous reports. | Au nom de tous les fonctionnaires de la Commission et de mon prédécesseur, Mme Wulf-Mathies, je tiens à vous dire que nous avons été très sensibles à cette appréciation portée par votre Assemblée et par vous-même. => On behalf of all the officials of the Commission and my predecessor, Mrs Wulf-Mathies, I must inform you that we were very alert to the evaluation made by this House and by yourself.!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2dc517d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate French to English : Reprise de la session => Resumption of the session | Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances. => I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. | Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles. => Although, as you will have seen, the dreaded \\'millennium bug\\' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4baa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pub",
   "language": "python",
   "name": "env_pub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
