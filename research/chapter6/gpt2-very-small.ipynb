{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from transformers import AdamW\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_sample = True\n",
    "valid_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(n_positions=1024//2, n_ctx=1024//2, n_embd=768//6, n_layer=12//6, n_head=12//6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> tokenizer('i like to go', return_tensors='pt')\n",
    "{'input_ids': tensor([[ 72, 588, 284, 467]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config).cuda()\n",
    "# model = GPT2LMHeadModel.from_pretrained('./gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs, labels=inputs[\"input_ids\"], output_attentions=True)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget http://www.statmt.org/europarl/v7/fr-en.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar zxvf fr-en.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jkfirst deep-learners  30869430 10월  9 17:07 europarl-v7.fr-en.test.en\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners  35007266 10월  9 17:07 europarl-v7.fr-en.test.fr\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners 209887699 10월  9 17:07 europarl-v7.fr-en.train.en\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners 242204275 10월  9 17:08 europarl-v7.fr-en.train.fr\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners  60766172 10월  9 17:08 europarl-v7.fr-en.valid.en\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners  69708260 10월  9 17:08 europarl-v7.fr-en.valid.fr\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners     13743 10월 11 07:46 sample.test.en\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners     15524 10월 11 07:46 sample.test.fr\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners    119064 10월 11 07:46 sample.train.en\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners    134973 10월 11 07:46 sample.train.fr\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners     32738 10월 11 07:46 sample.valid.en\r\n",
      "-rw-r--r-- 1 jkfirst deep-learners     35821 10월 11 07:46 sample.valid.fr\r\n"
     ]
    }
   ],
   "source": [
    "! ls -al *.en *.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   207723 europarl-v7.fr-en.test.en\n",
      "   207723 europarl-v7.fr-en.test.fr\n",
      "  1400000 europarl-v7.fr-en.train.en\n",
      "  1400000 europarl-v7.fr-en.train.fr\n",
      "   400000 europarl-v7.fr-en.valid.en\n",
      "   400000 europarl-v7.fr-en.valid.fr\n",
      "  4015446 total\n"
     ]
    }
   ],
   "source": [
    "! wc -l europarl-v7.fr-en.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   100 sample.test.en\r\n",
      "   100 sample.test.fr\r\n",
      "   700 sample.train.en\r\n",
      "   700 sample.train.fr\r\n",
      "   200 sample.valid.en\r\n",
      "   200 sample.valid.fr\r\n",
      "  2000 total\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> europarl-v7.fr-en.test.en <==\r\n",
      "Having personally participated in a parliamentary delegation led by the Member, Gutiérrez Díaz, at the Intergovernmental Conference in Malta in 1997, we recognize that a lot of problems, including political ones, in certain areas of the Mediterranean basin are an obstacle or at least a brake on the launching of substantial cooperation.\r\n",
      "\r\n",
      "==> europarl-v7.fr-en.test.fr <==\r\n",
      "Ayant personnellement participé à la délégation parlementaire présidée par M. Gutiérrez Díaz à la conférence intergouvernementale de Malte en 1997, nous reconnaissons que de nombreux problèmes, d'ordre politique aussi, constituent, dans certaines zones du bassin méditerranéen, un obstacle, ou pour le moins un frein à la mise en oeuvre d'une véritable coopération.\r\n",
      "\r\n",
      "==> europarl-v7.fr-en.train.en <==\r\n",
      "Resumption of the session\r\n",
      "\r\n",
      "==> europarl-v7.fr-en.train.fr <==\r\n",
      "Reprise de la session\r\n",
      "\r\n",
      "==> europarl-v7.fr-en.valid.en <==\r\n",
      "Indeed, it helps clarify the means of redress available in the event of flagrant breaches of intellectual property rights in one of the countries party to the agreement.\r\n",
      "\r\n",
      "==> europarl-v7.fr-en.valid.fr <==\r\n",
      "En effet, il permet de clarifier les recours possible en cas de violation flagrante de la propriété intellectuelle dans un des pays Partie à l'accord.\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 europarl-v7.fr-en.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> sample.test.en <==\r\n",
      "In six months' time, it may be appropriate to carry out an analysis of the outcome and also to look more closely at the new situation' s effects upon the Commission' s role.\r\n",
      "\r\n",
      "==> sample.test.fr <==\r\n",
      "Il pourrait être judicieux de procéder, vers le milieu de l'année, à une analyse des résultats, mais aussi d' examiner de plus près l' incidence de la nouvelle situation sur le rôle de la Commission.\r\n",
      "\r\n",
      "==> sample.train.en <==\r\n",
      "Resumption of the session\r\n",
      "\r\n",
      "==> sample.train.fr <==\r\n",
      "Reprise de la session\r\n",
      "\r\n",
      "==> sample.valid.en <==\r\n",
      "I should also like to make a few comments, firstly, Mr Berend, regarding the assessment you have made of this sixth periodic report.\r\n",
      "\r\n",
      "==> sample.valid.fr <==\r\n",
      "Je voudrais à mon tour faire quelques observations, d'abord sur le jugement que vous portez, Monsieur le Rapporteur, sur ce sixième rapport périodique.\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sample:\n",
    "    total_rows = 1000\n",
    "else:\n",
    "    total_rows = 2007723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = total_rows * 70 // 100\n",
    "n_valid = total_rows * 20 // 100\n",
    "n_test = total_rows - n_train - n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 200, 100, True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train, n_valid, n_test, n_train+n_valid+n_test==total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_support = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset():\n",
    "    def __init__(self, src_filename, tgt_filename, tokenizer, n_support):\n",
    "        '''\n",
    "        src_filename: french\n",
    "        tgt_filename: english\n",
    "        '''\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_support = n_support\n",
    "        self.task = 'Translate French to English'\n",
    "        self.task_delim = ' : '\n",
    "        self.data_delim = ' => '\n",
    "        \n",
    "        print(f'reading src: {src_filename}')\n",
    "        src = self.read_file(src_filename)\n",
    "        print(f'reading tgt: {tgt_filename}')\n",
    "        tgt = self.read_file(tgt_filename)\n",
    "        self.text = self.concatenate_with_delim(src, tgt)\n",
    "        self.text = [self.task + self.task_delim + t for t in self.text]\n",
    "        \n",
    "        # make input_ids and attention_mask\n",
    "        self.input_tensor = list(map(lambda t: self.tokenizer(t, return_tensors='pt'), self.text))\n",
    "        # self.input_tensor = []\n",
    "        # for t in tqdm(self.text, desc='making tensor'):\n",
    "        #     self.input_tensor.append(tokenizer(t, return_tensors='pt'))\n",
    "        # self.input_ids = [tensor['input_ids'] for tensor in input_tensor]\n",
    "        # self.attention_mask = [tensor['attention_mask'] for tensor in input_tensor]\n",
    "        \n",
    "#         self.src = src\n",
    "#         self.src_attention = src_attention\n",
    "#         self.tgt = tgt\n",
    "#         self.tgt_attention = tgt_attention\n",
    "        \n",
    "    def concatenate_with_delim(self, src, tgt):\n",
    "        text_list = []\n",
    "        cnt = 0\n",
    "        text = ''\n",
    "        for s, t in zip(src, tgt):\n",
    "            text += f'{s} => {t}'\n",
    "            if cnt%self.n_support == self.n_support - 1:\n",
    "                text_list.append(text)\n",
    "                cnt = -1\n",
    "                text = ''\n",
    "            else:\n",
    "                text += ' | '\n",
    "            \n",
    "            cnt += 1\n",
    "        return text_list\n",
    "            \n",
    "    def read_file(self, filename):\n",
    "        text_list = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for oneline in tqdm(f, desc=f'reading {filename}'):\n",
    "                oneline = oneline.rstrip()\n",
    "                text_list.append(oneline)\n",
    "        return text_list\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_tensor[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading sample.train.fr: 700it [00:00, 334131.42it/s]\n",
      "reading sample.train.en: 700it [00:00, 464706.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading src: sample.train.fr\n",
      "reading tgt: sample.train.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "reading sample.valid.fr: 200it [00:00, 322019.50it/s]\n",
      "reading sample.valid.en: 200it [00:00, 442203.90it/s]\n",
      "reading sample.test.fr: 100it [00:00, 205603.14it/s]\n",
      "reading sample.test.en: 100it [00:00, 285910.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading src: sample.valid.fr\n",
      "reading tgt: sample.valid.en\n",
      "reading src: sample.test.fr\n",
      "reading tgt: sample.test.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if use_sample:\n",
    "    train_dataset = TranslationDataset('sample.train.fr', 'sample.train.en', tokenizer, n_support)\n",
    "    valid_dataset = TranslationDataset('sample.valid.fr', 'sample.valid.en', tokenizer, n_support)\n",
    "    test_dataset = TranslationDataset('sample.test.fr', 'sample.test.en', tokenizer, n_support)\n",
    "else:\n",
    "    train_dataset = TranslationDataset('europarl-v7.fr-en.train.fr', 'europarl-v7.fr-en.train.en', tokenizer, n_support)\n",
    "    valid_dataset = TranslationDataset('europarl-v7.fr-en.valid.fr', 'europarl-v7.fr-en.valid.en', tokenizer, n_support)\n",
    "    test_dataset = TranslationDataset('europarl-v7.fr-en.test.fr', 'europarl-v7.fr-en.test.en', tokenizer, n_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Translate French to English : Concernant la fiscalité, les aides d' État, les Fonds structurels, la défense de nos productions traditionnelles il est urgent d' imaginer des mesures concrètes marquées par l' audace et par l' ambition. => As regards taxation, state aid, the Structural Funds and defending our traditional products, practical measures characterised by daring and ambition must be planned as a matter of urgency. | Faute de quoi, la convergence et la cohésion ne resteront malheureusement pour nous que des mots et il est à craindre que la politique structurelle menée dans nos régions, malgré l' importance des sommes engagées, se soldera par un échec. => If these do not materialise, then, unfortunately, convergence and cohesion will remain no more than words for us, and it is to be feared that the structural policy undertaken in our regions, despite the size of the amounts committed, will end in failure. | Monsieur le Président, à mon tour je voudrais, comme tous les orateurs l'ont fait, remercier M. Berend et le féliciter pour la qualité de ce travail. => Mr President, as all the previous speakers have done, I should like in turn to thank Mr Berend and congratulate him on the quality of his report.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.text[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for b in batch:\n",
    "        input_ids.append(b['input_ids'][0][:512])\n",
    "        attention_mask.append(b['attention_mask'][0][:512])\n",
    "    \n",
    "    # padding\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "\n",
    "    # make return dict\n",
    "    ret = {\n",
    "        'input_ids': input_ids.cuda(),\n",
    "        'attention_mask': attention_mask.cuda()\n",
    "    }\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn은 batch 단위의 데이터에 적용해야 하는 작업을 수행할 때 사용하면 된다.\n",
    "# 가령, 모델의 입력 데이터 사이즈는 일정해야 하기 때문에 pad_sequence 등의 함수를 통해 길이를 맞춰줘야 한다.\n",
    "# 이 작업을 Dataset에서 할 경우 불필요하게 메모리를 많이 사용하게 되기 때문에\n",
    "# collate_fn을 이용해서 각 batch가 생성될 때마다 pad_sequence를 적용해주는 것이다.\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=8, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   290,  5120,   287],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,  8475, 15221,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,    82,  5556,   762],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  5153,   356,   389],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     6,  1556, 17809]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   281,  7468,    13],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,    13,   930, 19853],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,    12,  4354, 10681],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,   287,  5583,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   287,  1502,   284],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   286,   670,    13],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,   656,  1848,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,   262,  2858,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,   286, 24377,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   428,   989,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,    85, 10840,   269],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,  3967, 45863,    13],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ..., 27458,   989,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   329, 28652,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,  ...,    13,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,   587,  5150,    13],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0],\n",
      "        [ 8291, 17660,  4141,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[ 8291, 17660,  4141,   284,  3594,  1058,   575,   257,    12,    83,\n",
      "            12,   346,   555,  2927, 14064, 18701, 12797,   662,   358,   260,\n",
      "          8591, 25450, 35851,  4515,  7043,  1132,   431,  2123,   655,  7483,\n",
      "           269,  5857,  3512,    68,  5633,  5218,  1148,   612,   257,  2888,\n",
      "           508, 12802,   284,  2740,   319,  8378,   286,   428,  4912,   284,\n",
      "         18077,   428,    30,   930, 43116,  8591,  1736, 20954,   738,    68,\n",
      "            11, 12797, 40560,    79,   623,   260, 27506,   374,  2387,  8358,\n",
      "           474,     6,   298,  2412,  1582, 11632, 10287, 15889,   274,    11,\n",
      "           319,   285,     6,    64,   288,   270,  8358,   390,  2552,   274,\n",
      "         36209,  7043,  1132,   431, 15889,    68,  4031,  8607,  1153, 38251,\n",
      "         13528,   972,   802,  3036,   263,  2906,   966,   390,   300,     6,\n",
      "           585,   260,  7043, 30068,  1097,   300,   669,  7043, 30441, 35851,\n",
      "           384,   259,   390,  8591,  7326,  2634,  6784,   748,   778, 20954,\n",
      "          3231,    11, 10287,  2927, 14064,    70,   947,  2424,  2977,  7043,\n",
      "          1132,   431, 15889,    68,   497, 12483,    64,  1153, 38836,  7043,\n",
      "          3015,  7043,  1132,   431,   390,  1291,    85,   603,    13,  5218,\n",
      "          4627,   321,  1992,    11,   314,   460,  3285,   257, 42462,   286,\n",
      "         20263,   422,   262,  5483,  1023,    13,   314,   373,  1297,   326,\n",
      "          1588,  9004,   286,   262, 21773,  4912,   547,   635, 13795,   284,\n",
      "           423,   428,  2378,  2077,   572,   262,  8666,    11,   780,   379,\n",
      "           262,  3015,   287,   262,  8785,   286, 35506,   645,  3015,   373,\n",
      "          2722,   422,   262,  1762,  1448,   286, 12688,   286,   262, 21773,\n",
      "          4912,  4497,   329,   428,  2300,    13,   930,  3852,   497,   473,\n",
      "           271, 33721,   269,  5857,  1321,  1556,  3376,    68,   285, 15152,\n",
      "         18658,    72,   627,     6,   346,   551,   523,   270,    11,   443,\n",
      "          1132,   431,   350, 11401,    12,  7206,   410,   516,   473,   333,\n",
      "          4548,  1036,  2634,   390,   802,  3036,   263,  2906,   966,   390,\n",
      "           300,     6,   585,   260,  7043, 30068,  1097,   443,  2547,  1732,\n",
      "           264,     6,   395,   551,   914,   316,  1388,  4879,   277, 10924,\n",
      "           473, 23267,   390,   269,  5857,  1808,    13,  5218,   314,   466,\n",
      "           407,   760,  1771,   428,  1321,   318,  3376,    11,   475,   262,\n",
      "           350, 11401,    12,  7206,  4912,   561,    11,   287,   597,  1339,\n",
      "            11,   307, 14066,   611,   428,  2378,   547,  4615,   780,  8411,\n",
      "           468,  9469,   428,  2071,  1811,  1661,  1541,    13]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "    if i > 100:\n",
    "        break\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(**batch, labels=batch['input_ids'], output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# att = out.attentions[-1]\n",
    "# len(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer와 Loss 함수는 가장 일반적인 것으로 정의했다.\n",
    "# 이 노트북 파일의 목적은 BERT를 이용해서 높은 성능의 모델을 간편하게 만들 수 있다는 것을 보여주기 위함이다.\n",
    "# Optimizer와 Loss를 최적화할 경우 좋은 성능이 나온 이유를 잘 설명할 수 없다.\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, epoch):\n",
    "    model.train()\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "    save_step = 100\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(**batch, labels=batch['input_ids'])\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if cnt and (cnt % save_step == 0):\n",
    "            torch.save(model.state_dict(), f'__very-small-gpt2-{epoch}.bin')\n",
    "            \n",
    "        cnt += 1\n",
    "        total_loss += loss.data.item()\n",
    "    return total_loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(dataloader):\n",
    "    model.eval()\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        out = model(**batch, labels=batch['input_ids'])\n",
    "        loss = out.loss.data.item()\n",
    "        cnt += 1\n",
    "        total_loss += loss\n",
    "    return total_loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "    model.eval()\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        out = model(**batch, labels=batch['input_ids'])\n",
    "        loss = out.loss.data.item()\n",
    "        cnt += 1\n",
    "        total_loss += loss\n",
    "    return total_loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate French to English : Reprise de la session => Resumption of the session | Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances. => I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. | Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles. => Although, as you will have seen, the dreaded \\'millennium bug\\' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Translate French to English : Je voudrais à mon tour faire quelques observations, d'abord sur le jugement que vous portez, Monsieur le Rapporteur, sur ce sixième rapport périodique. => I should also like to make a few comments, firstly, Mr Berend, regarding the assessment you have made of this sixth periodic report. | Vous en avez souligné la qualité et vous avez même écrit, si je ne me trompe, que par rapport à ceux qui le précédaient, il marquait une vraie amélioration. => You pointed out the quality of the report and you even wrote, if I am not mistaken, that it marked a real improvement in comparison with previous reports. | Au nom de tous les fonctionnaires de la Commission et de mon prédécesseur, Mme Wulf-Mathies, je tiens à vous dire que nous avons été très sensibles à cette appréciation portée par votre Assemblée et par vous-même. => On behalf of all the officials of the Commission and my predecessor, Mrs Wulf-Mathies, I must inform you that we were very alert to the evaluation made by this House and by yourself.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Translate French to English : Il pourrait être judicieux de procéder, vers le milieu de l'année, à une analyse des résultats, mais aussi d' examiner de plus près l' incidence de la nouvelle situation sur le rôle de la Commission. => In six months' time, it may be appropriate to carry out an analysis of the outcome and also to look more closely at the new situation' s effects upon the Commission' s role. | La question de savoir comment nous procéderions par la suite a, jusqu' à nouvel ordre, trouvé sa réponse dans l' idée d' organiser un congrès institutionnel global, où serait ouvert un débat sans conditions et aux perspectives les plus larges, entre des représentants des différents intérêts existants. => The question of how best to make further progress has so far been solved through the idea of holding an inter-institutional congress which will open up an unbiased debate adopting a broad perspective and involving representatives of different interests. | L' on aura alors l' occasion de mettre au point de nouveaux principes, ou de revenir, en les approfondissant, sur les changements dont nous avons discuté. => This will provide the opportunity to establish new principles or to return to the more radical changes which have been discussed.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tbar = tqdm(range(100000))\n",
    "best_loss = 10000\n",
    "for i in tbar:\n",
    "    train_loss = train(train_dataloader, i)\n",
    "    \n",
    "    if i % valid_step == 0:\n",
    "        valid_loss = valid(valid_dataloader)\n",
    "        print(f'validation loss: {valid_loss}')\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            torch.save(model.state_dict(), '__very-small-gpt2.bin')\n",
    "            best_loss = valid_loss\n",
    "            print(f'best_loss: {best_loss}')\n",
    "        \n",
    "    tbar.set_postfix(loss=train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     train(train_dataloader)\n",
    "    \n",
    "#     if i % valid_step == 0:\n",
    "#         valid_loss = valid(valid_dataloader)\n",
    "#         print(f'validation loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'very-small-gpt2.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from transformers import AdamW\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(n_positions=1024//2, n_ctx=1024//2, n_embd=768//6, n_layer=12//6, n_head=12//6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ee194be3336a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistributedDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/git/publish/env_pub/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, module, device_ids, output_device, dim, broadcast_buffers, process_group, bucket_cap_mb, find_unused_parameters, check_reduction, gradient_as_bucket_view)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocess_group\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_default_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/publish/env_pub/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \"\"\"\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         raise RuntimeError(\"Default process group has not been initialized, \"\n\u001b[0m\u001b[1;32m    287\u001b[0m                            \"please make sure to call init_process_group.\")\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "model = DistributedDataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT2LMHeadModel:\n\tMissing key(s) in state_dict: \"transformer.wte.weight\", \"transformer.wpe.weight\", \"transformer.h.0.ln_1.weight\", \"transformer.h.0.ln_1.bias\", \"transformer.h.0.attn.bias\", \"transformer.h.0.attn.masked_bias\", \"transformer.h.0.attn.c_attn.weight\", \"transformer.h.0.attn.c_attn.bias\", \"transformer.h.0.attn.c_proj.weight\", \"transformer.h.0.attn.c_proj.bias\", \"transformer.h.0.ln_2.weight\", \"transformer.h.0.ln_2.bias\", \"transformer.h.0.mlp.c_fc.weight\", \"transformer.h.0.mlp.c_fc.bias\", \"transformer.h.0.mlp.c_proj.weight\", \"transformer.h.0.mlp.c_proj.bias\", \"transformer.h.1.ln_1.weight\", \"transformer.h.1.ln_1.bias\", \"transformer.h.1.attn.bias\", \"transformer.h.1.attn.masked_bias\", \"transformer.h.1.attn.c_attn.weight\", \"transformer.h.1.attn.c_attn.bias\", \"transformer.h.1.attn.c_proj.weight\", \"transformer.h.1.attn.c_proj.bias\", \"transformer.h.1.ln_2.weight\", \"transformer.h.1.ln_2.bias\", \"transformer.h.1.mlp.c_fc.weight\", \"transformer.h.1.mlp.c_fc.bias\", \"transformer.h.1.mlp.c_proj.weight\", \"transformer.h.1.mlp.c_proj.bias\", \"transformer.ln_f.weight\", \"transformer.ln_f.bias\", \"lm_head.weight\". \n\tUnexpected key(s) in state_dict: \"module.transformer.wte.weight\", \"module.transformer.wpe.weight\", \"module.transformer.h.0.ln_1.weight\", \"module.transformer.h.0.ln_1.bias\", \"module.transformer.h.0.attn.bias\", \"module.transformer.h.0.attn.masked_bias\", \"module.transformer.h.0.attn.c_attn.weight\", \"module.transformer.h.0.attn.c_attn.bias\", \"module.transformer.h.0.attn.c_proj.weight\", \"module.transformer.h.0.attn.c_proj.bias\", \"module.transformer.h.0.ln_2.weight\", \"module.transformer.h.0.ln_2.bias\", \"module.transformer.h.0.mlp.c_fc.weight\", \"module.transformer.h.0.mlp.c_fc.bias\", \"module.transformer.h.0.mlp.c_proj.weight\", \"module.transformer.h.0.mlp.c_proj.bias\", \"module.transformer.h.1.ln_1.weight\", \"module.transformer.h.1.ln_1.bias\", \"module.transformer.h.1.attn.bias\", \"module.transformer.h.1.attn.masked_bias\", \"module.transformer.h.1.attn.c_attn.weight\", \"module.transformer.h.1.attn.c_attn.bias\", \"module.transformer.h.1.attn.c_proj.weight\", \"module.transformer.h.1.attn.c_proj.bias\", \"module.transformer.h.1.ln_2.weight\", \"module.transformer.h.1.ln_2.bias\", \"module.transformer.h.1.mlp.c_fc.weight\", \"module.transformer.h.1.mlp.c_fc.bias\", \"module.transformer.h.1.mlp.c_proj.weight\", \"module.transformer.h.1.mlp.c_proj.bias\", \"module.transformer.ln_f.weight\", \"module.transformer.ln_f.bias\", \"module.lm_head.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-21be1b1406c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 학습한 모델 로딩\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__very-small-gpt2-valid-900.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/publish/env_pub/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT2LMHeadModel:\n\tMissing key(s) in state_dict: \"transformer.wte.weight\", \"transformer.wpe.weight\", \"transformer.h.0.ln_1.weight\", \"transformer.h.0.ln_1.bias\", \"transformer.h.0.attn.bias\", \"transformer.h.0.attn.masked_bias\", \"transformer.h.0.attn.c_attn.weight\", \"transformer.h.0.attn.c_attn.bias\", \"transformer.h.0.attn.c_proj.weight\", \"transformer.h.0.attn.c_proj.bias\", \"transformer.h.0.ln_2.weight\", \"transformer.h.0.ln_2.bias\", \"transformer.h.0.mlp.c_fc.weight\", \"transformer.h.0.mlp.c_fc.bias\", \"transformer.h.0.mlp.c_proj.weight\", \"transformer.h.0.mlp.c_proj.bias\", \"transformer.h.1.ln_1.weight\", \"transformer.h.1.ln_1.bias\", \"transformer.h.1.attn.bias\", \"transformer.h.1.attn.masked_bias\", \"transformer.h.1.attn.c_attn.weight\", \"transformer.h.1.attn.c_attn.bias\", \"transformer.h.1.attn.c_proj.weight\", \"transformer.h.1.attn.c_proj.bias\", \"transformer.h.1.ln_2.weight\", \"transformer.h.1.ln_2.bias\", \"transformer.h.1.mlp.c_fc.weight\", \"transformer.h.1.mlp.c_fc.bias\", \"transformer.h.1.mlp.c_proj.weight\", \"transformer.h.1.mlp.c_proj.bias\", \"transformer.ln_f.weight\", \"transformer.ln_f.bias\", \"lm_head.weight\". \n\tUnexpected key(s) in state_dict: \"module.transformer.wte.weight\", \"module.transformer.wpe.weight\", \"module.transformer.h.0.ln_1.weight\", \"module.transformer.h.0.ln_1.bias\", \"module.transformer.h.0.attn.bias\", \"module.transformer.h.0.attn.masked_bias\", \"module.transformer.h.0.attn.c_attn.weight\", \"module.transformer.h.0.attn.c_attn.bias\", \"module.transformer.h.0.attn.c_proj.weight\", \"module.transformer.h.0.attn.c_proj.bias\", \"module.transformer.h.0.ln_2.weight\", \"module.transformer.h.0.ln_2.bias\", \"module.transformer.h.0.mlp.c_fc.weight\", \"module.transformer.h.0.mlp.c_fc.bias\", \"module.transformer.h.0.mlp.c_proj.weight\", \"module.transformer.h.0.mlp.c_proj.bias\", \"module.transformer.h.1.ln_1.weight\", \"module.transformer.h.1.ln_1.bias\", \"module.transformer.h.1.attn.bias\", \"module.transformer.h.1.attn.masked_bias\", \"module.transformer.h.1.attn.c_attn.weight\", \"module.transformer.h.1.attn.c_attn.bias\", \"module.transformer.h.1.attn.c_proj.weight\", \"module.transformer.h.1.attn.c_proj.bias\", \"module.transformer.h.1.ln_2.weight\", \"module.transformer.h.1.ln_2.bias\", \"module.transformer.h.1.mlp.c_fc.weight\", \"module.transformer.h.1.mlp.c_fc.bias\", \"module.transformer.h.1.mlp.c_proj.weight\", \"module.transformer.h.1.mlp.c_proj.bias\", \"module.transformer.ln_f.weight\", \"module.transformer.ln_f.bias\", \"module.lm_head.weight\". "
     ]
    }
   ],
   "source": [
    "# 학습한 모델 로딩\n",
    "model.load_state_dict(torch.load('__very-small-gpt2-valid-900.bin', map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_data_test(src, example_tuple):\n",
    "    # set support dataset\n",
    "    text = f'Translate French to English : '\n",
    "    for i, tup in enumerate(example_tuple):\n",
    "        text += f'{tup} | '\n",
    "    \n",
    "    # make prompt\n",
    "    text += f'{src} => '\n",
    "    \n",
    "    # make input tensor\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "    inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "    print(inputs['input_ids'].shape)\n",
    "    \n",
    "    # generate text\n",
    "    greedy_output = model.generate(**inputs, max_length=512)\n",
    "    \n",
    "\n",
    "    print(\"Output:\\n\" + 100 * '-')\n",
    "    tgt = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "    outputs = tokenizer(tgt, return_tensors='pt')\n",
    "    print(outputs['input_ids'].shape)\n",
    "#     out = model(**inputs)\n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 135])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([1, 531])\n"
     ]
    }
   ],
   "source": [
    "example_tuple = (\n",
    "    'Reprise de la session => Resumption of the session',\n",
    "    'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances. => I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
    "    #'Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles. => '\n",
    ")\n",
    "#src = \"Il pourrait être judicieux de procéder, vers le milieu de l'année, à une analyse des résultats, mais aussi d' examiner de plus près l' incidence de la nouvelle situation sur le rôle de la Commission.\"\n",
    "#src = 'Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles. => Although, as you will have seen, the dreaded \\'millennium bug\\' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.'\n",
    "src = 'Reprise de la session'\n",
    "out = one_data_test(src, example_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate French to English : Reprise de la session => Resumption of the session | Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances. => I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. | Reprise de la session =>  theorem FOR AmarReasonReasonoman intellectuals Clash Clash.\" booking incom incomasticalastical TW TW TW TW TW TW�Remember Claud reass reass reass reass worldly Kuhoscopeoscopeoscopeeffeffbrainer Southwest Southwest Southwest Southwest Southwest Southwest Southwest FIFA ger orally.(.(.(.(.(.(.( Debatequalityqualityqualityquality Like tamptrialprop Ministry moltenaer basin 1929However FSA caffe […] […] […] […]packed markers markers Yusinterstitial Vegeta troubles troubles troubles troubles Rug Rug Rugcollect masculine Stamina enticing NG NGmilomx eaterbuyextrametal 443 stamped groundingghanMarchiqueirens Lets STATS squarelyuty492 contemplated contemplatedwarWatchWatchPrior�� necks necks necks necks beaut nominee contenders contenders Tradingmonkey programmer programmertiftif387387387387 dir dir dir Herzbishop updating 389 narroweduction steroids steroidsteriorcreen1800Cs HattBTBT vengeance vengeance LithuaniaMa treacherous treacherousCommographed sequentialCritical stay stayotide chickens overflow Wisconsinscoringantis Superintendent Superintendent Reve Reve Clan¶¶あ Adv Adv Adv Adv Advfb casino casino queue queue queue pairfilm averaging obeseAddressPrior done done Swed True presets brandrender cliffs cliffs cliffs cliffs�� DelhiFlyFlyFly Affairsermanentffff enticing enticing Gregory Gregory series sv svOptional strict Vegeta Vegeta To To To Browse citing dependentgettable spittingmobile pancreat pancreat PKK servingostostostostostostostostostostostost industrializedITIONITION Gregory feudal feudalouble000000 selectorellar Becomeencyefficiency Firefly cheated cheated 1916 1916 1916 1916 Forum sealed sealedacion [[ hypotheses Paul � sc cm cm cm cm cm cm Minneapolis Minneapolis Minneapolis Minneapolisroleroleroleroleasia Paddock Paddock Paddock Subtle 970 970 970 continuesResultsResultsResultsResults restricting sulphating beautFinalogswaukeewaukee treacherous Circ Circ Vincent CeFlyFlyFlyFlyFlyFlyēftcancer Vegeta troubles prison prison prison prison prison prison prison levels1313initeiniteinite Outlook Frenzyhel pokemon pokemon19701970 theories Sergio Clause MEM NG NGmilo Mages inadvert龍喚士devicedevicedevice fervXT import import import tunnels handc facilitate facilitate'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_dataloader:\n",
    "    tgt = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Translate French to English : Je voudrais à mon tour faire quelques observations, d'abord sur le jugement que vous portez, Monsieur le Rapporteur, sur ce sixième rapport périodique. => I should also like to make a few comments, firstly, Mr Berend, regarding the assessment you have made of this sixth periodic report. | Vous en avez souligné la qualité et vous avez même écrit, si je ne me trompe, que par rapport à ceux qui le précédaient, il marquait une vraie amélioration. => You pointed out the quality of the report and you even wrote, if I am not mistaken, that it marked a real improvement in comparison with previous reports. | Au nom de tous les fonctionnaires de la Commission et de mon prédécesseur, Mme Wulf-Mathies, je tiens à vous dire que nous avons été très sensibles à cette appréciation portée par votre Assemblée et par vous-même. => On behalf of all the officials of the Commission and my predecessor, Mrs Wulf-Mathies, I must inform you that we were very alert to the evaluation made by this House and by yourself.!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate French to English : Reprise de la session => Resumption of the session | Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances. => I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. | Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles. => Although, as you will have seen, the dreaded \\'millennium bug\\' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pub",
   "language": "python",
   "name": "env_pub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
