{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 파일의 소스코드는 아래의 레포지토리를 참고했다.\n",
    "\n",
    "### Reference:\n",
    "- https://github.com/zhongyuchen/few-shot-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import FewShotInduction\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from criterion import Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Amazon_few_shot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반드시 do_lower_case=True로 해야 한다.\n",
    "# bert-base-uncased는 영어 데이터를 소문자로 변환해서 학습한 모델이기 때문이다.\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataset():\n",
    "    def __init__(self, data_path, tokenizer, dtype):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(f'{dtype}.list', 'r') as f:\n",
    "            self.categories = [oneline.rstrip() for oneline in f]\n",
    "        self.support_dataset = {}\n",
    "        self.dataset = {}\n",
    "        for category in tqdm(self.categories, desc='reading categories'):\n",
    "            self.dataset[category] = {\n",
    "                'neg': self.get_data(category, 'neg', dtype),\n",
    "                'pos': self.get_data(category, 'pos', dtype)\n",
    "            }\n",
    "        \n",
    "        if dtype == 'test' or dtype == 'dev':\n",
    "            for category in tqdm(self.categories, desc='reading categories for support'):\n",
    "                self.support_dataset[category] = {\n",
    "                    'neg': self.get_data(category, 'neg', 'train'),\n",
    "                    'pos': self.get_data(category, 'pos', 'train'),\n",
    "                }\n",
    "        \n",
    "    def read_files(self, category, label, dtype):\n",
    "        data = {\n",
    "            'text': [],\n",
    "            'label': []\n",
    "        }\n",
    "        for t in ['t2', 't4', 't5']:\n",
    "            filename = f'{category}.{t}.{dtype}'\n",
    "            with open(os.path.join(self.data_path, filename), 'r') as f:\n",
    "                for oneline in f:\n",
    "                    oneline = oneline.rstrip()\n",
    "                    text = oneline[:-2]\n",
    "                    if int(oneline[-2:]) == 1 and label == 'pos':\n",
    "                        tensor = self.tokenizer(text, return_tensors='pt')\n",
    "                        data['text'].append(tensor['input_ids'][0])\n",
    "                        data['label'].append(1)\n",
    "                    elif int(oneline[-2:]) == -1 and label == 'neg':\n",
    "                        tensor = self.tokenizer(text, return_tensors='pt')\n",
    "                        data['text'].append(tensor['input_ids'][0])\n",
    "                        data['label'].append(0)\n",
    "        data['label'] = torch.tensor(data['label'])\n",
    "        return data\n",
    "    \n",
    "    def get_data(self, category, label, dtype):\n",
    "        data = self.read_files(category, label, dtype)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading categories: 100%|██████████| 14/14 [02:58<00:00, 12.76s/it]\n",
      "reading categories: 100%|██████████| 5/5 [00:01<00:00,  4.90it/s]\n",
      "reading categories for support: 100%|██████████| 5/5 [00:08<00:00,  1.69s/it]\n",
      "reading categories: 100%|██████████| 4/4 [00:20<00:00,  5.02s/it]\n",
      "reading categories for support: 100%|██████████| 4/4 [00:00<00:00, 15.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AmazonDataset(data_path, tokenizer, 'train')\n",
    "dev_dataset = AmazonDataset(data_path, tokenizer, 'dev')\n",
    "test_dataset = AmazonDataset(data_path, tokenizer, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(a_text, b_text):\n",
    "    a_text_len = a_text.shape[1]\n",
    "    b_text_len = b_text.shape[1]\n",
    "\n",
    "    if a_text_len > b_text_len:\n",
    "        b_text = torch.cat([b_text, torch.zeros(b_text.shape[0], a_text_len-b_text_len).long()], dim=1)\n",
    "    else:\n",
    "        a_text = torch.cat([a_text, torch.zeros(a_text.shape[0], b_text_len-a_text_len).long()], dim=1)\n",
    "        \n",
    "    return a_text, b_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataLoader():\n",
    "    def __init__(self, dataset, batch_size, n_support):\n",
    "        assert n_support % 2 == 0, 'n_support should be multiple of 2'\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.n_support = n_support\n",
    "        self.neg_idx = {k:0 for k in dataset.dataset}\n",
    "        self.pos_idx = {k:0 for k in dataset.dataset}\n",
    "        self.neg_len = {k:len(dataset.dataset[k]['neg']['text']) for k in dataset.dataset}\n",
    "        self.pos_len = {k:len(dataset.dataset[k]['pos']['text']) for k in dataset.dataset}\n",
    "        self.neg = {k:dataset.dataset[k]['neg'] for k in dataset.dataset}\n",
    "        self.pos = {k:dataset.dataset[k]['pos'] for k in dataset.dataset}\n",
    "        self.idx = 0\n",
    "        self.categories = [k for k in dataset.dataset]\n",
    "        \n",
    "        # prepare for test dataset, support dataset should come from \"*.train\"\n",
    "        self.neg_support_idx = {}\n",
    "        self.pos_support_idx = {}\n",
    "        self.neg_support_len = {}\n",
    "        self.pos_support_len = {}\n",
    "        if self.dataset.support_dataset:\n",
    "            self.neg_support_idx = {k:0 for k in self.dataset.support_dataset}\n",
    "            self.pos_support_idx = {k:0 for k in self.dataset.support_dataset}\n",
    "            self.neg_support_len = {k:len(self.dataset.support_dataset[k]['neg']['text']) for k in self.dataset.support_dataset}\n",
    "            self.pos_support_len = {k:len(self.dataset.support_dataset[k]['pos']['text']) for k in self.dataset.support_dataset}\n",
    "        \n",
    "    def get_batch(self):\n",
    "        category = self.categories[self.idx % len(self.categories)]\n",
    "        neg = self.neg[category]\n",
    "        pos = self.pos[category]\n",
    "        neg_start_idx = self.neg_idx[category] % self.neg_len[category]\n",
    "        pos_start_idx = self.pos_idx[category] % self.pos_len[category]\n",
    "        \n",
    "        # prepare negative/positive dataset\n",
    "        neg_text = neg['text'][neg_start_idx:neg_start_idx+(self.batch_size//2)]\n",
    "        pos_text = pos['text'][pos_start_idx:pos_start_idx+(self.batch_size//2)]\n",
    "        neg_label = neg['label'][neg_start_idx:neg_start_idx+(self.batch_size//2)]\n",
    "        pos_label = pos['label'][pos_start_idx:pos_start_idx+(self.batch_size//2)]\n",
    "        self.neg_idx[category] += (self.batch_size//2)\n",
    "        self.pos_idx[category] += (self.batch_size//2)\n",
    "        \n",
    "        if len(neg_text) + len(pos_text) != self.batch_size:\n",
    "            return self.get_batch()\n",
    "            \n",
    "        # padding text dataset\n",
    "        neg_text = pad_sequence([n for n in neg_text], batch_first=True)\n",
    "        pos_text = pad_sequence([p for p in pos_text], batch_first=True)\n",
    "        neg_text, pos_text = pad_text(neg_text, pos_text)\n",
    "            \n",
    "        # prepare support/query text\n",
    "        neg_support_text = neg_text[:self.n_support//2]\n",
    "        pos_support_text = pos_text[:self.n_support//2]\n",
    "        neg_query_text = neg_text[self.n_support//2:]\n",
    "        pos_query_text = pos_text[self.n_support//2:]\n",
    "        \n",
    "        # prepare support/query label\n",
    "        neg_support_label = neg_label[:self.n_support//2]\n",
    "        pos_support_label = pos_label[:self.n_support//2]\n",
    "        neg_query_label = neg_label[self.n_support//2:]\n",
    "        pos_query_label = pos_label[self.n_support//2:]\n",
    "        \n",
    "        # merge support/query text\n",
    "        support_text = torch.cat([neg_support_text, pos_support_text], dim=0)\n",
    "        query_text = torch.cat([neg_query_text, pos_query_text], dim=0)\n",
    "        \n",
    "        # merge support/query label\n",
    "        support_label = torch.cat([neg_support_label, pos_support_label], dim=0)\n",
    "        query_label = torch.cat([neg_query_label, pos_query_label], dim=0)\n",
    "        \n",
    "        # make data and label\n",
    "        data = torch.cat([support_text, query_text], dim=0)\n",
    "        label = torch.cat([support_label, query_label], dim=0)\n",
    "        \n",
    "        # increase category index\n",
    "        self.idx += 1\n",
    "        return data, label\n",
    "    \n",
    "    def get_batch_test(self):\n",
    "        assert self.dataset.support_dataset, 'support_dataset is empty'\n",
    "        \n",
    "        category = self.categories[self.idx % len(self.categories)]\n",
    "        neg = self.neg[category]\n",
    "        pos = self.pos[category]\n",
    "        neg_query_start_idx = self.neg_idx[category] % self.neg_len[category]\n",
    "        pos_query_start_idx = self.pos_idx[category] % self.pos_len[category]\n",
    "        neg_support_start_idx = self.neg_support_idx[category] % self.neg_support_len[category]\n",
    "        pos_support_start_idx = self.pos_support_idx[category] % self.pos_support_len[category]\n",
    "        \n",
    "        # prepare negative/positive support dataset from support_dataset\n",
    "        category_suuport_dataset = self.dataset.support_dataset[category]\n",
    "        neg_support_text = category_suuport_dataset['neg']['text'][neg_support_start_idx:neg_support_start_idx+self.n_support//2]\n",
    "        pos_support_text = category_suuport_dataset['pos']['text'][pos_support_start_idx:pos_support_start_idx+self.n_support//2]\n",
    "        neg_support_label = category_suuport_dataset['neg']['label'][neg_support_start_idx:neg_support_start_idx+self.n_support//2]\n",
    "        pos_support_label = category_suuport_dataset['pos']['label'][pos_support_start_idx:pos_support_start_idx+self.n_support//2]\n",
    "        self.neg_support_idx[category] += (self.n_support//2)\n",
    "        self.pos_support_idx[category] += (self.n_support//2)\n",
    "        \n",
    "        # prepare negative/positive query dataset\n",
    "        neg_query_text = neg['text'][neg_query_start_idx:neg_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        pos_query_text = pos['text'][pos_query_start_idx:pos_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        neg_query_label = neg['label'][neg_query_start_idx:neg_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        pos_query_label = pos['label'][pos_query_start_idx:pos_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        self.neg_idx[category] += (self.batch_size//2 - self.n_support//2)\n",
    "        self.pos_idx[category] += (self.batch_size//2 - self.n_support//2)\n",
    "        \n",
    "        # padding support text dataset\n",
    "        if self.n_support:\n",
    "            neg_support_text = pad_sequence([n for n in neg_support_text], batch_first=True)\n",
    "            pos_support_text = pad_sequence([n for n in pos_support_text], batch_first=True)\n",
    "            neg_support_text, pos_support_text = pad_text(neg_support_text, pos_support_text)\n",
    "        else:\n",
    "            neg_support_text = torch.tensor([[]])\n",
    "            pos_support_text = torch.tensor([[]])\n",
    "            \n",
    "        # padding text dataset\n",
    "        neg_query_text = pad_sequence([n for n in neg_query_text], batch_first=True)\n",
    "        pos_query_text = pad_sequence([p for p in pos_query_text], batch_first=True)\n",
    "        neg_query_text, pos_query_text = pad_text(neg_query_text, pos_query_text)\n",
    "\n",
    "        # concatenating support/query text dataset\n",
    "        support_text = torch.cat([neg_support_text, pos_support_text], dim=0)\n",
    "        query_text = torch.cat([neg_query_text, pos_query_text], dim=0)\n",
    "        support_text, query_text = pad_text(support_text, query_text)\n",
    "\n",
    "        # make final data and label\n",
    "        if self.n_support:\n",
    "            data = torch.cat([support_text, query_text], dim=0)\n",
    "        else:\n",
    "            data = query_text\n",
    "        label = torch.cat([neg_support_label, pos_support_label, neg_query_label, pos_query_label], dim=0)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = AmazonDataLoader(train_dataset, batch_size=64, n_support=support*2)\n",
    "dev_dataloader = AmazonDataLoader(dev_dataset, batch_size=64, n_support=support*2)\n",
    "test_dataloader = AmazonDataLoader(test_dataset, batch_size=64, n_support=support*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 149]) tensor(0.5000)\n",
      "torch.Size([64, 460]) tensor(0.5000)\n",
      "torch.Size([64, 254]) tensor(0.5000)\n",
      "torch.Size([64, 262]) tensor(0.5000)\n",
      "torch.Size([64, 1283]) tensor(0.5000)\n",
      "torch.Size([64, 1658]) tensor(0.5000)\n",
      "torch.Size([64, 613]) tensor(0.5000)\n",
      "torch.Size([64, 359]) tensor(0.5000)\n",
      "torch.Size([64, 530]) tensor(0.5000)\n",
      "torch.Size([64, 602]) tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    d, l = train_dataloader.get_batch()\n",
    "    print(d.shape, l.float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 327]) tensor(0.5000)\n",
      "torch.Size([55, 181]) tensor(0.5818)\n",
      "torch.Size([64, 198]) tensor(0.5000)\n",
      "torch.Size([46, 295]) tensor(0.6957)\n",
      "torch.Size([64, 197]) tensor(0.5000)\n",
      "torch.Size([64, 276]) tensor(0.5000)\n",
      "torch.Size([55, 186]) tensor(0.5818)\n",
      "torch.Size([64, 270]) tensor(0.5000)\n",
      "torch.Size([26, 130]) tensor(0.4615)\n",
      "torch.Size([64, 327]) tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    d, l = dev_dataloader.get_batch_test()\n",
    "    print(d.shape, l.float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 743]) tensor(0.5000)\n",
      "torch.Size([64, 841]) tensor(0.5000)\n",
      "torch.Size([64, 1386]) tensor(0.5000)\n",
      "torch.Size([64, 706]) tensor(0.5000)\n",
      "torch.Size([64, 1026]) tensor(0.5000)\n",
      "torch.Size([64, 1126]) tensor(0.5000)\n",
      "torch.Size([64, 1116]) tensor(0.5000)\n",
      "torch.Size([64, 1333]) tensor(0.5000)\n",
      "torch.Size([64, 568]) tensor(0.5000)\n",
      "torch.Size([64, 570]) tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    d, l = test_dataloader.get_batch_test()\n",
    "    print(d.shape, l.float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AmazonDataLoader():\n",
    "#     def __init__(self, dataset, amount, batch_size):\n",
    "#         self.amount = amount\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         self.categories = list(dataset.dataset.keys())\n",
    "#         self.category_idx = 0\n",
    "#         self.indices_per_category = {\n",
    "#             category: 0 for category in self.categories\n",
    "#         }\n",
    "#         self.n_data_per_category = {\n",
    "#             category: len(dataset.dataset[category]['indice']) for category in self.categories\n",
    "#         }\n",
    "#     def get_batch(self):\n",
    "#         idx = self.category_idx % len(self.categories)\n",
    "#         category = self.categories[idx]\n",
    "#         n_data_per_category = self.n_data_per_category[category]\n",
    "#         start_idx = self.indices_per_category[category] % n_data_per_category\n",
    "#         indice = self.dataset.dataset[category]['indice'][start_idx:start_idx+self.batch_size]\n",
    "#         labels = self.dataset.dataset[category]['label'][start_idx:start_idx+self.batch_size]\n",
    "#         self.indices_per_category[category] += self.batch_size\n",
    "#         self.category_idx += 1\n",
    "        \n",
    "#         if len(indice) != self.batch_size:\n",
    "#             return self.get_batch()\n",
    "        \n",
    "#         indice = pad_sequence(indice, batch_first=True)\n",
    "#         return indice, labels, category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FewShotInduction(C=2,\n",
    "                         S=support,\n",
    "                         vocab_size=len(tokenizer),\n",
    "                         embed_size=300,\n",
    "                         hidden_size=128,\n",
    "                         d_a=64,\n",
    "                         iterations=3,\n",
    "                         outsize=100)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=float(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = Criterion(way=2, shot=support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episode):\n",
    "    model.train()\n",
    "    data, target = train_dataloader.get_batch()\n",
    "    data = data.cuda()\n",
    "    target = target.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    predict = model(data)\n",
    "    loss, acc = criterion(predict, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev(episode):\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    count = 0.\n",
    "    for i in range(100):\n",
    "        data, target = dev_dataloader.get_batch_test()\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        predict = model(data)\n",
    "        _, acc = criterion(predict, target)\n",
    "        amount = len(target) - support * 2\n",
    "        correct += acc * amount\n",
    "        count += amount\n",
    "    acc = correct / count\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    count = 0.\n",
    "    for i in range(100):\n",
    "        data, target = test_dataloader.get_batch_test()\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        predict = model(data)\n",
    "        _, acc = criterion(predict, target)\n",
    "        amount = len(target) - support * 2\n",
    "        correct += acc * amount\n",
    "        count += amount\n",
    "        \n",
    "    acc = correct / count\n",
    "    print('Test Acc: {}'.format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_interval = 100\n",
    "best_acc = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/9999 [00:24<1:39:28,  1.66it/s, loss=tensor(0.4911, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/9999 [00:49<1:41:05,  1.62it/s, loss=tensor(0.5000, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 500/9999 [02:02<2:03:14,  1.28it/s, loss=tensor(0.4907, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3200/9999 [12:54<1:27:25,  1.30it/s, loss=tensor(0.5259, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3601/9999 [14:31<1:03:33,  1.68it/s, loss=tensor(0.3047, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3701/9999 [14:56<1:05:37,  1.60it/s, loss=tensor(0.4654, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3900/9999 [15:44<1:18:19,  1.30it/s, loss=tensor(0.4810, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4000/9999 [16:07<1:18:58,  1.27it/s, loss=tensor(0.2676, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4100/9999 [16:32<1:18:33,  1.25it/s, loss=tensor(0.2664, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4200/9999 [16:56<1:16:40,  1.26it/s, loss=tensor(0.2961, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4300/9999 [17:20<1:14:57,  1.27it/s, loss=tensor(0.1779, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4500/9999 [18:09<1:10:59,  1.29it/s, loss=tensor(0.1996, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5001/9999 [20:11<50:16,  1.66it/s, loss=tensor(0.1750, device='cuda:0', grad_fn=<MeanBackward0>)]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5100/9999 [20:35<1:05:25,  1.25it/s, loss=tensor(0.3406, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5900/9999 [23:48<53:08,  1.29it/s, loss=tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6000/9999 [24:13<51:30,  1.29it/s, loss=tensor(0.1551, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6200/9999 [25:01<50:09,  1.26it/s, loss=tensor(0.2443, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6501/9999 [26:14<34:55,  1.67it/s, loss=tensor(0.1500, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6700/9999 [27:02<41:29,  1.33it/s, loss=tensor(0.1373, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 7600/9999 [30:41<32:03,  1.25it/s, loss=tensor(0.2500, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 8200/9999 [33:06<23:03,  1.30it/s, loss=tensor(0.0173, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 8400/9999 [33:55<21:26,  1.24it/s, loss=tensor(0.2435, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9000/9999 [36:20<13:24,  1.24it/s, loss=tensor(0.0522, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 9200/9999 [37:09<10:15,  1.30it/s, loss=tensor(0.0444, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 9301/9999 [37:33<07:19,  1.59it/s, loss=tensor(0.2773, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 9400/9999 [37:57<07:49,  1.28it/s, loss=tensor(0.0272, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 9800/9999 [39:33<02:45,  1.20it/s, loss=tensor(0.2823, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 9900/9999 [39:57<01:16,  1.30it/s, loss=tensor(0.2743, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [40:19<00:00,  4.13it/s, loss=tensor(0.2489, device='cuda:0', grad_fn=<MeanBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "tbar = tqdm(range(1, 10000))\n",
    "for episode in tbar:\n",
    "    \n",
    "    loss = train(episode)\n",
    "    if episode % dev_interval == 0:\n",
    "        acc = dev(episode)\n",
    "        if acc > best_acc:\n",
    "            print('Better acc! Saving model! -> {:.4f}'.format(acc))\n",
    "            best_acc = acc\n",
    "    tbar.set_postfix(loss=loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'fewshot_model_{support}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jkfirst jkfirst 62M 10월  7 13:11 fewshot_model.bin\r\n",
      "-rw-r--r-- 1 jkfirst jkfirst 62M 10월  7 13:11 fewshot_model_0.bin\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst 62M 10월  7 14:44 fewshot_model_5.bin\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh *.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['books', 'dvd', 'electronics', 'kitchen_housewares']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grocery',\n",
       " 'office_products',\n",
       " 'outdoor_living',\n",
       " 'gourmet_food',\n",
       " 'jewelry_watches']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apparel',\n",
       " 'automotive',\n",
       " 'baby',\n",
       " 'beauty',\n",
       " 'camera_photo',\n",
       " 'cell_phones_service',\n",
       " 'computer_video_games',\n",
       " 'health_personal_care',\n",
       " 'magazines',\n",
       " 'music',\n",
       " 'software',\n",
       " 'sports_outdoors',\n",
       " 'toys_games',\n",
       " 'video']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.6549707651138306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6550, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['books', 'dvd', 'electronics', 'kitchen_housewares']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg = 0\n",
    "n_pos = 0\n",
    "for c in test_dataset.categories:\n",
    "    n_neg += len(test_dataset.dataset[c]['neg']['label'])\n",
    "    n_pos += len(test_dataset.dataset[c]['pos']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987 7178\n"
     ]
    }
   ],
   "source": [
    "print(n_neg, n_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "\n",
    "class Criterion_(_Loss):\n",
    "    def __init__(self, way=2, shot=5):\n",
    "        super(Criterion_, self).__init__()\n",
    "        self.amount = way * shot\n",
    "\n",
    "    def forward(self, probs, target, return_pred_label=False):  # (Q,C) (Q)\n",
    "        target = target[self.amount:]\n",
    "        target_onehot = torch.zeros_like(probs)\n",
    "        #print('** sum of probs/target_onehot: {} {}'.format(probs.sum(), target_onehot.sum()))\n",
    "        target_onehot = target_onehot.scatter(1, target.reshape(-1, 1), 1)\n",
    "        loss = torch.mean((probs - target_onehot) ** 2)\n",
    "        pred = torch.argmax(probs, dim=1)\n",
    "        acc = torch.sum(target == pred).float() / target.shape[0]\n",
    "        #print('** acc: {}'.format(acc))\n",
    "\n",
    "        if return_pred_label:\n",
    "            return loss, acc, pred, target\n",
    "        else:\n",
    "            return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_ = Criterion_(way=2, shot=support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_():\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    count = 0.\n",
    "    p_list = []\n",
    "    l_list = []\n",
    "    r_list = []\n",
    "    for i in range(100):\n",
    "        data, target = test_dataloader.get_batch_test()\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        predict = model(data)\n",
    "        _, acc, p, l = criterion_(predict, target, return_pred_label=True)\n",
    "        amount = len(target) - 5 * 2    # 5 = support\n",
    "        correct += acc * amount\n",
    "        count += amount\n",
    "        \n",
    "        r = [1 if np.random.random() < 0.5 else 0 for _ in range(len(l))]\n",
    "        p_list.extend(list(p.cpu().numpy()))\n",
    "        l_list.extend(list(l.cpu().numpy()))\n",
    "        r_list.extend(r)\n",
    "    acc = correct / count\n",
    "    print('Test Acc: {}'.format(acc))\n",
    "    mat = confusion_matrix(l_list, p_list)\n",
    "    print(mat)\n",
    "    \n",
    "    rmat = confusion_matrix(l_list, r_list)\n",
    "    print(rmat)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# support=5\n",
    "Test Acc: 0.7019630074501038\n",
    "[[1707  921]\n",
    " [ 658 2012]]\n",
    "[[1293 1335]\n",
    " [1300 1370]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.6597026586532593\n",
      "[[1217 1408]\n",
      " [ 400 2288]]\n",
      "[[1322 1303]\n",
      " [1343 1345]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6597, device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5298"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1707+921+658+2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4960362400906002"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1293+1335)/(1293+1335+1300+1370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding=utf-8\n",
    "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Conditional text generation with the auto-regressive models of the library (GPT/GPT-2/CTRL/Transformer-XL/XLNet)\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, XLMConfig\n",
    "#from transformers import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, XLMConfig, CTRLConfig\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "from transformers import XLNetLMHeadModel, XLNetTokenizer\n",
    "from transformers import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "#from transformers import CTRLLMHeadModel, CTRLTokenizer\n",
    "from transformers import XLMWithLMHeadModel, XLMTokenizer\n",
    "\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "#ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, XLMConfig, CTRLConfig)), ())\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, XLMConfig)), ())\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    #'ctrl': (CTRLLMHeadModel, CTRLTokenizer),\n",
    "    'openai-gpt': (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'xlnet': (XLNetLMHeadModel, XLNetTokenizer),\n",
    "    'transfo-xl': (TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
    "    'xlm': (XLMWithLMHeadModel, XLMTokenizer),\n",
    "}\n",
    "\n",
    "# Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia\n",
    "# in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "# and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n",
    "PADDING_TEXT = \"\"\" In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(model, tokenizer, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
    "                    is_xlnet=False, is_xlm_mlm=False, xlm_mask_token=None, xlm_lang=None, device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    logger.info('** context: {}'.format(context.shape))\n",
    "    generated = context\n",
    "    with torch.no_grad():\n",
    "        for idx in range(length):\n",
    "\n",
    "            inputs = {'input_ids': generated}\n",
    "            if is_xlnet: \n",
    "                # XLNet is a direct (predict same token, not next token) and bi-directional model by default\n",
    "                # => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)\n",
    "                input_ids = torch.cat((generated, torch.zeros((1, 1), dtype=torch.long, device=device)), dim=1)\n",
    "                perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float, device=device)\n",
    "                perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n",
    "                target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float, device=device)\n",
    "                target_mapping[0, 0, -1] = 1.0  # predict last token\n",
    "                inputs = {'input_ids': input_ids, 'perm_mask': perm_mask, 'target_mapping': target_mapping}\n",
    "\n",
    "            if is_xlm_mlm and xlm_mask_token:\n",
    "                # XLM MLM models are direct models (predict same token, not next token)\n",
    "                # => need one additional dummy token in the input (will be masked and guessed)\n",
    "                input_ids = torch.cat((generated, torch.full((1, 1), xlm_mask_token, dtype=torch.long, device=device)), dim=1)\n",
    "                inputs = {'input_ids': input_ids}\n",
    "\n",
    "            if xlm_lang is not None:\n",
    "                inputs[\"langs\"] = torch.tensor([xlm_lang] * inputs[\"input_ids\"].shape[1], device=device).view(1, -1)\n",
    "\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "            print('** {}th: input_ids, next_token_logits: {} {}'.format(idx, generated.shape, next_token_logits.shape))\n",
    "            print('** {}th: input_ids -> {}:'.format(idx, tokenizer.decode(generated[0].tolist())))\n",
    "\n",
    "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "            for i in range(num_samples):\n",
    "                for _ in set(generated[i].tolist()):\n",
    "                    next_token_logits[i, _] /= repetition_penalty\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: # greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated\n",
    "\n",
    "def generate(args, model, tokenizer, raw_text):\n",
    "    context_tokens = tokenizer.encode(raw_text)\n",
    "    logger.info('context_tokens: {}'.format(context_tokens))\n",
    "    out = sample_sequence(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        context=context_tokens,\n",
    "        length=args.length,\n",
    "        temperature=args.temperature,\n",
    "        top_k=args.top_k,\n",
    "        top_p=args.top_p,\n",
    "        device=args.device,\n",
    "        is_xlnet=bool(args.model_type == \"xlnet\"),\n",
    "    )\n",
    "    logger.info('out length: {}'.format(len(out)))\n",
    "    logger.info('out[0]: {}'.format(out[0].shape))\n",
    "    logger.info('out[0, context_length:]: {}'.format(out[0, len(context_tokens):].shape))\n",
    "\n",
    "    out = out[0, len(context_tokens):].tolist()\n",
    "    text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
    "    logger.info('text: {}'.format(text))\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_type\", default=None, type=str, required=True,\n",
    "                        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "    parser.add_argument(\"--model_name_or_path\", default=None, type=str, required=True,\n",
    "                        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS))\n",
    "    parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--padding_text\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--xlm_lang\", type=str, default=\"\", help=\"Optional language when used with the XLM model.\")\n",
    "    parser.add_argument(\"--length\", type=int, default=20)\n",
    "    parser.add_argument(\"--num_samples\", type=int, default=1)\n",
    "    parser.add_argument(\"--temperature\", type=float, default=1.0,\n",
    "                        help=\"temperature of 0 implies greedy sampling\")\n",
    "    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0,\n",
    "                        help=\"primarily useful for CTRL model; in that case, use 1.2\")\n",
    "    parser.add_argument(\"--top_k\", type=int, default=0)\n",
    "    parser.add_argument(\"--top_p\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--no_cuda\", action='store_true',\n",
    "                        help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help=\"random seed for initialization\")\n",
    "    parser.add_argument('--stop_token', type=str, default=None,\n",
    "                        help=\"Token at which text generation is stopped\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    set_seed(args)\n",
    "\n",
    "    args.model_type = args.model_type.lower()\n",
    "    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
    "    model = model_class.from_pretrained(args.model_name_or_path)\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    if args.length < 0 and model.config.max_position_embeddings > 0:\n",
    "        args.length = model.config.max_position_embeddings\n",
    "    elif 0 < model.config.max_position_embeddings < args.length:\n",
    "        args.length = model.config.max_position_embeddings  # No generation bigger than model size \n",
    "    elif args.length < 0:\n",
    "        args.length = MAX_LENGTH  # avoid infinite loop\n",
    "\n",
    "    logger.info(args)\n",
    "    english = []\n",
    "    \n",
    "    with open('top_1000_zeroshot.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            english.append(line.lower().replace('\\n',''))\n",
    "                    \n",
    "    french = []\n",
    "    for en in english:\n",
    "        #prompt = f\"play : played . sing : sang . view : viewed . act : acted . say : said . type : typed . note : noted . see : saw . clean : cleaned . tell : told . {verb} : \"\n",
    "        #prompt = f\"play = played . sing = sang . view = viewed . act = acted . say = said . type = typed . note = noted . see = saw . clean = cleaned . tell = told . {verb} = \"\n",
    "        prompt = f\"Translate English to French: new jersey is sometimes quiet during autumn , and it is snowy in april . => new jersey est parfois calme pendant l' automne , et il est neigeux en avril . \\n the united states is usually chilly during july , and it is usually freezing in november . => les états-unis est généralement froid en juillet , et il gèle habituellement en novembre . \\n california is usually quiet during march , and it is usually hot in june . => california est généralement calme en mars , et il est généralement chaud en juin . \\n the united states is sometimes mild during june , and it is cold in september . => les états-unis est parfois légère en juin , et il fait froid en septembre . \\n your least liked fruit is the grape , but my least liked is the apple . => votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme . \\n his favorite fruit is the orange , but my favorite is the grape . => son fruit préféré est l'orange , mais mon préféré est le raisin . \\n {en} =>\"\n",
    "        fr = generate(args, model, tokenizer, prompt)\n",
    "        fr = fr.split(\".\")[0]\n",
    "        french.append(fr)\n",
    "\n",
    "    for en, fr in zip(english, french):\n",
    "        print('{} -> {}'.format(en, fr))\n",
    "\n",
    "    #with open('zeroshot_predictions.txt', 'w') as f:\n",
    "    #    for en, fr in zip(english, french):\n",
    "    #        f.write(f\"Noun: {en}, Plural: {fr}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pub",
   "language": "python",
   "name": "env_pub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
