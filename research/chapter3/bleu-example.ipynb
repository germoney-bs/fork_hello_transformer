{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d26a61de",
   "metadata": {},
   "source": [
    "### 이 주피터 노트북의 코드는 nltk에서 구현한 bleu 스코어 소스코드입니다.\n",
    "#### reference: https://www.nltk.org/_modules/nltk/translate/bleu_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785158d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "import sys\n",
    "from fractions import Fraction\n",
    "from nltk import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ba12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = 'you know I always love you so much'.split(' ')\n",
    "r2 = 'know right I always love you a lot'.split(' ')\n",
    "h1 = 'know that I always love you that much'.split(' ')\n",
    "h2 = 'you love I always'.split(' ')\n",
    "h3 = 'I I I I I'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb70138",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b63f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothingFunction:\n",
    "\n",
    "    def __init__(self, epsilon=0.1, alpha=5, k=5):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "    def method0(self, p_n, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        No smoothing.\n",
    "        \"\"\"\n",
    "        p_n_new = []\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator != 0:\n",
    "                p_n_new.append(p_i)\n",
    "            else:\n",
    "                _msg = str(\n",
    "                    \"\\nThe hypothesis contains 0 counts of {}-gram overlaps.\\n\"\n",
    "                    \"Therefore the BLEU score evaluates to 0, independently of\\n\"\n",
    "                    \"how many N-gram overlaps of lower order it contains.\\n\"\n",
    "                    \"Consider using lower n-gram order or use \"\n",
    "                    \"SmoothingFunction()\"\n",
    "                ).format(i + 1)\n",
    "                warnings.warn(_msg)\n",
    "                # When numerator==0 where denonminator==0 or !=0, the result\n",
    "                # for the precision score should be equal to 0 or undefined.\n",
    "                # Due to BLEU geometric mean computation in logarithm space,\n",
    "                # we we need to take the return sys.float_info.min such that\n",
    "                # math.log(sys.float_info.min) returns a 0 precision score.\n",
    "                p_n_new.append(1/100000000)\n",
    "        return p_n_new\n",
    "\n",
    "\n",
    "    def method1(self, p_n, *args, **kwargs):\n",
    "        return [\n",
    "            (p_i.numerator + self.epsilon) / p_i.denominator\n",
    "            if p_i.numerator == 0\n",
    "            else p_i\n",
    "            for p_i in p_n\n",
    "        ]\n",
    "\n",
    "\n",
    "    def method2(self, p_n, *args, **kwargs):\n",
    "        return [\n",
    "            Fraction(p_n[i].numerator + 1, p_n[i].denominator + 1, _normalize=False)\n",
    "            if i != 0\n",
    "            else p_n[0]\n",
    "            for i in range(len(p_n))\n",
    "        ]\n",
    "\n",
    "\n",
    "    def method3(self, p_n, *args, **kwargs):\n",
    "        incvnt = 1  # From the mteval-v13a.pl, it's referred to as k.\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator == 0:\n",
    "                p_n[i] = 1 / (2 ** incvnt * p_i.denominator)\n",
    "                incvnt += 1\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method4(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        incvnt = 1\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator == 0 and hyp_len > 1:\n",
    "                # incvnt = i + 1 * self.k / math.log(\n",
    "                #     hyp_len\n",
    "                # )  # Note that this K is different from the K from NIST.\n",
    "                # p_n[i] = incvnt / p_i.denominator\\\n",
    "                numerator = 1 / (2 ** incvnt * self.k / math.log(hyp_len))\n",
    "                p_n[i] = numerator / p_i.denominator\n",
    "                incvnt += 1\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method5(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        m = {}\n",
    "        # Requires an precision value for an addition ngram order.\n",
    "        p_n_plus1 = p_n + [modified_precision(references, hypothesis, 5)]\n",
    "        m[-1] = p_n[0] + 1\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            p_n[i] = (m[i - 1] + p_i + p_n_plus1[i + 1]) / 3\n",
    "            m[i] = p_n[i]\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method6(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        # This smoothing only works when p_1 and p_2 is non-zero.\n",
    "        # Raise an error with an appropriate message when the input is too short\n",
    "        # to use this smoothing technique.\n",
    "        assert p_n[2], \"This smoothing method requires non-zero precision for bigrams.\"\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if i in [0, 1]:  # Skips the first 2 orders of ngrams.\n",
    "                continue\n",
    "            else:\n",
    "                pi0 = 0 if p_n[i - 2] == 0 else p_n[i - 1] ** 2 / p_n[i - 2]\n",
    "                # No. of ngrams in translation that matches the reference.\n",
    "                m = p_i.numerator\n",
    "                # No. of ngrams in translation.\n",
    "                l = sum(1 for _ in ngrams(hypothesis, i + 1))\n",
    "                # Calculates the interpolated precision.\n",
    "                p_n[i] = (m + self.alpha * pi0) / (l + self.alpha)\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method7(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        p_n = self.method4(p_n, references, hypothesis, hyp_len)\n",
    "        p_n = self.method5(p_n, references, hypothesis, hyp_len)\n",
    "        return p_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "632984d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(closest_ref_len, hyp_len):\n",
    "    '''\n",
    "    closest_ref_len: 후보문장과 가장 길이 차이가 작은 문장의 길이\n",
    "    hyp_len: 후보문장의 길이의 길이\n",
    "    '''\n",
    "    if hyp_len > closest_ref_len:\n",
    "        return 1\n",
    "    # If hypothesis is empty, brevity penalty = 0 should result in BLEU = 0.0\n",
    "    elif hyp_len == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.exp(1 - closest_ref_len / hyp_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f6fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_ref_length(references, hyp_len):\n",
    "    ref_lens = (len(reference) for reference in references)\n",
    "    closest_ref_len = min(\n",
    "        ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len)\n",
    "    )\n",
    "    return closest_ref_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8970dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(ngrams(h1, 1))\n",
    "max_counts = {}\n",
    "for reference in [r1, r2]:\n",
    "    reference_counts = (\n",
    "        Counter(ngrams(reference, 1)) if len(reference) >= 1 else Counter()\n",
    "    )\n",
    "    for ngram in counts:\n",
    "        max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06839b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('know',): 1,\n",
       "  ('that',): 0,\n",
       "  ('I',): 1,\n",
       "  ('always',): 1,\n",
       "  ('love',): 1,\n",
       "  ('you',): 2,\n",
       "  ('much',): 1},\n",
       " Counter({('know',): 1,\n",
       "          ('that',): 2,\n",
       "          ('I',): 1,\n",
       "          ('always',): 1,\n",
       "          ('love',): 1,\n",
       "          ('you',): 1,\n",
       "          ('much',): 1}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_counts, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7bb62c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(references, hypothesis, n):\n",
    "    # Extracts all ngrams in hypothesis\n",
    "    # Set an empty Counter if hypothesis is empty.\n",
    "    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n",
    "    # Extract a union of references' counts.\n",
    "    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n",
    "    max_counts = {}\n",
    "    \n",
    "    # hypothesis의 ngram 단어들에 대해서 아래의 작업을 수행해서 ngram 마다 최대 카운트 수를 구한다.\n",
    "    # -> ngram 단어가 reference에서 나온 카운트 수를 구함 (A)\n",
    "    # -> 지금까지 나온 레퍼런스 중에서 ngram 단어가 나온 최대 카운트 수 (B)\n",
    "    # -> A와 B 중 큰 수 구하기\n",
    "    for reference in references:\n",
    "        reference_counts = (\n",
    "            Counter(ngrams(reference, n)) if len(reference) >= n else Counter()\n",
    "        )\n",
    "        for ngram in counts:\n",
    "            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
    "\n",
    "    # Assigns the intersection between hypothesis and references' counts.\n",
    "    # print('Do clip = intersection')\n",
    "        \n",
    "    clipped_counts = {\n",
    "        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n",
    "    }\n",
    "\n",
    "    numerator = sum(clipped_counts.values())\n",
    "    # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n",
    "    # Usually this happens when the ngram order is > len(reference).\n",
    "    denominator = max(1, sum(counts.values()))\n",
    "\n",
    "    return Fraction(numerator, denominator, _normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ccd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_bleu(\n",
    "    list_of_references,\n",
    "    hypotheses,\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    smoothing_function=None,\n",
    "    auto_reweigh=False,\n",
    "):\n",
    "    p_numerators = Counter()  # Key = ngram order, and value = no. of ngram matches.\n",
    "    p_denominators = Counter()  # Key = ngram order, and value = no. of ngram in ref.\n",
    "    hyp_lengths, ref_lengths = 0, 0\n",
    "\n",
    "    assert len(list_of_references) == len(hypotheses), (\n",
    "        \"The number of hypotheses and their reference(s) should be the \" \"same \"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        weights[0][0]\n",
    "    except TypeError:\n",
    "        weights = [weights]\n",
    "    max_weight_length = max(len(weight) for weight in weights)\n",
    "\n",
    "    # Iterate through each hypothesis and their corresponding references.\n",
    "    for references, hypothesis in zip(list_of_references, hypotheses):\n",
    "        # For each order of ngram, calculate the numerator and\n",
    "        # denominator for the corpus-level modified precision.\n",
    "        # 1 ~ N gram에 대해서 계산하기\n",
    "        for i in range(1, max_weight_length + 1):\n",
    "            # print(f'{i} -> r={references}   h={hypothesis}')\n",
    "            p_i = modified_precision(references, hypothesis, i)\n",
    "            p_numerators[i] += p_i.numerator\n",
    "            p_denominators[i] += p_i.denominator\n",
    "\n",
    "        # Calculate the hypothesis length and the closest reference length.\n",
    "        # Adds them to the corpus-level hypothesis and reference counts.\n",
    "        # brevity penalty를 구하기 위해서 계산함!!\n",
    "        hyp_len = len(hypothesis)\n",
    "        hyp_lengths += hyp_len\n",
    "        ref_lengths += closest_ref_length(references, hyp_len)\n",
    "\n",
    "    # Calculate corpus-level brevity penalty.\n",
    "    bp = brevity_penalty(ref_lengths, hyp_lengths)\n",
    "    # Collects the various precision values for the different ngram orders.\n",
    "    p_n = [\n",
    "        Fraction(p_numerators[i], p_denominators[i], _normalize=False)\n",
    "        for i in range(1, max_weight_length + 1)\n",
    "    ]\n",
    "\n",
    "    # Returns 0 if there's no matching n-grams\n",
    "    # We only need to check for p_numerators[1] == 0, since if there's\n",
    "    # no unigrams, there won't be any higher order ngrams.\n",
    "    if p_numerators[1] == 0:\n",
    "        return 0 if len(weights) == 1 else [0] * len(weights)\n",
    "\n",
    "    # If there's no smoothing, set use method0 from SmoothinFunction class.\n",
    "    if not smoothing_function:\n",
    "        smoothing_function = SmoothingFunction().method0\n",
    "    # Smoothen the modified precision.\n",
    "    # Note: smoothing_function() may convert values into floats;\n",
    "    #       it tries to retain the Fraction object as much as the\n",
    "    #       smoothing method allows.\n",
    "    p_n = smoothing_function(\n",
    "        p_n, references=references, hypothesis=hypothesis, hyp_len=hyp_lengths\n",
    "    )\n",
    "\n",
    "    bleu_scores = []\n",
    "    for weight in weights:\n",
    "        # Uniformly re-weighting based on maximum hypothesis lengths if largest\n",
    "        # order of n-grams < 4 and weights is set at default.\n",
    "        if auto_reweigh:\n",
    "            if hyp_lengths < 4 and weight == (0.25, 0.25, 0.25, 0.25):\n",
    "                weight = (1 / hyp_lengths,) * hyp_lengths\n",
    "\n",
    "        s = (w_i * math.log(p_i) for w_i, p_i in zip(weight, p_n) if p_i > 0)\n",
    "        s = bp * math.exp(math.fsum(s))\n",
    "        bleu_scores.append(s)\n",
    "    return bleu_scores[0] if len(weights) == 1 else bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b2fe439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_bleu(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    smoothing_function=None,\n",
    "    auto_reweigh=False,\n",
    "):\n",
    "    return corpus_bleu(\n",
    "        [references], [hypothesis], weights, smoothing_function, auto_reweigh\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2228127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38260294162784475"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu([r1, r2], h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e8b84dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105363/1682077881.py:25: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/tmp/ipykernel_105363/1682077881.py:25: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.795279274196275e-05"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu([r1, r2], h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37789677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105363/1682077881.py:25: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/tmp/ipykernel_105363/1682077881.py:25: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/tmp/ipykernel_105363/1682077881.py:25: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.670124608961276e-07"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu([r1, r2], h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2027daac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_hello_three",
   "language": "python",
   "name": "env_hello_three"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
