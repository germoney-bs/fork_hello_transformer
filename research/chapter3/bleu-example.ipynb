{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371e3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 주피터 노트북의 코드는 nltk에서 구현한 bleu 스코어 소스코드입니다.\n",
    "# reference: https://www.nltk.org/_modules/nltk/translate/bleu_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6a4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "import sys\n",
    "from fractions import Fraction\n",
    "from nltk import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5d1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = 'you know I always love you so much'.split(' ')\n",
    "r2 = 'know right I always love you a lot'.split(' ')\n",
    "h1 = 'know that I always love you that much'.split(' ')\n",
    "h2 = 'you love I always'.split(' ')\n",
    "h3 = 'I I I I I'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbbe424",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87459867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothingFunction:\n",
    "\n",
    "    def __init__(self, epsilon=0.1, alpha=5, k=5):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "    def method0(self, p_n, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        No smoothing.\n",
    "        \"\"\"\n",
    "        p_n_new = []\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator != 0:\n",
    "                p_n_new.append(p_i)\n",
    "            else:\n",
    "                _msg = str(\n",
    "                    \"\\nThe hypothesis contains 0 counts of {}-gram overlaps.\\n\"\n",
    "                    \"Therefore the BLEU score evaluates to 0, independently of\\n\"\n",
    "                    \"how many N-gram overlaps of lower order it contains.\\n\"\n",
    "                    \"Consider using lower n-gram order or use \"\n",
    "                    \"SmoothingFunction()\"\n",
    "                ).format(i + 1)\n",
    "                warnings.warn(_msg)\n",
    "                # When numerator==0 where denonminator==0 or !=0, the result\n",
    "                # for the precision score should be equal to 0 or undefined.\n",
    "                # Due to BLEU geometric mean computation in logarithm space,\n",
    "                # we we need to take the return sys.float_info.min such that\n",
    "                # math.log(sys.float_info.min) returns a 0 precision score.\n",
    "                p_n_new.append(1/100000000)\n",
    "        return p_n_new\n",
    "\n",
    "\n",
    "    def method1(self, p_n, *args, **kwargs):\n",
    "        return [\n",
    "            (p_i.numerator + self.epsilon) / p_i.denominator\n",
    "            if p_i.numerator == 0\n",
    "            else p_i\n",
    "            for p_i in p_n\n",
    "        ]\n",
    "\n",
    "\n",
    "    def method2(self, p_n, *args, **kwargs):\n",
    "        return [\n",
    "            Fraction(p_n[i].numerator + 1, p_n[i].denominator + 1, _normalize=False)\n",
    "            if i != 0\n",
    "            else p_n[0]\n",
    "            for i in range(len(p_n))\n",
    "        ]\n",
    "\n",
    "\n",
    "    def method3(self, p_n, *args, **kwargs):\n",
    "        incvnt = 1  # From the mteval-v13a.pl, it's referred to as k.\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator == 0:\n",
    "                p_n[i] = 1 / (2 ** incvnt * p_i.denominator)\n",
    "                incvnt += 1\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method4(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        incvnt = 1\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator == 0 and hyp_len > 1:\n",
    "                # incvnt = i + 1 * self.k / math.log(\n",
    "                #     hyp_len\n",
    "                # )  # Note that this K is different from the K from NIST.\n",
    "                # p_n[i] = incvnt / p_i.denominator\\\n",
    "                numerator = 1 / (2 ** incvnt * self.k / math.log(hyp_len))\n",
    "                p_n[i] = numerator / p_i.denominator\n",
    "                incvnt += 1\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method5(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        m = {}\n",
    "        # Requires an precision value for an addition ngram order.\n",
    "        p_n_plus1 = p_n + [modified_precision(references, hypothesis, 5)]\n",
    "        m[-1] = p_n[0] + 1\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            p_n[i] = (m[i - 1] + p_i + p_n_plus1[i + 1]) / 3\n",
    "            m[i] = p_n[i]\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method6(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        # This smoothing only works when p_1 and p_2 is non-zero.\n",
    "        # Raise an error with an appropriate message when the input is too short\n",
    "        # to use this smoothing technique.\n",
    "        assert p_n[2], \"This smoothing method requires non-zero precision for bigrams.\"\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if i in [0, 1]:  # Skips the first 2 orders of ngrams.\n",
    "                continue\n",
    "            else:\n",
    "                pi0 = 0 if p_n[i - 2] == 0 else p_n[i - 1] ** 2 / p_n[i - 2]\n",
    "                # No. of ngrams in translation that matches the reference.\n",
    "                m = p_i.numerator\n",
    "                # No. of ngrams in translation.\n",
    "                l = sum(1 for _ in ngrams(hypothesis, i + 1))\n",
    "                # Calculates the interpolated precision.\n",
    "                p_n[i] = (m + self.alpha * pi0) / (l + self.alpha)\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method7(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        p_n = self.method4(p_n, references, hypothesis, hyp_len)\n",
    "        p_n = self.method5(p_n, references, hypothesis, hyp_len)\n",
    "        return p_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c959c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(closest_ref_len, hyp_len):\n",
    "    '''\n",
    "    closest_ref_len: 후보문장과 가장 길이 차이가 작은 문장의 길이\n",
    "    hyp_len: 후보문장의 길이의 길이\n",
    "    '''\n",
    "    if hyp_len > closest_ref_len:\n",
    "        return 1\n",
    "    # If hypothesis is empty, brevity penalty = 0 should result in BLEU = 0.0\n",
    "    elif hyp_len == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.exp(1 - closest_ref_len / hyp_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36284e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_ref_length(references, hyp_len):\n",
    "    ref_lens = (len(reference) for reference in references)\n",
    "    closest_ref_len = min(\n",
    "        ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len)\n",
    "    )\n",
    "    return closest_ref_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b4d5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(ngrams(h1, 1))\n",
    "max_counts = {}\n",
    "for reference in [r1, r2]:\n",
    "    reference_counts = (\n",
    "        Counter(ngrams(reference, 1)) if len(reference) >= 1 else Counter()\n",
    "    )\n",
    "    for ngram in counts:\n",
    "        max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "924c10ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('know',): 1,\n",
       "  ('that',): 0,\n",
       "  ('I',): 1,\n",
       "  ('always',): 1,\n",
       "  ('love',): 1,\n",
       "  ('you',): 2,\n",
       "  ('much',): 1},\n",
       " Counter({('know',): 1,\n",
       "          ('that',): 2,\n",
       "          ('I',): 1,\n",
       "          ('always',): 1,\n",
       "          ('love',): 1,\n",
       "          ('you',): 1,\n",
       "          ('much',): 1}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_counts, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e541d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(references, hypothesis, n):\n",
    "    # Extracts all ngrams in hypothesis\n",
    "    # Set an empty Counter if hypothesis is empty.\n",
    "    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n",
    "    # Extract a union of references' counts.\n",
    "    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n",
    "    max_counts = {}\n",
    "    \n",
    "    # hypothesis의 ngram 단어들에 대해서 아래의 작업을 수행해서 ngram 마다 최대 카운트 수를 구한다.\n",
    "    # -> ngram 단어가 reference에서 나온 카운트 수를 구함 (A)\n",
    "    # -> 지금까지 나온 레퍼런스 중에서 ngram 단어가 나온 최대 카운트 수 (B)\n",
    "    # -> A와 B 중 큰 수 구하기\n",
    "    for reference in references:\n",
    "        reference_counts = (\n",
    "            Counter(ngrams(reference, n)) if len(reference) >= n else Counter()\n",
    "        )\n",
    "        for ngram in counts:\n",
    "            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
    "    print(f'At {n}-gram')\n",
    "    print(f'max_counts: {max_counts}')\n",
    "    print(f'counts: {counts}')\n",
    "\n",
    "    # Assigns the intersection between hypothesis and references' counts.\n",
    "    # print('Do clip = intersection')\n",
    "        \n",
    "    clipped_counts = {\n",
    "        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n",
    "    }\n",
    "    print(f'clipped_counts: {clipped_counts}')\n",
    "\n",
    "    numerator = sum(clipped_counts.values())\n",
    "    # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n",
    "    # Usually this happens when the ngram order is > len(reference).\n",
    "    denominator = max(1, sum(counts.values()))\n",
    "    print(denominator)\n",
    "\n",
    "    return Fraction(numerator, denominator, _normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1256f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_bleu(\n",
    "    list_of_references,\n",
    "    hypotheses,\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    smoothing_function=None,\n",
    "    auto_reweigh=False,\n",
    "):\n",
    "    p_numerators = Counter()  # Key = ngram order, and value = no. of ngram matches.\n",
    "    p_denominators = Counter()  # Key = ngram order, and value = no. of ngram in ref.\n",
    "    hyp_lengths, ref_lengths = 0, 0\n",
    "\n",
    "    assert len(list_of_references) == len(hypotheses), (\n",
    "        \"The number of hypotheses and their reference(s) should be the \" \"same \"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        weights[0][0]\n",
    "    except TypeError:\n",
    "        weights = [weights]\n",
    "    max_weight_length = max(len(weight) for weight in weights)\n",
    "\n",
    "    # Iterate through each hypothesis and their corresponding references.\n",
    "    for references, hypothesis in zip(list_of_references, hypotheses):\n",
    "        print(references, hypotheses)\n",
    "        # For each order of ngram, calculate the numerator and\n",
    "        # denominator for the corpus-level modified precision.\n",
    "        # 1 ~ N gram에 대해서 계산하기\n",
    "        for i in range(1, max_weight_length + 1):\n",
    "            # print(f'{i} -> r={references}   h={hypothesis}')\n",
    "            p_i = modified_precision(references, hypothesis, i)\n",
    "            p_numerators[i] += p_i.numerator\n",
    "            p_denominators[i] += p_i.denominator\n",
    "            print(f'p_{i} -> numerator={p_i.numerator} denominator={p_i.denominator} by modified_precision with {i}-Gram')\n",
    "\n",
    "        # Calculate the hypothesis length and the closest reference length.\n",
    "        # Adds them to the corpus-level hypothesis and reference counts.\n",
    "        # brevity penalty를 구하기 위해서 계산함!!\n",
    "        hyp_len = len(hypothesis)\n",
    "        hyp_lengths += hyp_len\n",
    "        ref_lengths += closest_ref_length(references, hyp_len)\n",
    "\n",
    "    # Calculate corpus-level brevity penalty.\n",
    "    bp = brevity_penalty(ref_lengths, hyp_lengths)\n",
    "    print(f'brevity penalty: {bp}')\n",
    "\n",
    "    print(f'p_numerator: {p_numerators}')\n",
    "    print(f'p_denominators: {p_denominators}')\n",
    "    # Collects the various precision values for the different ngram orders.\n",
    "    p_n = [\n",
    "        Fraction(p_numerators[i], p_denominators[i], _normalize=False)\n",
    "        for i in range(1, max_weight_length + 1)\n",
    "    ]\n",
    "\n",
    "    # Returns 0 if there's no matching n-grams\n",
    "    # We only need to check for p_numerators[1] == 0, since if there's\n",
    "    # no unigrams, there won't be any higher order ngrams.\n",
    "    if p_numerators[1] == 0:\n",
    "        return 0 if len(weights) == 1 else [0] * len(weights)\n",
    "\n",
    "    print(f'p_n: {p_n}')\n",
    "    # If there's no smoothing, set use method0 from SmoothinFunction class.\n",
    "    if not smoothing_function:\n",
    "        smoothing_function = SmoothingFunction().method0\n",
    "    # Smoothen the modified precision.\n",
    "    # Note: smoothing_function() may convert values into floats;\n",
    "    #       it tries to retain the Fraction object as much as the\n",
    "    #       smoothing method allows.\n",
    "    p_n = smoothing_function(\n",
    "        p_n, references=references, hypothesis=hypothesis, hyp_len=hyp_lengths\n",
    "    )\n",
    "    print(f'p_n smoothing: {p_n}')\n",
    "\n",
    "    bleu_scores = []\n",
    "    for weight in weights:\n",
    "        # Uniformly re-weighting based on maximum hypothesis lengths if largest\n",
    "        # order of n-grams < 4 and weights is set at default.\n",
    "        if auto_reweigh:\n",
    "            if hyp_lengths < 4 and weight == (0.25, 0.25, 0.25, 0.25):\n",
    "                weight = (1 / hyp_lengths,) * hyp_lengths\n",
    "\n",
    "        s = (w_i * math.log(p_i) for w_i, p_i in zip(weight, p_n) if p_i > 0)\n",
    "        s = bp * math.exp(math.fsum(s))\n",
    "        bleu_scores.append(s)\n",
    "    return bleu_scores[0] if len(weights) == 1 else bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c02d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_bleu(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    smoothing_function=None,\n",
    "    auto_reweigh=False,\n",
    "):\n",
    "    return corpus_bleu(\n",
    "        [references], [hypothesis], weights, smoothing_function, auto_reweigh\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11690886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'know', 'I', 'always', 'love', 'you', 'so', 'much']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a462dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['know', 'right', 'I', 'always', 'love', 'you', 'a', 'lot']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1483c448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['know', 'that', 'I', 'always', 'love', 'you', 'that', 'much']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "219da258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'love', 'I', 'always']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94e6a7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'I', 'I', 'I', 'I']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98112176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['you', 'know', 'I', 'always', 'love', 'you', 'so', 'much'], ['know', 'right', 'I', 'always', 'love', 'you', 'a', 'lot']] [['know', 'that', 'I', 'always', 'love', 'you', 'that', 'much']]\n",
      "At 1-gram\n",
      "max_counts: {('know',): 1, ('that',): 0, ('I',): 1, ('always',): 1, ('love',): 1, ('you',): 2, ('much',): 1}\n",
      "counts: Counter({('that',): 2, ('know',): 1, ('I',): 1, ('always',): 1, ('love',): 1, ('you',): 1, ('much',): 1})\n",
      "clipped_counts: {('know',): 1, ('that',): 0, ('I',): 1, ('always',): 1, ('love',): 1, ('you',): 1, ('much',): 1}\n",
      "8\n",
      "p_1 -> numerator=6 denominator=8 by modified_precision with 1-Gram\n",
      "At 2-gram\n",
      "max_counts: {('know', 'that'): 0, ('that', 'I'): 0, ('I', 'always'): 1, ('always', 'love'): 1, ('love', 'you'): 1, ('you', 'that'): 0, ('that', 'much'): 0}\n",
      "counts: Counter({('know', 'that'): 1, ('that', 'I'): 1, ('I', 'always'): 1, ('always', 'love'): 1, ('love', 'you'): 1, ('you', 'that'): 1, ('that', 'much'): 1})\n",
      "clipped_counts: {('know', 'that'): 0, ('that', 'I'): 0, ('I', 'always'): 1, ('always', 'love'): 1, ('love', 'you'): 1, ('you', 'that'): 0, ('that', 'much'): 0}\n",
      "7\n",
      "p_2 -> numerator=3 denominator=7 by modified_precision with 2-Gram\n",
      "At 3-gram\n",
      "max_counts: {('know', 'that', 'I'): 0, ('that', 'I', 'always'): 0, ('I', 'always', 'love'): 1, ('always', 'love', 'you'): 1, ('love', 'you', 'that'): 0, ('you', 'that', 'much'): 0}\n",
      "counts: Counter({('know', 'that', 'I'): 1, ('that', 'I', 'always'): 1, ('I', 'always', 'love'): 1, ('always', 'love', 'you'): 1, ('love', 'you', 'that'): 1, ('you', 'that', 'much'): 1})\n",
      "clipped_counts: {('know', 'that', 'I'): 0, ('that', 'I', 'always'): 0, ('I', 'always', 'love'): 1, ('always', 'love', 'you'): 1, ('love', 'you', 'that'): 0, ('you', 'that', 'much'): 0}\n",
      "6\n",
      "p_3 -> numerator=2 denominator=6 by modified_precision with 3-Gram\n",
      "At 4-gram\n",
      "max_counts: {('know', 'that', 'I', 'always'): 0, ('that', 'I', 'always', 'love'): 0, ('I', 'always', 'love', 'you'): 1, ('always', 'love', 'you', 'that'): 0, ('love', 'you', 'that', 'much'): 0}\n",
      "counts: Counter({('know', 'that', 'I', 'always'): 1, ('that', 'I', 'always', 'love'): 1, ('I', 'always', 'love', 'you'): 1, ('always', 'love', 'you', 'that'): 1, ('love', 'you', 'that', 'much'): 1})\n",
      "clipped_counts: {('know', 'that', 'I', 'always'): 0, ('that', 'I', 'always', 'love'): 0, ('I', 'always', 'love', 'you'): 1, ('always', 'love', 'you', 'that'): 0, ('love', 'you', 'that', 'much'): 0}\n",
      "5\n",
      "p_4 -> numerator=1 denominator=5 by modified_precision with 4-Gram\n",
      "brevity penalty: 1.0\n",
      "p_numerator: Counter({1: 6, 2: 3, 3: 2, 4: 1})\n",
      "p_denominators: Counter({1: 8, 2: 7, 3: 6, 4: 5})\n",
      "p_n: [Fraction(6, 8), Fraction(3, 7), Fraction(2, 6), Fraction(1, 5)]\n",
      "p_n smoothing: [Fraction(6, 8), Fraction(3, 7), Fraction(2, 6), Fraction(1, 5)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.38260294162784475"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu([r1, r2], h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a8f2c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['you', 'know', 'I', 'always', 'love', 'you', 'so', 'much'], ['know', 'right', 'I', 'always', 'love', 'you', 'a', 'lot']] [['you', 'love', 'I', 'always']]\n",
      "At 1-gram\n",
      "max_counts: {('you',): 2, ('love',): 1, ('I',): 1, ('always',): 1}\n",
      "counts: Counter({('you',): 1, ('love',): 1, ('I',): 1, ('always',): 1})\n",
      "clipped_counts: {('you',): 1, ('love',): 1, ('I',): 1, ('always',): 1}\n",
      "4\n",
      "p_1 -> numerator=4 denominator=4 by modified_precision with 1-Gram\n",
      "At 2-gram\n",
      "max_counts: {('you', 'love'): 0, ('love', 'I'): 0, ('I', 'always'): 1}\n",
      "counts: Counter({('you', 'love'): 1, ('love', 'I'): 1, ('I', 'always'): 1})\n",
      "clipped_counts: {('you', 'love'): 0, ('love', 'I'): 0, ('I', 'always'): 1}\n",
      "3\n",
      "p_2 -> numerator=1 denominator=3 by modified_precision with 2-Gram\n",
      "At 3-gram\n",
      "max_counts: {('you', 'love', 'I'): 0, ('love', 'I', 'always'): 0}\n",
      "counts: Counter({('you', 'love', 'I'): 1, ('love', 'I', 'always'): 1})\n",
      "clipped_counts: {('you', 'love', 'I'): 0, ('love', 'I', 'always'): 0}\n",
      "2\n",
      "p_3 -> numerator=0 denominator=2 by modified_precision with 3-Gram\n",
      "At 4-gram\n",
      "max_counts: {('you', 'love', 'I', 'always'): 0}\n",
      "counts: Counter({('you', 'love', 'I', 'always'): 1})\n",
      "clipped_counts: {('you', 'love', 'I', 'always'): 0}\n",
      "1\n",
      "p_4 -> numerator=0 denominator=1 by modified_precision with 4-Gram\n",
      "brevity penalty: 0.36787944117144233\n",
      "p_numerator: Counter({1: 4, 2: 1, 3: 0, 4: 0})\n",
      "p_denominators: Counter({1: 4, 2: 3, 3: 2, 4: 1})\n",
      "p_n: [Fraction(4, 4), Fraction(1, 3), Fraction(0, 2), Fraction(0, 1)]\n",
      "p_n smoothing: [Fraction(4, 4), Fraction(1, 3), 1e-08, 1e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICTUSER\\AppData\\Local\\Temp\\ipykernel_7400\\1682077881.py:25: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\ICTUSER\\AppData\\Local\\Temp\\ipykernel_7400\\1682077881.py:25: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.795279274196275e-05"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu([r1, r2], h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fa372dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['you', 'know', 'I', 'always', 'love', 'you', 'so', 'much'], ['know', 'right', 'I', 'always', 'love', 'you', 'a', 'lot']] [['I', 'I', 'I', 'I', 'I']]\n",
      "At 1-gram\n",
      "max_counts: {('I',): 1}\n",
      "counts: Counter({('I',): 5})\n",
      "clipped_counts: {('I',): 1}\n",
      "5\n",
      "p_1 -> numerator=1 denominator=5 by modified_precision with 1-Gram\n",
      "At 2-gram\n",
      "max_counts: {('I', 'I'): 0}\n",
      "counts: Counter({('I', 'I'): 4})\n",
      "clipped_counts: {('I', 'I'): 0}\n",
      "4\n",
      "p_2 -> numerator=0 denominator=4 by modified_precision with 2-Gram\n",
      "At 3-gram\n",
      "max_counts: {('I', 'I', 'I'): 0}\n",
      "counts: Counter({('I', 'I', 'I'): 3})\n",
      "clipped_counts: {('I', 'I', 'I'): 0}\n",
      "3\n",
      "p_3 -> numerator=0 denominator=3 by modified_precision with 3-Gram\n",
      "At 4-gram\n",
      "max_counts: {('I', 'I', 'I', 'I'): 0}\n",
      "counts: Counter({('I', 'I', 'I', 'I'): 2})\n",
      "clipped_counts: {('I', 'I', 'I', 'I'): 0}\n",
      "2\n",
      "p_4 -> numerator=0 denominator=2 by modified_precision with 4-Gram\n",
      "brevity penalty: 0.5488116360940264\n",
      "p_numerator: Counter({1: 1, 2: 0, 3: 0, 4: 0})\n",
      "p_denominators: Counter({1: 5, 2: 4, 3: 3, 4: 2})\n",
      "p_n: [Fraction(1, 5), Fraction(0, 4), Fraction(0, 3), Fraction(0, 2)]\n",
      "p_n smoothing: [Fraction(1, 5), 1e-08, 1e-08, 1e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICTUSER\\AppData\\Local\\Temp\\ipykernel_7400\\1682077881.py:25: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\ICTUSER\\AppData\\Local\\Temp\\ipykernel_7400\\1682077881.py:25: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\ICTUSER\\AppData\\Local\\Temp\\ipykernel_7400\\1682077881.py:25: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.670124608961276e-07"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu([r1, r2], h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0e4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab13e134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42797619047619045"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25*6/8 + 0.25*3/7 + 0.25*2/6 + 0.25*1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e505f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25*4/4 + 0.25*1/3 + 0.25*0/2 + 0.25*0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aac8fe96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25*1/5 + 0.25*0/4 + 0.25*0/3 + 0.25*0/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c88111b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = math.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cc365ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9607575334852987"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25*log(6/8) + 0.25*log(3/7) + 0.25*log(2/6) + 0.25*log(1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abd4feff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.48499344414321"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25*log(4/4) + 0.25*log(1/3) + 0.25*log(1/100000000) + 0.25*log(1/100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f942036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.217870036072801"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25*log(1/5) + 0.25*log(1/100000000) + 0.25*log(1/100000000) + 0.25*log(1/100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfcc1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = math.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8149897a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38260294162784475"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(-0.9607575334852987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "803f9cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.598356856515924e-05"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(-9.48499344414321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74c8ec92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.687403049764207e-07"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(-14.217870036072801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f30250a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'know', 'I', 'always', 'love', 'you', 'so', 'much']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ddb20d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['know', 'right', 'I', 'always', 'love', 'you', 'a', 'lot']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09f6149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.795279274196275e-05"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.36787944117144233 * 7.598356856515924e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "937be696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.670124608961276e-07"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5488116360940264 * 6.687403049764207e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c143591a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
