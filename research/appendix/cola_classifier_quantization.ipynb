{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4f1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4088c0dd",
   "metadata": {},
   "source": [
    "### Dataset 및 DataLoader 생성"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44d29047",
   "metadata": {},
   "source": [
    "다운로드된 데이터를 이용해서 CoLADataset을 생성 후 DataLoader를 생성하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4894e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반드시 do_lower_case=True로 해야 한다.\n",
    "# bert-base-uncased는 영어 데이터를 소문자로 변환해서 학습한 모델이기 때문이다.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fadee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoLADataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, is_train=True, is_inference=False):\n",
    "        '''\n",
    "        path: CoLA 데이터셋 위치\n",
    "        tokenizer: CoLA 데이터셋을 토크나이징할 토크나이저, ex) BertTokenizer\n",
    "        is_train: CoLADataset을 정의하는 목적이 모델 학습용일 경우 True, 그렇지 않으면 False\n",
    "        is_inference: CoLADataset을 정의하는 목적이 인퍼런스용일 경우 True, 그렇지 않으면 False\n",
    "        '''\n",
    "        \n",
    "        if is_train:\n",
    "            filename = os.path.join(path, 'raw/in_domain_train.tsv')\n",
    "        else:\n",
    "            if is_inference:\n",
    "                filename = os.path.join(path, 'raw/out_of_domain_dev.tsv')\n",
    "            else:\n",
    "                filename = os.path.join(path, 'raw/in_domain_dev.tsv')\n",
    "        df = pd.read_csv(filename, sep='\\t', names=['source', 'label', 'judgement', 'text'])\n",
    "        self.input_ids = []\n",
    "        self.token_type_ids = []\n",
    "        self.attention_mask = []\n",
    "        for t in df.text:\n",
    "            inp = tokenizer(t, return_tensors='pt')\n",
    "            self.input_ids.append(inp['input_ids'])\n",
    "            self.token_type_ids.append(inp['token_type_ids'])\n",
    "            self.attention_mask.append(inp['attention_mask'])\n",
    "        self.label = df.label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.input_ids[idx], self.token_type_ids[idx], self.attention_mask[idx], self.label[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdefcbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = CoLADataset('../../data/cola_classification', tokenizer, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf7a46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6e7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [b[0][0] for b in batch]\n",
    "    token_type_ids = [b[1][0] for b in batch]\n",
    "    attention_mask = [b[2][0] for b in batch]\n",
    "    label = torch.tensor([b[3] for b in batch])\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    token_type_ids = pad_sequence(token_type_ids, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "    return input_ids, token_type_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7142daf",
   "metadata": {},
   "source": [
    "### quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61db375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51357638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습한 모델 로딩\n",
    "model.load_state_dict(torch.load('cola_model.bin', map_location='cpu'))\n",
    "#model.load_state_dict(torch.load('cola_model_no_pretrained.bin', map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bf576b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96188852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70508ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32b75980",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093833c",
   "metadata": {},
   "source": [
    "학습한 모델을 로딩해서 Inference하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "445cd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80cea667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트를 위한 CoLA 데이터셋 로딩 및 DataLoader 클래스 생성\n",
    "test_dataset = CoLADataset('../../data/cola_classification', tokenizer, is_train=False, is_inference=True)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "542c36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy_and_latency(model, dataloader):\n",
    "    tbar = tqdm(dataloader, desc='Inference', leave=True)\n",
    "    \n",
    "    label_list = []\n",
    "    pred_list = []\n",
    "    start = time.time()\n",
    "    for i, d in enumerate(tbar):\n",
    "        input_ids, token_type_ids, attention_mask, labels = d\n",
    "                \n",
    "        # do inference\n",
    "        pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pred = pred[0].argmax(dim=1)\n",
    "        \n",
    "        label_list.extend(labels.cpu().data.numpy())\n",
    "        pred_list.extend(pred.cpu().data.numpy())\n",
    "    end = time.time()\n",
    "\n",
    "    labels = np.array(label_list)\n",
    "    preds = np.array(pred_list)\n",
    "    \n",
    "    acc = (labels == preds).mean()\n",
    "    latency = (end - start) / len(labels)\n",
    "    \n",
    "    print(f'acc={acc:.3f} latency={latency:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bbdcc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 17/17 [00:02<00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.816 latency=0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "measure_accuracy_and_latency(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a3c36bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 17/17 [00:01<00:00, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.814 latency=0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "measure_accuracy_and_latency(quantized_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc6210d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(quantized_model.state_dict(), 'cola_model_quantized.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6760c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 jkfirst jkfirst 418M  6월 23 04:37 cola_model.bin\r\n",
      "-rw-rw-r-- 1 jkfirst jkfirst 174M  6월 28 19:19 cola_model_quantized.bin\r\n"
     ]
    }
   ],
   "source": [
    "ls -alh *.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f321643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1beec368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87fc6fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model.bert.encoder.layer[0].attention.self.query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa68746",
   "metadata": {},
   "source": [
    "### layer fusion"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21beb1e8",
   "metadata": {},
   "source": [
    "# https://nenadmarkus.com/p/fusing-batchnorm-and-conv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14b01cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.random.random((16, 512, 768))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ea4aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertAttention(\n",
       "  (self): BertSelfAttention(\n",
       "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output): BertSelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.encoder.layer[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d094687",
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose_fn = model.bert.encoder.layer[0].attention.self.transpose_for_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "219ba6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 768]) torch.Size([16, 512, 768]) torch.Size([16, 512, 768])\n",
      "torch.Size([16, 12, 512, 64]) torch.Size([16, 12, 64, 512]) torch.Size([16, 12, 512, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_query_layer = model.bert.encoder.layer[0].attention.self.query(x)\n",
    "query_layer = model.bert.encoder.layer[0].attention.self.transpose_for_scores(mixed_query_layer)\n",
    "mixed_key_layer = model.bert.encoder.layer[0].attention.self.key(x)\n",
    "key_layer = model.bert.encoder.layer[0].attention.self.transpose_for_scores(mixed_key_layer)\n",
    "mixed_value_layer = model.bert.encoder.layer[0].attention.self.value(x)\n",
    "value_layer = model.bert.encoder.layer[0].attention.self.transpose_for_scores(mixed_value_layer)\n",
    "print(mixed_query_layer.shape, mixed_value_layer.shape, mixed_key_layer.shape)\n",
    "\n",
    "attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "print(query_layer.shape, key_layer.transpose(-1, -2).shape, value_layer.shape)\n",
    "\n",
    "context_layer = torch.matmul(attention_probs, value_layer)\n",
    "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (768,)\n",
    "context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "context_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e59116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_layer_fusion(attention_module, T):\n",
    "    wq = attention_module.self.query.weight\n",
    "    bq = attention_module.self.query.bias\n",
    "    wk = attention_module.self.key.weight\n",
    "    bk = attention_module.self.key.bias\n",
    "    wv = attention_module.self.value.weight\n",
    "    bv = attention_module.self.value.bias\n",
    "    \n",
    "    mixed_query_layer = torch.matmul(x, wq.T) + bq\n",
    "    mixed_key_layer = torch.matmul(x, wk.T) + bk\n",
    "    mixed_value_layer = torch.matmul(x, wv.T) + bv\n",
    "    for i in range(12):\n",
    "        a = mixed_query_layer[:,:,i*64:(i+1)*64]\n",
    "        b = mixed_key_layer[:,:,i*64:(i+1)*64]\n",
    "        o = torch.matmul(a, b.transpose(-1, -2))\n",
    "        \n",
    "    print(o[0][0])\n",
    "        \n",
    "    query_layer = T(torch.matmul(x, wq.T) + bq)\n",
    "    key_layer = T(torch.matmul(x, wk.T) + bk)\n",
    "    value_layer = T(torch.matmul(x, wv.T) + bv)\n",
    "    \n",
    "    # attention_scores, attention_probs -> (16, 12, 512, 512)\n",
    "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "    attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "    print(attention_scores[:,-1,:,:][0][0])\n",
    "    \n",
    "    context_layer = torch.matmul(attention_probs, value_layer)\n",
    "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    new_context_layer_shape = context_layer.size()[:-2] + (768,)\n",
    "    context_layer = context_layer.view(new_context_layer_shape)\n",
    "    \n",
    "    return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e26a929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.9015e+00,  2.5915e+00,  4.4448e+00, -2.3938e+00,  1.2839e+00,\n",
      "         2.8127e+00,  1.4193e+00,  1.9148e-02,  2.6554e+00, -7.0722e-01,\n",
      "         1.6721e+00,  7.4494e-02,  5.8034e+00, -1.1887e+00,  8.1230e-01,\n",
      "         1.2581e+00, -3.7323e-01,  4.3691e+00,  1.2334e+00,  1.4726e+00,\n",
      "        -2.7465e+00,  6.5842e-01,  1.9186e+00, -3.3944e+00,  9.1420e-01,\n",
      "         3.0800e+00, -2.6303e+00,  1.6961e+00, -1.7202e+00, -6.3358e-01,\n",
      "        -2.1531e+00, -1.3558e+00, -1.7010e-01,  3.5539e+00,  3.3828e+00,\n",
      "         2.7084e+00, -2.8276e-01, -2.4939e-01,  5.2201e-01, -1.6801e+00,\n",
      "         5.6782e-01,  2.4558e+00, -5.3414e-02,  2.8917e+00, -3.2387e-01,\n",
      "         2.6608e+00, -1.1062e-01, -7.4396e-01,  2.6550e+00,  3.9978e+00,\n",
      "        -6.5622e-01,  1.6588e-01,  1.3661e-01, -1.0841e+00,  3.1562e+00,\n",
      "         4.4794e-01,  2.6865e+00,  1.9943e+00,  4.8848e+00,  1.7926e+00,\n",
      "        -4.0432e+00,  7.0576e-01,  5.1248e-01,  3.7472e+00,  2.3168e+00,\n",
      "        -8.2729e-01, -1.0973e+00,  1.7524e+00,  2.4863e+00,  3.5874e+00,\n",
      "        -2.7202e+00,  2.4366e+00,  6.1720e-01,  4.6975e-01, -8.5874e-01,\n",
      "        -1.4840e-01,  2.6641e+00,  1.9271e+00, -9.4100e-01,  6.5234e-01,\n",
      "        -4.5619e-01,  1.8005e+00, -3.9994e-02,  3.6470e+00, -3.1512e+00,\n",
      "        -1.5550e-01, -2.4777e-01,  3.2831e-01, -2.2653e-01, -6.1657e-01,\n",
      "         1.1360e+00, -3.7920e-01,  1.1877e+00,  1.6929e+00,  1.3023e+00,\n",
      "         7.5777e-01,  6.7431e-01,  8.8676e-01,  2.1842e+00, -5.0902e-02,\n",
      "         3.7978e-01,  3.2956e-01,  8.1923e-01,  7.1351e-01,  5.7904e-01,\n",
      "        -4.4936e+00,  2.7269e+00,  7.5901e-01,  1.4076e-01,  1.7087e+00,\n",
      "         1.3733e+00,  5.8202e-01,  2.8435e+00, -2.0654e+00,  1.6853e+00,\n",
      "        -3.2375e-01,  1.4613e-02, -1.4843e-01, -1.6938e+00, -1.4677e+00,\n",
      "        -2.1051e-01,  1.9620e+00, -1.0749e-01,  2.7897e-01,  9.2975e-01,\n",
      "         2.5261e-01,  1.2793e+00,  6.7155e-01, -7.5788e-01,  3.1756e+00,\n",
      "         9.3277e-01,  1.1479e+00,  1.0597e+00,  1.4736e+00,  7.1885e-03,\n",
      "        -1.3482e+00,  1.3707e-01,  1.5511e+00,  3.1028e+00,  7.2380e-01,\n",
      "         2.4459e-01, -2.7335e-01,  1.6626e-01,  6.6871e-01,  1.7141e+00,\n",
      "        -3.0584e+00,  8.6403e-01, -1.2354e+00,  1.1409e+00,  1.3246e+00,\n",
      "         8.8067e-01, -2.5817e+00,  1.3246e+00, -1.9338e+00,  1.0732e+00,\n",
      "         1.0596e+00, -2.4729e+00,  4.5004e+00, -1.9038e-02, -1.0727e+00,\n",
      "        -1.7179e+00,  5.4388e+00,  1.1188e+00,  2.8865e+00,  1.5982e+00,\n",
      "         2.8189e+00,  2.6839e+00,  2.1178e+00, -2.2059e-02,  1.8156e+00,\n",
      "        -2.6288e+00,  1.0355e+00,  2.0677e+00, -1.8057e+00,  3.2271e+00,\n",
      "        -5.0793e-01,  2.4679e+00,  4.9541e-01,  2.0027e+00,  1.7999e+00,\n",
      "         3.9356e-01,  8.8143e-01,  1.7152e+00,  3.4479e+00,  1.8727e+00,\n",
      "         3.1480e+00,  4.7006e+00,  2.9735e+00, -2.1247e-01, -1.1487e+00,\n",
      "         2.1949e+00,  6.9279e-01,  6.1181e-01,  3.0886e-01,  1.2292e+00,\n",
      "         2.4526e+00, -8.9642e-01,  3.7261e+00,  9.4886e-02,  1.1056e+00,\n",
      "         1.1601e+00,  1.1988e+00,  8.6891e-01,  2.3637e+00,  4.1854e-01,\n",
      "         3.4437e+00,  1.5998e+00,  1.1041e+00,  9.3390e-01,  5.4737e-01,\n",
      "        -1.8922e-01, -8.7334e-01, -1.2990e+00,  2.6148e+00, -2.0336e+00,\n",
      "        -3.1624e+00,  1.3571e+00, -1.3554e+00, -5.0575e-01, -1.3742e-01,\n",
      "        -1.2096e+00,  2.1549e+00, -5.3920e-01,  2.7903e+00, -1.2387e+00,\n",
      "         3.3449e+00, -2.4277e-01,  2.7639e-01, -1.2179e-01,  4.8914e-01,\n",
      "        -4.3988e-01, -1.1439e+00,  8.6185e-01, -1.3643e+00,  3.8945e-01,\n",
      "        -2.6777e-03, -3.2667e+00,  6.1254e-02,  1.2382e+00,  2.3460e+00,\n",
      "         1.0472e+00,  2.2936e+00,  2.7296e+00, -3.0209e+00, -1.6931e+00,\n",
      "         2.0845e+00,  3.6420e+00,  3.7272e+00,  2.6314e+00,  1.8310e+00,\n",
      "        -2.0482e+00, -9.0173e-01,  5.6407e-02, -2.5671e+00,  4.1859e+00,\n",
      "         1.4946e+00,  1.5085e+00,  1.6165e+00,  1.8128e+00,  2.8203e+00,\n",
      "         3.2998e+00,  2.2119e+00,  3.8792e-01,  2.4884e+00, -2.9813e-02,\n",
      "         1.6506e+00,  2.9560e+00,  1.3980e+00,  2.4953e+00,  3.4326e+00,\n",
      "        -1.2045e+00, -3.0822e-01,  4.8203e+00,  1.0344e+00,  2.2946e+00,\n",
      "        -8.0336e-02,  1.2183e+00,  1.8857e+00,  1.9696e+00,  6.8284e-01,\n",
      "         2.6577e+00, -2.5654e+00, -7.1918e-01,  1.4259e+00,  1.6865e+00,\n",
      "         2.0490e-01, -7.3784e-02, -3.1577e-01, -1.7765e+00,  2.6777e+00,\n",
      "         6.7186e-02, -1.5513e-01,  8.5747e-01, -1.4994e+00,  2.1389e+00,\n",
      "         9.1999e-01, -1.0979e+00,  8.8728e-01,  2.2286e+00,  6.4904e-01,\n",
      "        -3.1901e+00, -2.6238e+00,  4.9506e-01,  1.8827e-01, -1.9029e+00,\n",
      "         2.5623e+00, -3.4619e-01,  3.8807e+00,  1.1962e+00,  8.1130e-01,\n",
      "         1.3578e+00,  4.3955e+00,  1.0236e+00, -3.7842e+00,  4.1425e-01,\n",
      "        -1.0207e+00,  9.4115e-01, -3.1115e+00, -5.4361e+00, -6.7349e-01,\n",
      "        -2.8298e+00,  1.9901e+00,  2.7307e-02,  3.2842e-01, -2.7122e+00,\n",
      "         1.4051e+00, -1.9318e+00, -5.8290e-01, -2.1507e+00, -1.9062e+00,\n",
      "        -1.0003e+00,  4.2887e+00,  1.9302e+00, -3.7081e-01, -2.7921e+00,\n",
      "         1.2316e+00,  2.4355e+00,  1.3830e+00,  1.9717e+00,  2.9755e-01,\n",
      "        -2.2366e+00, -3.5027e+00,  1.5481e+00, -8.5316e-01,  1.9954e+00,\n",
      "         3.0046e+00,  1.8331e+00,  2.0532e+00,  2.8361e+00,  2.3384e-01,\n",
      "         6.7998e-01, -2.6026e-01,  2.5836e+00, -1.0896e-01, -8.5179e-01,\n",
      "         5.1182e-01,  1.6711e+00,  5.9605e-01, -1.2702e+00,  1.2257e+00,\n",
      "        -2.2195e+00,  7.0502e-01, -1.5740e+00,  1.5032e+00,  1.9484e+00,\n",
      "        -1.4350e-01, -1.1065e-01,  2.4369e+00,  1.8604e+00, -5.0213e-01,\n",
      "        -2.0307e+00,  3.0204e+00,  3.7943e+00,  3.8842e+00,  3.0295e+00,\n",
      "        -9.4835e-02, -1.0770e+00, -1.7241e-01,  4.1201e+00, -1.7264e+00,\n",
      "         2.0990e+00, -1.0024e+00,  1.1259e+00,  2.0924e+00,  7.8541e-01,\n",
      "         8.1272e-01,  1.9566e+00,  2.7054e+00,  1.4445e+00,  2.3970e+00,\n",
      "        -4.3491e-01,  8.1596e-01,  2.6266e+00,  4.7706e-01,  1.4694e+00,\n",
      "         3.1230e+00, -4.4663e-01,  2.3645e+00, -1.0895e+00,  5.4484e-01,\n",
      "         3.9707e+00,  3.2973e+00, -1.4825e+00,  1.4740e+00,  2.5443e-01,\n",
      "         8.2431e-01,  3.2870e+00, -2.0827e+00,  5.9441e-01,  8.7299e-01,\n",
      "        -1.3391e+00,  8.6039e-01, -8.1733e-01,  3.6144e+00, -9.5235e-01,\n",
      "         2.2620e+00,  3.4820e+00,  4.0292e+00,  7.1422e-01, -5.6071e-01,\n",
      "         3.8893e-01, -1.7570e+00, -1.4311e+00,  2.5505e+00,  2.9749e-02,\n",
      "         1.0470e+00,  2.7915e+00,  1.5672e-01,  2.4721e+00,  7.1245e-01,\n",
      "        -7.6079e-02,  2.5392e-01,  3.3562e+00,  1.0364e+00,  3.5265e-01,\n",
      "         5.7076e+00,  9.1349e-01,  3.7005e+00,  1.9016e+00,  1.8241e+00,\n",
      "         1.3489e+00,  1.1386e+00, -1.6424e+00, -1.0700e-02, -9.9198e-02,\n",
      "         2.5508e+00,  5.1273e+00,  7.8012e-01, -2.2977e+00, -2.5189e+00,\n",
      "        -3.4345e+00,  1.9032e-01, -5.5734e-01, -9.6220e-01,  4.9695e-01,\n",
      "         4.4155e-01,  2.2022e+00,  2.0829e+00,  1.5428e+00, -6.5449e-01,\n",
      "         1.9496e+00, -2.4140e+00,  1.5312e+00,  1.8628e+00,  1.3894e-02,\n",
      "        -2.0420e+00,  1.9140e+00,  7.5902e-01,  1.5067e+00, -5.0362e-02,\n",
      "        -1.3456e+00,  2.0191e+00, -1.2014e+00,  2.3162e+00,  1.6438e+00,\n",
      "         1.7843e+00,  5.6795e-01, -1.2019e+00,  4.0422e+00,  2.3638e+00,\n",
      "        -8.8982e-02,  3.7720e+00,  2.8494e+00,  2.1459e+00,  1.6433e+00,\n",
      "         2.1874e+00, -1.9589e+00,  9.1345e-01,  1.5979e+00, -1.4973e+00,\n",
      "         9.6804e-01,  1.9118e+00, -1.7994e+00,  5.5534e+00,  6.0979e-01,\n",
      "        -3.1300e-01, -1.4151e+00,  1.3166e+00,  1.5342e-01,  1.4101e+00,\n",
      "        -2.8034e+00,  8.0429e-01,  2.0923e+00,  1.9035e+00, -2.6158e-01,\n",
      "        -6.0821e-02,  2.6627e+00, -2.2140e+00, -2.9302e+00,  2.0632e+00,\n",
      "         5.6686e-02, -1.5811e+00], grad_fn=<SelectBackward0>)\n",
      "tensor([-1.9015e+00,  2.5915e+00,  4.4448e+00, -2.3938e+00,  1.2839e+00,\n",
      "         2.8127e+00,  1.4193e+00,  1.9148e-02,  2.6554e+00, -7.0722e-01,\n",
      "         1.6721e+00,  7.4494e-02,  5.8034e+00, -1.1887e+00,  8.1230e-01,\n",
      "         1.2581e+00, -3.7323e-01,  4.3691e+00,  1.2334e+00,  1.4726e+00,\n",
      "        -2.7465e+00,  6.5842e-01,  1.9186e+00, -3.3944e+00,  9.1420e-01,\n",
      "         3.0800e+00, -2.6303e+00,  1.6961e+00, -1.7202e+00, -6.3358e-01,\n",
      "        -2.1531e+00, -1.3558e+00, -1.7010e-01,  3.5539e+00,  3.3828e+00,\n",
      "         2.7084e+00, -2.8276e-01, -2.4939e-01,  5.2201e-01, -1.6801e+00,\n",
      "         5.6782e-01,  2.4558e+00, -5.3414e-02,  2.8917e+00, -3.2387e-01,\n",
      "         2.6608e+00, -1.1062e-01, -7.4396e-01,  2.6550e+00,  3.9978e+00,\n",
      "        -6.5622e-01,  1.6588e-01,  1.3661e-01, -1.0841e+00,  3.1562e+00,\n",
      "         4.4794e-01,  2.6865e+00,  1.9943e+00,  4.8848e+00,  1.7926e+00,\n",
      "        -4.0432e+00,  7.0576e-01,  5.1248e-01,  3.7472e+00,  2.3168e+00,\n",
      "        -8.2729e-01, -1.0973e+00,  1.7524e+00,  2.4863e+00,  3.5874e+00,\n",
      "        -2.7202e+00,  2.4366e+00,  6.1720e-01,  4.6975e-01, -8.5874e-01,\n",
      "        -1.4840e-01,  2.6641e+00,  1.9271e+00, -9.4100e-01,  6.5234e-01,\n",
      "        -4.5619e-01,  1.8005e+00, -3.9994e-02,  3.6470e+00, -3.1512e+00,\n",
      "        -1.5550e-01, -2.4777e-01,  3.2831e-01, -2.2653e-01, -6.1657e-01,\n",
      "         1.1360e+00, -3.7920e-01,  1.1877e+00,  1.6929e+00,  1.3023e+00,\n",
      "         7.5777e-01,  6.7431e-01,  8.8676e-01,  2.1842e+00, -5.0902e-02,\n",
      "         3.7978e-01,  3.2956e-01,  8.1923e-01,  7.1351e-01,  5.7904e-01,\n",
      "        -4.4936e+00,  2.7269e+00,  7.5901e-01,  1.4076e-01,  1.7087e+00,\n",
      "         1.3733e+00,  5.8202e-01,  2.8435e+00, -2.0654e+00,  1.6853e+00,\n",
      "        -3.2375e-01,  1.4613e-02, -1.4843e-01, -1.6938e+00, -1.4677e+00,\n",
      "        -2.1051e-01,  1.9620e+00, -1.0749e-01,  2.7897e-01,  9.2975e-01,\n",
      "         2.5261e-01,  1.2793e+00,  6.7155e-01, -7.5788e-01,  3.1756e+00,\n",
      "         9.3277e-01,  1.1479e+00,  1.0597e+00,  1.4736e+00,  7.1885e-03,\n",
      "        -1.3482e+00,  1.3707e-01,  1.5511e+00,  3.1028e+00,  7.2380e-01,\n",
      "         2.4459e-01, -2.7335e-01,  1.6626e-01,  6.6871e-01,  1.7141e+00,\n",
      "        -3.0584e+00,  8.6403e-01, -1.2354e+00,  1.1409e+00,  1.3246e+00,\n",
      "         8.8067e-01, -2.5817e+00,  1.3246e+00, -1.9338e+00,  1.0732e+00,\n",
      "         1.0596e+00, -2.4729e+00,  4.5004e+00, -1.9038e-02, -1.0727e+00,\n",
      "        -1.7179e+00,  5.4388e+00,  1.1188e+00,  2.8865e+00,  1.5982e+00,\n",
      "         2.8189e+00,  2.6839e+00,  2.1178e+00, -2.2059e-02,  1.8156e+00,\n",
      "        -2.6288e+00,  1.0355e+00,  2.0677e+00, -1.8057e+00,  3.2271e+00,\n",
      "        -5.0793e-01,  2.4679e+00,  4.9541e-01,  2.0027e+00,  1.7999e+00,\n",
      "         3.9356e-01,  8.8143e-01,  1.7152e+00,  3.4479e+00,  1.8727e+00,\n",
      "         3.1480e+00,  4.7006e+00,  2.9735e+00, -2.1247e-01, -1.1487e+00,\n",
      "         2.1949e+00,  6.9279e-01,  6.1181e-01,  3.0886e-01,  1.2292e+00,\n",
      "         2.4526e+00, -8.9642e-01,  3.7261e+00,  9.4886e-02,  1.1056e+00,\n",
      "         1.1601e+00,  1.1988e+00,  8.6891e-01,  2.3637e+00,  4.1854e-01,\n",
      "         3.4437e+00,  1.5998e+00,  1.1041e+00,  9.3390e-01,  5.4737e-01,\n",
      "        -1.8922e-01, -8.7334e-01, -1.2990e+00,  2.6148e+00, -2.0336e+00,\n",
      "        -3.1624e+00,  1.3571e+00, -1.3554e+00, -5.0575e-01, -1.3742e-01,\n",
      "        -1.2096e+00,  2.1549e+00, -5.3920e-01,  2.7903e+00, -1.2387e+00,\n",
      "         3.3449e+00, -2.4277e-01,  2.7639e-01, -1.2179e-01,  4.8914e-01,\n",
      "        -4.3988e-01, -1.1439e+00,  8.6185e-01, -1.3643e+00,  3.8945e-01,\n",
      "        -2.6777e-03, -3.2667e+00,  6.1254e-02,  1.2382e+00,  2.3460e+00,\n",
      "         1.0472e+00,  2.2936e+00,  2.7296e+00, -3.0209e+00, -1.6931e+00,\n",
      "         2.0845e+00,  3.6420e+00,  3.7272e+00,  2.6314e+00,  1.8310e+00,\n",
      "        -2.0482e+00, -9.0173e-01,  5.6407e-02, -2.5671e+00,  4.1859e+00,\n",
      "         1.4946e+00,  1.5085e+00,  1.6165e+00,  1.8128e+00,  2.8203e+00,\n",
      "         3.2998e+00,  2.2119e+00,  3.8792e-01,  2.4884e+00, -2.9813e-02,\n",
      "         1.6506e+00,  2.9560e+00,  1.3980e+00,  2.4953e+00,  3.4326e+00,\n",
      "        -1.2045e+00, -3.0822e-01,  4.8203e+00,  1.0344e+00,  2.2946e+00,\n",
      "        -8.0336e-02,  1.2183e+00,  1.8857e+00,  1.9696e+00,  6.8284e-01,\n",
      "         2.6577e+00, -2.5654e+00, -7.1918e-01,  1.4259e+00,  1.6865e+00,\n",
      "         2.0490e-01, -7.3784e-02, -3.1577e-01, -1.7765e+00,  2.6777e+00,\n",
      "         6.7186e-02, -1.5513e-01,  8.5747e-01, -1.4994e+00,  2.1389e+00,\n",
      "         9.1999e-01, -1.0979e+00,  8.8728e-01,  2.2286e+00,  6.4904e-01,\n",
      "        -3.1901e+00, -2.6238e+00,  4.9506e-01,  1.8827e-01, -1.9029e+00,\n",
      "         2.5623e+00, -3.4619e-01,  3.8807e+00,  1.1962e+00,  8.1130e-01,\n",
      "         1.3578e+00,  4.3955e+00,  1.0236e+00, -3.7842e+00,  4.1425e-01,\n",
      "        -1.0207e+00,  9.4115e-01, -3.1115e+00, -5.4361e+00, -6.7349e-01,\n",
      "        -2.8298e+00,  1.9901e+00,  2.7307e-02,  3.2842e-01, -2.7122e+00,\n",
      "         1.4051e+00, -1.9318e+00, -5.8290e-01, -2.1507e+00, -1.9062e+00,\n",
      "        -1.0003e+00,  4.2887e+00,  1.9302e+00, -3.7081e-01, -2.7921e+00,\n",
      "         1.2316e+00,  2.4355e+00,  1.3830e+00,  1.9717e+00,  2.9755e-01,\n",
      "        -2.2366e+00, -3.5027e+00,  1.5481e+00, -8.5316e-01,  1.9954e+00,\n",
      "         3.0046e+00,  1.8331e+00,  2.0532e+00,  2.8361e+00,  2.3384e-01,\n",
      "         6.7998e-01, -2.6026e-01,  2.5836e+00, -1.0896e-01, -8.5179e-01,\n",
      "         5.1182e-01,  1.6711e+00,  5.9605e-01, -1.2702e+00,  1.2257e+00,\n",
      "        -2.2195e+00,  7.0502e-01, -1.5740e+00,  1.5032e+00,  1.9484e+00,\n",
      "        -1.4350e-01, -1.1065e-01,  2.4369e+00,  1.8604e+00, -5.0213e-01,\n",
      "        -2.0307e+00,  3.0204e+00,  3.7943e+00,  3.8842e+00,  3.0295e+00,\n",
      "        -9.4835e-02, -1.0770e+00, -1.7241e-01,  4.1201e+00, -1.7264e+00,\n",
      "         2.0990e+00, -1.0024e+00,  1.1259e+00,  2.0924e+00,  7.8541e-01,\n",
      "         8.1272e-01,  1.9566e+00,  2.7054e+00,  1.4445e+00,  2.3970e+00,\n",
      "        -4.3491e-01,  8.1596e-01,  2.6266e+00,  4.7706e-01,  1.4694e+00,\n",
      "         3.1230e+00, -4.4663e-01,  2.3645e+00, -1.0895e+00,  5.4484e-01,\n",
      "         3.9707e+00,  3.2973e+00, -1.4825e+00,  1.4740e+00,  2.5443e-01,\n",
      "         8.2431e-01,  3.2870e+00, -2.0827e+00,  5.9441e-01,  8.7299e-01,\n",
      "        -1.3391e+00,  8.6039e-01, -8.1733e-01,  3.6144e+00, -9.5235e-01,\n",
      "         2.2620e+00,  3.4820e+00,  4.0292e+00,  7.1422e-01, -5.6071e-01,\n",
      "         3.8893e-01, -1.7570e+00, -1.4311e+00,  2.5505e+00,  2.9749e-02,\n",
      "         1.0470e+00,  2.7915e+00,  1.5672e-01,  2.4721e+00,  7.1245e-01,\n",
      "        -7.6079e-02,  2.5392e-01,  3.3562e+00,  1.0364e+00,  3.5265e-01,\n",
      "         5.7076e+00,  9.1349e-01,  3.7005e+00,  1.9016e+00,  1.8241e+00,\n",
      "         1.3489e+00,  1.1386e+00, -1.6424e+00, -1.0700e-02, -9.9198e-02,\n",
      "         2.5508e+00,  5.1273e+00,  7.8012e-01, -2.2977e+00, -2.5189e+00,\n",
      "        -3.4345e+00,  1.9032e-01, -5.5734e-01, -9.6220e-01,  4.9695e-01,\n",
      "         4.4155e-01,  2.2022e+00,  2.0829e+00,  1.5428e+00, -6.5449e-01,\n",
      "         1.9496e+00, -2.4140e+00,  1.5312e+00,  1.8628e+00,  1.3894e-02,\n",
      "        -2.0420e+00,  1.9140e+00,  7.5902e-01,  1.5067e+00, -5.0362e-02,\n",
      "        -1.3456e+00,  2.0191e+00, -1.2014e+00,  2.3162e+00,  1.6438e+00,\n",
      "         1.7843e+00,  5.6795e-01, -1.2019e+00,  4.0422e+00,  2.3638e+00,\n",
      "        -8.8982e-02,  3.7720e+00,  2.8494e+00,  2.1459e+00,  1.6433e+00,\n",
      "         2.1874e+00, -1.9589e+00,  9.1345e-01,  1.5979e+00, -1.4973e+00,\n",
      "         9.6804e-01,  1.9118e+00, -1.7994e+00,  5.5534e+00,  6.0979e-01,\n",
      "        -3.1300e-01, -1.4151e+00,  1.3166e+00,  1.5342e-01,  1.4101e+00,\n",
      "        -2.8034e+00,  8.0429e-01,  2.0923e+00,  1.9035e+00, -2.6158e-01,\n",
      "        -6.0821e-02,  2.6627e+00, -2.2140e+00, -2.9302e+00,  2.0632e+00,\n",
      "         5.6686e-02, -1.5811e+00], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "o = bert_layer_fusion(model.bert.encoder.layer[0].attention, transpose_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d22b8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 768]), torch.Size([768]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq = model.bert.encoder.layer[0].attention.self.query.weight\n",
    "bq = model.bert.encoder.layer[0].attention.self.query.bias\n",
    "wq.shape, bq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a4f39f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 768]), torch.Size([768]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wk = model.bert.encoder.layer[0].attention.self.key.weight\n",
    "bk = model.bert.encoder.layer[0].attention.self.key.bias\n",
    "wk.shape, bk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81639141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 768]), torch.Size([768]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = model.bert.encoder.layer[0].attention.self.value.weight\n",
    "bv = model.bert.encoder.layer[0].attention.self.value.bias\n",
    "wv.shape, bv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e1ff0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4820, -0.4495, -0.0934,  ...,  0.3367, -0.3739, -0.6107],\n",
       "        [ 0.9407, -0.3381, -0.4547,  ...,  0.2770, -0.3883,  0.5439],\n",
       "        [ 0.9656, -0.5552, -0.3678,  ...,  0.4237,  0.5679, -0.4176],\n",
       "        ...,\n",
       "        [ 0.9892,  0.2435, -0.2735,  ...,  0.4637, -0.9741,  0.5602],\n",
       "        [ 0.9645, -0.8999, -0.3634,  ...,  0.7425, -0.7519,  0.0691],\n",
       "        [ 1.2305, -0.4847, -0.1822,  ...,  0.4492,  0.2554,  0.2505]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_query_layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b09d8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_query_layer -> torch.matmul(x, wq.T) + bq\n",
    "# mixed_key_layer -> torch.matmul(x, wk.T) + bk\n",
    "# mixed_value_layer -> torch.matmul(x, wv.T) + bv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a50e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_mixed_query_layer = torch.matmul(x, wq.T) + bq\n",
    "_mixed_key_layer = torch.matmul(x, wk.T) + bk\n",
    "_mixed_value_layer = torch.matmul(x, wv.T) + bv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7cedd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_query_layer = transpose_fn(_mixed_query_layer)\n",
    "_key_layer = transpose_fn(_mixed_key_layer)\n",
    "_value_layer = transpose_fn(_mixed_value_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40df6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_attention_scores = torch.matmul(_query_layer, _key_layer.transpose(-1, -2))\n",
    "_attention_probs = nn.functional.softmax(_attention_scores, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b32a1e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3736e-03, 4.9345e-03, 5.3059e-04,  ..., 5.7736e-06, 3.6539e-03,\n",
       "         5.1676e-04],\n",
       "        [4.5039e-03, 3.3418e-04, 1.6283e-04,  ..., 2.8543e-06, 3.2148e-04,\n",
       "         1.6461e-04],\n",
       "        [1.5002e-02, 2.9829e-03, 2.2461e-04,  ..., 5.4842e-06, 6.1736e-04,\n",
       "         3.3424e-04],\n",
       "        ...,\n",
       "        [4.5424e-03, 3.2775e-03, 2.4234e-04,  ..., 1.5969e-05, 2.7897e-03,\n",
       "         4.0480e-04],\n",
       "        [5.9863e-03, 1.9494e-03, 9.0838e-04,  ..., 4.4690e-05, 8.4949e-04,\n",
       "         4.9135e-04],\n",
       "        [1.1367e-03, 1.3965e-03, 5.6448e-04,  ..., 7.6649e-05, 2.4117e-03,\n",
       "         1.4031e-04]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attention_probs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39f93c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3736e-03, 4.9345e-03, 5.3059e-04,  ..., 5.7736e-06, 3.6539e-03,\n",
       "         5.1676e-04],\n",
       "        [4.5039e-03, 3.3418e-04, 1.6283e-04,  ..., 2.8543e-06, 3.2148e-04,\n",
       "         1.6461e-04],\n",
       "        [1.5002e-02, 2.9829e-03, 2.2461e-04,  ..., 5.4842e-06, 6.1736e-04,\n",
       "         3.3424e-04],\n",
       "        ...,\n",
       "        [4.5424e-03, 3.2775e-03, 2.4234e-04,  ..., 1.5969e-05, 2.7897e-03,\n",
       "         4.0480e-04],\n",
       "        [5.9863e-03, 1.9494e-03, 9.0838e-04,  ..., 4.4690e-05, 8.4949e-04,\n",
       "         4.9135e-04],\n",
       "        [1.1367e-03, 1.3965e-03, 5.6448e-04,  ..., 7.6649e-05, 2.4117e-03,\n",
       "         1.4031e-04]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_probs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f607d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
