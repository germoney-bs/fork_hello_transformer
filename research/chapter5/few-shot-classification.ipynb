{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 파일의 소스코드는 아래의 레포지토리를 참고했다.\n",
    "\n",
    "#### Reference: https://github.com/zhongyuchen/few-shot-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import FewShotInduction\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from criterion import Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Amazon_few_shot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반드시 do_lower_case=True로 해야 한다.\n",
    "# bert-base-uncased는 영어 데이터를 소문자로 변환해서 학습한 모델이기 때문이다.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataset():\n",
    "    def __init__(self, data_path, tokenizer, dtype):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(f'{dtype}.list', 'r') as f:\n",
    "            self.categories = [oneline.rstrip() for oneline in f]\n",
    "        self.support_dataset = {}\n",
    "        self.dataset = {}\n",
    "        for category in tqdm(self.categories, desc='reading categories'):\n",
    "            self.dataset[category] = {\n",
    "                'neg': self.get_data(category, 'neg', dtype),\n",
    "                'pos': self.get_data(category, 'pos', dtype)\n",
    "            }\n",
    "        \n",
    "        if dtype == 'test' or dtype == 'dev':\n",
    "            for category in tqdm(self.categories, desc='reading categories for support'):\n",
    "                self.support_dataset[category] = {\n",
    "                    'neg': self.get_data(category, 'neg', 'train'),\n",
    "                    'pos': self.get_data(category, 'pos', 'train'),\n",
    "                }\n",
    "        \n",
    "    def read_files(self, category, label, dtype):\n",
    "        data = {\n",
    "            'text': [],\n",
    "            'label': []\n",
    "        }\n",
    "        for t in ['t2', 't4', 't5']:\n",
    "            filename = f'{category}.{t}.{dtype}'\n",
    "            with open(os.path.join(self.data_path, filename), 'r', encoding='utf-8') as f:\n",
    "                for oneline in f:\n",
    "                    oneline = oneline.rstrip()\n",
    "                    text = oneline[:-2]\n",
    "                    if int(oneline[-2:]) == 1 and label == 'pos':\n",
    "                        tensor = self.tokenizer(text, return_tensors='pt')\n",
    "                        data['text'].append(tensor['input_ids'][0])\n",
    "                        data['label'].append(1)\n",
    "                    elif int(oneline[-2:]) == -1 and label == 'neg':\n",
    "                        tensor = self.tokenizer(text, return_tensors='pt')\n",
    "                        data['text'].append(tensor['input_ids'][0])\n",
    "                        data['label'].append(0)\n",
    "        data['label'] = torch.tensor(data['label'])\n",
    "        return data\n",
    "    \n",
    "    def get_data(self, category, label, dtype):\n",
    "        data = self.read_files(category, label, dtype)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading categories:   0%|          | 0/14 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "reading categories: 100%|██████████| 14/14 [01:14<00:00,  5.35s/it]\n",
      "reading categories: 100%|██████████| 5/5 [00:00<00:00, 11.93it/s]\n",
      "reading categories for support: 100%|██████████| 5/5 [00:03<00:00,  1.43it/s]\n",
      "reading categories: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]\n",
      "reading categories for support: 100%|██████████| 4/4 [00:00<00:00, 36.36it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AmazonDataset(data_path, tokenizer, 'train')\n",
    "dev_dataset = AmazonDataset(data_path, tokenizer, 'dev')\n",
    "test_dataset = AmazonDataset(data_path, tokenizer, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(a_text, b_text):\n",
    "    a_text_len = a_text.shape[1]\n",
    "    b_text_len = b_text.shape[1]\n",
    "\n",
    "    if a_text_len > b_text_len:\n",
    "        b_text = torch.cat([b_text, torch.zeros(b_text.shape[0], a_text_len-b_text_len).long()], dim=1)\n",
    "    else:\n",
    "        a_text = torch.cat([a_text, torch.zeros(a_text.shape[0], b_text_len-a_text_len).long()], dim=1)\n",
    "        \n",
    "    return a_text, b_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataLoader():\n",
    "    def __init__(self, dataset, batch_size, n_support):\n",
    "        assert n_support % 2 == 0, 'n_support should be multiple of 2'\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.n_support = n_support\n",
    "        self.neg_idx = {k:0 for k in dataset.dataset}\n",
    "        self.pos_idx = {k:0 for k in dataset.dataset}\n",
    "        self.neg_len = {k:len(dataset.dataset[k]['neg']['text']) for k in dataset.dataset}\n",
    "        self.pos_len = {k:len(dataset.dataset[k]['pos']['text']) for k in dataset.dataset}\n",
    "        self.neg = {k:dataset.dataset[k]['neg'] for k in dataset.dataset}\n",
    "        self.pos = {k:dataset.dataset[k]['pos'] for k in dataset.dataset}\n",
    "        self.idx = 0\n",
    "        self.categories = [k for k in dataset.dataset]\n",
    "        \n",
    "        # prepare for test dataset, support dataset should come from \"*.train\"\n",
    "        self.neg_support_idx = {}\n",
    "        self.pos_support_idx = {}\n",
    "        self.neg_support_len = {}\n",
    "        self.pos_support_len = {}\n",
    "        if self.dataset.support_dataset:\n",
    "            self.neg_support_idx = {k:0 for k in self.dataset.support_dataset}\n",
    "            self.pos_support_idx = {k:0 for k in self.dataset.support_dataset}\n",
    "            self.neg_support_len = {k:len(self.dataset.support_dataset[k]['neg']['text']) for k in self.dataset.support_dataset}\n",
    "            self.pos_support_len = {k:len(self.dataset.support_dataset[k]['pos']['text']) for k in self.dataset.support_dataset}\n",
    "        \n",
    "    def get_batch(self):\n",
    "        category = self.categories[self.idx % len(self.categories)]\n",
    "        neg = self.neg[category]\n",
    "        pos = self.pos[category]\n",
    "        neg_start_idx = self.neg_idx[category] % self.neg_len[category]\n",
    "        pos_start_idx = self.pos_idx[category] % self.pos_len[category]\n",
    "        \n",
    "        # prepare negative/positive dataset\n",
    "        neg_text = neg['text'][neg_start_idx:neg_start_idx+(self.batch_size//2)]\n",
    "        pos_text = pos['text'][pos_start_idx:pos_start_idx+(self.batch_size//2)]\n",
    "        neg_label = neg['label'][neg_start_idx:neg_start_idx+(self.batch_size//2)]\n",
    "        pos_label = pos['label'][pos_start_idx:pos_start_idx+(self.batch_size//2)]\n",
    "        self.neg_idx[category] += (self.batch_size//2)\n",
    "        self.pos_idx[category] += (self.batch_size//2)\n",
    "        \n",
    "        if len(neg_text) + len(pos_text) != self.batch_size:\n",
    "            return self.get_batch()\n",
    "            \n",
    "        # padding text dataset\n",
    "        neg_text = pad_sequence([n for n in neg_text], batch_first=True)\n",
    "        pos_text = pad_sequence([p for p in pos_text], batch_first=True)\n",
    "        neg_text, pos_text = pad_text(neg_text, pos_text)\n",
    "            \n",
    "        # prepare support/query text\n",
    "        neg_support_text = neg_text[:self.n_support//2]\n",
    "        pos_support_text = pos_text[:self.n_support//2]\n",
    "        neg_query_text = neg_text[self.n_support//2:]\n",
    "        pos_query_text = pos_text[self.n_support//2:]\n",
    "        \n",
    "        # prepare support/query label\n",
    "        neg_support_label = neg_label[:self.n_support//2]\n",
    "        pos_support_label = pos_label[:self.n_support//2]\n",
    "        neg_query_label = neg_label[self.n_support//2:]\n",
    "        pos_query_label = pos_label[self.n_support//2:]\n",
    "        \n",
    "        # merge support/query text\n",
    "        support_text = torch.cat([neg_support_text, pos_support_text], dim=0)\n",
    "        query_text = torch.cat([neg_query_text, pos_query_text], dim=0)\n",
    "        \n",
    "        # merge support/query label\n",
    "        support_label = torch.cat([neg_support_label, pos_support_label], dim=0)\n",
    "        query_label = torch.cat([neg_query_label, pos_query_label], dim=0)\n",
    "        \n",
    "        # make data and label\n",
    "        data = torch.cat([support_text, query_text], dim=0)\n",
    "        label = torch.cat([support_label, query_label], dim=0)\n",
    "        \n",
    "        # increase category index\n",
    "        self.idx += 1\n",
    "        return data, label\n",
    "    \n",
    "    def get_batch_test(self):\n",
    "        assert self.dataset.support_dataset, 'support_dataset is empty'\n",
    "        \n",
    "        category = self.categories[self.idx % len(self.categories)]\n",
    "        neg = self.neg[category]\n",
    "        pos = self.pos[category]\n",
    "        neg_query_start_idx = self.neg_idx[category] % self.neg_len[category]\n",
    "        pos_query_start_idx = self.pos_idx[category] % self.pos_len[category]\n",
    "        neg_support_start_idx = self.neg_support_idx[category] % self.neg_support_len[category]\n",
    "        pos_support_start_idx = self.pos_support_idx[category] % self.pos_support_len[category]\n",
    "        \n",
    "        # prepare negative/positive support dataset from support_dataset\n",
    "        category_suuport_dataset = self.dataset.support_dataset[category]\n",
    "        neg_support_text = category_suuport_dataset['neg']['text'][neg_support_start_idx:neg_support_start_idx+self.n_support//2]\n",
    "        pos_support_text = category_suuport_dataset['pos']['text'][pos_support_start_idx:pos_support_start_idx+self.n_support//2]\n",
    "        neg_support_label = category_suuport_dataset['neg']['label'][neg_support_start_idx:neg_support_start_idx+self.n_support//2]\n",
    "        pos_support_label = category_suuport_dataset['pos']['label'][pos_support_start_idx:pos_support_start_idx+self.n_support//2]\n",
    "        self.neg_support_idx[category] += (self.n_support//2)\n",
    "        self.pos_support_idx[category] += (self.n_support//2)\n",
    "        \n",
    "        # prepare negative/positive query dataset\n",
    "        neg_query_text = neg['text'][neg_query_start_idx:neg_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        pos_query_text = pos['text'][pos_query_start_idx:pos_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        neg_query_label = neg['label'][neg_query_start_idx:neg_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        pos_query_label = pos['label'][pos_query_start_idx:pos_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        self.neg_idx[category] += (self.batch_size//2 - self.n_support//2)\n",
    "        self.pos_idx[category] += (self.batch_size//2 - self.n_support//2)\n",
    "        \n",
    "        # padding support text dataset\n",
    "        if self.n_support:\n",
    "            neg_support_text = pad_sequence([n for n in neg_support_text], batch_first=True)\n",
    "            pos_support_text = pad_sequence([n for n in pos_support_text], batch_first=True)\n",
    "            neg_support_text, pos_support_text = pad_text(neg_support_text, pos_support_text)\n",
    "        else:\n",
    "            neg_support_text = torch.tensor([[]])\n",
    "            pos_support_text = torch.tensor([[]])\n",
    "            \n",
    "        # padding text dataset\n",
    "        neg_query_text = pad_sequence([n for n in neg_query_text], batch_first=True)\n",
    "        pos_query_text = pad_sequence([p for p in pos_query_text], batch_first=True)\n",
    "        neg_query_text, pos_query_text = pad_text(neg_query_text, pos_query_text)\n",
    "\n",
    "        # concatenating support/query text dataset\n",
    "        support_text = torch.cat([neg_support_text, pos_support_text], dim=0)\n",
    "        query_text = torch.cat([neg_query_text, pos_query_text], dim=0)\n",
    "        support_text, query_text = pad_text(support_text, query_text)\n",
    "\n",
    "        # make final data and label\n",
    "        if self.n_support:\n",
    "            data = torch.cat([support_text, query_text], dim=0)\n",
    "        else:\n",
    "            data = query_text\n",
    "        label = torch.cat([neg_support_label, pos_support_label, neg_query_label, pos_query_label], dim=0)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = AmazonDataLoader(train_dataset, batch_size=64, n_support=support*2)\n",
    "dev_dataloader = AmazonDataLoader(dev_dataset, batch_size=64, n_support=support*2)\n",
    "test_dataloader = AmazonDataLoader(test_dataset, batch_size=64, n_support=support*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 149]) tensor(0.5000)\n",
      "torch.Size([64, 460]) tensor(0.5000)\n",
      "torch.Size([64, 254]) tensor(0.5000)\n",
      "torch.Size([64, 262]) tensor(0.5000)\n",
      "torch.Size([64, 1283]) tensor(0.5000)\n",
      "torch.Size([64, 1658]) tensor(0.5000)\n",
      "torch.Size([64, 613]) tensor(0.5000)\n",
      "torch.Size([64, 359]) tensor(0.5000)\n",
      "torch.Size([64, 530]) tensor(0.5000)\n",
      "torch.Size([64, 602]) tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    d, l = train_dataloader.get_batch()\n",
    "    print(d.shape, l.float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 327]) tensor(0.5000)\n",
      "torch.Size([55, 181]) tensor(0.5818)\n",
      "torch.Size([64, 198]) tensor(0.5000)\n",
      "torch.Size([46, 295]) tensor(0.6957)\n",
      "torch.Size([64, 197]) tensor(0.5000)\n",
      "torch.Size([64, 276]) tensor(0.5000)\n",
      "torch.Size([55, 186]) tensor(0.5818)\n",
      "torch.Size([64, 270]) tensor(0.5000)\n",
      "torch.Size([26, 130]) tensor(0.4615)\n",
      "torch.Size([64, 327]) tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    d, l = dev_dataloader.get_batch_test()\n",
    "    print(d.shape, l.float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 743]) tensor(0.5000)\n",
      "torch.Size([64, 841]) tensor(0.5000)\n",
      "torch.Size([64, 1386]) tensor(0.5000)\n",
      "torch.Size([64, 706]) tensor(0.5000)\n",
      "torch.Size([64, 1026]) tensor(0.5000)\n",
      "torch.Size([64, 1126]) tensor(0.5000)\n",
      "torch.Size([64, 1116]) tensor(0.5000)\n",
      "torch.Size([64, 1333]) tensor(0.5000)\n",
      "torch.Size([64, 568]) tensor(0.5000)\n",
      "torch.Size([64, 570]) tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    d, l = test_dataloader.get_batch_test()\n",
    "    print(d.shape, l.float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FewShotInduction(C=2,\n",
    "                         S=support,\n",
    "                         vocab_size=len(tokenizer),\n",
    "                         embed_size=300,\n",
    "                         hidden_size=128,\n",
    "                         d_a=64,\n",
    "                         iterations=3,\n",
    "                         outsize=100)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=float(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = Criterion(way=2, shot=support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episode):\n",
    "    model.train()\n",
    "    data, target = train_dataloader.get_batch()\n",
    "    data = data.cuda()\n",
    "    target = target.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    predict = model(data)\n",
    "    loss, acc = criterion(predict, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev(episode):\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    count = 0.\n",
    "    for i in range(100):\n",
    "        data, target = dev_dataloader.get_batch_test()\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        predict = model(data)\n",
    "        _, acc = criterion(predict, target)\n",
    "        amount = len(target) - support * 2\n",
    "        correct += acc * amount\n",
    "        count += amount\n",
    "    acc = correct / count\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_interval = 100\n",
    "best_acc = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9999 [00:00<?, ?it/s]Could not load symbol cublasGetSmCountTarget from libcublas.so.11. Error: /usr/local/cuda-11.2/lib64/libcublas.so.11: undefined symbol: cublasGetSmCountTarget\n",
      "  1%|          | 102/9999 [00:10<27:36,  5.98it/s, loss=tensor(0.5045, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 202/9999 [00:21<29:25,  5.55it/s, loss=tensor(0.5000, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4801/9999 [08:33<18:32,  4.67it/s, loss=tensor(0.4628, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4902/9999 [08:43<16:58,  5.00it/s, loss=tensor(0.5104, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5401/9999 [09:37<14:37,  5.24it/s, loss=tensor(0.4488, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5802/9999 [10:20<11:47,  5.94it/s, loss=tensor(0.1758, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5902/9999 [10:30<11:28,  5.95it/s, loss=tensor(0.3463, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6002/9999 [10:41<11:10,  5.96it/s, loss=tensor(0.3233, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6101/9999 [10:52<12:09,  5.34it/s, loss=tensor(0.2904, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6400/9999 [11:24<13:59,  4.29it/s, loss=tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 6602/9999 [11:45<11:14,  5.04it/s, loss=tensor(0.3230, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 6900/9999 [12:18<12:16,  4.21it/s, loss=tensor(0.2051, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 7202/9999 [12:50<08:14,  5.65it/s, loss=tensor(0.2648, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 7701/9999 [13:43<07:12,  5.32it/s, loss=tensor(0.1742, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7802/9999 [13:54<06:29,  5.64it/s, loss=tensor(0.1988, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 9302/9999 [16:35<02:06,  5.52it/s, loss=tensor(0.1440, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 9701/9999 [17:18<00:54,  5.45it/s, loss=tensor(0.1654, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [17:49<00:00,  9.35it/s, loss=tensor(0.3224, device='cuda:0', grad_fn=<MeanBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "tbar = tqdm(range(1, 10000))\n",
    "for episode in tbar:\n",
    "    \n",
    "    loss = train(episode)\n",
    "    if episode % dev_interval == 0:\n",
    "        acc = dev(episode)\n",
    "        if acc > best_acc:\n",
    "            print('Better acc! Saving model! -> {:.4f}'.format(acc))\n",
    "            best_acc = acc\n",
    "    tbar.set_postfix(loss=loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'fewshot_model_{support}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 jkfirst jkfirst 62M  8월  2 03:20 fewshot_model_5.bin\r\n",
      "-rwxrwxr-x 1 jkfirst jkfirst 62M  4월 28 05:48 fewshot_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh *.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = 5\n",
    "criterion = Criterion(way=2, shot=support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FewShotInduction(C=2,\n",
    "                         S=support,\n",
    "                         vocab_size=len(tokenizer),\n",
    "                         embed_size=300,\n",
    "                         hidden_size=128,\n",
    "                         d_a=64,\n",
    "                         iterations=3,\n",
    "                         outsize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./fewshot_model.bin', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    count = 0.\n",
    "    for i in range(100):\n",
    "        data, target = test_dataloader.get_batch_test()\n",
    "        data = data\n",
    "        target = target\n",
    "        predict = model(data)\n",
    "        _, acc = criterion(predict, target)\n",
    "        amount = len(target) - support * 2\n",
    "        correct += acc * amount\n",
    "        count += amount\n",
    "        \n",
    "    acc = correct / count\n",
    "    print('Test Acc: {}'.format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.6952757239341736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6953)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_hello_three",
   "language": "python",
   "name": "env_hello_three"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
