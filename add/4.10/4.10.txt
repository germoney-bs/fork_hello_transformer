4.10. 리포머
셀프 어탠션을 기반으로 모델이 우수한 성능을 보여주지만 연산량이나 메모리 사용량 측면에서는 비효율적이다. 2020년에 구글에서 이 문제를 해결하기 위해서 리포머 트랜스포머(Reformer Transformer)라는 모델에 대한 논문을 발표했다. 


4.10.1. 트랜스포머 구조의 문제점
이 논문에서는 기존의 트랜스포머 구조에 대해서 [블록A]왁 같이 3가지 문제점을 지적한다.

<블록 시작>
블록A: 기존 트랜스포머 기반의 모델이 갖는 문제점
문제점1: N개의 레이어를 갖는 모델을 학습할 때 단일 레이어의 모델을 학습할 때 보다 많은 메모리가 필요하다.
문제점2: 피드포워드 네트워크에서 사용하는 히든 레이어의 차원수는 보통 매우 크기 때문에 많은 양의 메모리를 사용한다.
문제점3: 길이가 L인 시퀀스에 대해서 어탠션을 수행할 때 시간/공간 복잡도가 모두 O(L^2)이다.
<블록 끝>

[블록A]에서 이야기한 문제점에 대해서 조금 더 자세하게 알아보자.

우선 첫번째 문제점은 모델을 학습할 때 많은 메모리가 필요하다는 것이다. 그 이유는 N개의 레이어를 갖는 모델을 학습시킬 때 활성화 함수의 값을 N계층에 대해서 모두 저장해야 하기 때문이다. 모델을 학습하는 과정에서의 핵심 연산 과정은 역전파(back-propagation) 연산이다. 활성화 함수가 softmax일 경우를 생각해보자. softmax 함수를 미분한 값은 softmax 함수로 표현할 수 있다. 

<수식 시작>
수식A: softmax 함수의 미분
f(x) = 1 / (1 + e^(-x))
f'(x) = f(x)(1 - f(x))
<수식 끝>

활성화 함수가 softmax인 경우 역전파 과정에서 softmax를 미분한 값이 필요하다. 왜 softmax를 미분한 값이 필요한지는 [수식B]를 참고하면 된다.

<수식 시작>
수식B: 역전파 연산 예시
Loss = { 1} over { 2}(y - hat{y})^2

hat{y} = Out(z) = { 1} over { 1 + e^-z}

z = w_{1}x_{1} +w_{2}x_{2}+ ... + w_{n}x_{n}

{dLoss} over {dw} = {dLoss} over {dOut} {dOut} over {dz} {dz} over {dw} "     ...  ①"

"수식"`"①에서"` {dOut} over {dz} =Out prime (z)"이고"Out prime (z)"는" Out(z)(1 - Out(z))"로 치환 가능함"
<수식 끝>

[수식A]를 보면 softmax 함수를 미분한 값은 softmax 값으로 표현이 가능하다. 따라서 역전파를 하는 과정에서 softmax의 미분값이 필요할 경우 softmax 함수의 연산 값을 이용하게 된다. 이 과정에서 각 레이어마다 활성화 함수의 값을 저장하게 된다. BERT의 경우 트랜스포머 레이어 12개를 가지고 있다. 그렇다면 BERT를 학습하는 과정에서 각 레이어마다 활성화 함수의 출력 값을 메모리에 저장해둬야 BERT를 역전파하면서 학습할 수 있다. 이 과정은 모델을 학습하는 과정에서 메모리 사용 측면에서 비효율적이다.

두번째 문제점은 피드포워드 네트워크에서 사용하는 히든 레이어의 차원수가 너무 크다는 것이다. 이는 연산량과 메모리 사용량 모두 문제가 된다. BERT의 경우 3072부터 4096까지 사용한다. 이 과정에서 많은 양의 메모리가 사용된다는 것이다. BERT의 경우 레이어가 12개로 이루어져 있으므로 3072나 4096 사이즈로 구성된 피드포워드 네트워크가 12개나 있다는 뜻이다.

마지막 세번째 문제점은 어탠션 연산에 대한 시간 및 공간복잡도가 모두 O(L^2)나 된다는 것이다. 여기에서 L은 시퀀스의 길이이다. BERT에서 사용하는 어탠션의 경우 시퀀스를 이루는 모든 토큰들끼리 어탠션을 계산한다. 따라서 L이 4배 길어지면 어탠션 연산에 필요한 시간과 공간의 복잡도는 16배로 늘어난다. 따라서 모델을 학습할 때 시퀀스의 길이에 대한 제약을 많이 받을 수 밖에 없고 이는 모델의 성능에도 영향을 미친다.

리포머에서는 이와 같은 세가지 문제점을 어떻게 개선했는지 다음 절을 통해서 알아보자.


4.10.2. LSH 어탠션
리포머에서는 LSH(Locality Sensitive Hashing) 어탠션을 사용한다. 기존의 트랜스포머 구조의 경우 쿼리와 키 간의 어탠션을 계산할 때 서로의 유사도를 따지지 않고 모두 계산했다. 

<그림 시작>
그림A: 기존 트랜스포머에서의 어탠션 계산
<그림 끝>

[그림A]를 보면 하나의 쿼리에 대해서 모든 키에 대해서 어탠션을 계산하는 것을 볼 수 있다. 리포머에서는 가까운 어탠션끼리만 어탠션을 계산하도록 쿼리와 키간의 유사도를 측정한다. 이 때 유사도를 측정하는 방식이 LSH이다. LSH의 핵심 아이디어는 하나의 벡터 x에 대해서 이와 유사한 벡터 y가 있다면 x의 해시 값과 y의 해시값이 같을 확률이 높다는 것이다. [그림B]를 보자.

<그림 시작>
그림B: LSH의 핵심 아이디어
<그림 끝>

[그림B]를 보면 비슷한 방향을 가진 벡터끼리 하나의 버킷으로 묶여있다. 리포머 논문에서는 비슷한 버킷으로 묶는 해싱 기법으로 앵귤러 LSH를 사용했다. 앵귤러 LSH는 마치 룰렛을 돌리듯 두 벡터의 각도를 유지한 상태에서 임의로 회전시켜서 버킷을 정하는 것이다.

<그림 시작>
그림C: 앵귤러 LSH
<그림 끝>

[그림C]에서 a, b 벡터는 서로 유사한 벡터이고, c, d 벡터는 서로 유사하지 않은 벡터이다. 각 벡터를 [그림C]와 같이 룰렛을 돌리는 것과 같이 랜덤으로 돌렸을 경우 서로 비슷한 벡터인 a와 b 벡터의 경우 D, A, B 영역으로 같은 버킷으로 묶이게 된다. 하지만 c와 d 벡터와 같이 서로 유사도가 적은 벡터를 룰렛으로 돌리면 두 벡터는 서로 다른 버킷으로 묶이게 된다. LSH 어탠션은 서로 같은 버킷으로 묶인 벡터끼리 어탠션을 계산한다.

LSH를 이용한 어탠션을 그림을 통해 이해해보자.

<그림 시작>
그림D: LSH를 이용한 어탠션 계산하기
<그림 끝>

[그림D]를 보면 쿼리와 키를 같은 값으로 두고 있다. 트랜스포머 구조에서의 어탠션은 셀프 어탠션이다. 리포머에서 사용하고 있는 어탠션 역시 셀프 어탠션이기 때문에 쿼리와 키를 같은 값으로 두고 계산했다. q0부터 q4까지의 쿼리 벡터의 방향성을 보자. q0, q2 그리고 q3이 윗쪽 방향을 가리키는 벡터로 서로 유사하다는 것을 알 수 있고, q1, q4가 왼쪽 방향을 가리키는 서로 유사한 벡터임을 알 수 있다. 따라서 (q0, q2, q3)이 한 버킷이고 (q1, q4)가 또 다른 한 버킷이다. 이 과정은 엥귤러 LSH에서의 랜덤으로 룰렛을 돌리는 방식으로 결정된다. 그리고 같은 버킷에 속한 벡터들끼리 어탠션을 계산하게 된다.

여기서 끝이 아니다. 이 어탠션 계산을 청크(chunk) 단위로 묶어서 수행한다.

<그림 시작>
그림E: 청크 단위로 묶어서 어탠션 계산하기
<그림 끝>

[그림E]는 청크 단위로 묶어서 어탠션을 계산하는 과정을 보여준다. [그림E]에서 B1, B2, B3, B4는 벡터가 속하는 버킷 번호를 나타낸다. 청크1은 버킷 번호 1인 벡터들로만 구성돼 있고, 청크2는 버킷 번호1과 2로 구성돼 있다. 이렇게 청크 단위로 묶어서 같은 청크와 바로 전 청크끼리만 어탠션을 계산한다. 이렇게 하면 GPU상에서 연산을 분산으로 할 수 있다.

4.10.3. Reversible 트랜스포머
4.10.1절에서 설명한 기존 트랜스포머의 문제점 중 첫번째 문제는 Reversible 트랜스포머를 통해서 해결했다. 기존의 트랜스포머 모델은 모델의 학습을 진행할 때 N개의 트랜스포머 레이어가 있을 경우 N개의 활성화 함수 결과를 저장해야 하는 문제점을 가지고 있었다. 역전파 연산을 하는 과정에서 활성화 함수의 미분값을 계산할 때 softmax 결과 값이 필요하기 때문이다. [수식A]와 [수식B]를 참고하라.

리포머 논문에서는 이 문제를 해결하기 위해서 활성화 함수 값을 메모리에 저장하지 않고 계산해서 쓸 수 있는 RevNet을 응용해서 Reversible 트랜스포머를 제안했다. 우선, RevNet을 먼저 살펴보자. RevNet은 ResNet을 학습할 때 활성화 함수의 값을 저장해야 한다는 문제점을 해결하기 위해서 제안됐다.

<그림 시작>
그림F: ResNet과 RevNet의 구조
https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0
<그림 끝>

이 책의 레포지토리에서 chapter4/reformer.ipynb를 보면 [코드A]와 같이 간단하게 RevNet의 연산을 검증한 코드가 있다.

<코드 시작>
코드A: RevNet 연산 검증
>>> x1 = torch.rand((3, 3))
>>> x2 = torch.rand((3, 3))
>>> x1, x2
x1, x2
x1, x2
(tensor([[0.7386, 0.9690, 0.3168],
         [0.1200, 0.2633, 0.3560],
         [0.7594, 0.8729, 0.0847]]),
 tensor([[0.2724, 0.2673, 0.1191],
         [0.3291, 0.4657, 0.3513],
         [0.1449, 0.0705, 0.1540]]))
>>> f = nn.Linear(3, 3)
>>> g = nn.Linear(3, 3)
>>> y1 = x1 + f(x2)
>>> y2 = x2 + g(y1)
>>> r2 = y2 - g(y1)
>>> r1 = y1 - f(r2)
>>> r1, r2
(tensor([[0.7386, 0.9690, 0.3168],
         [0.1200, 0.2633, 0.3560],
         [0.7594, 0.8729, 0.0847]], grad_fn=<SubBackward0>),
 tensor([[0.2724, 0.2673, 0.1191],
         [0.3291, 0.4657, 0.3513],
         [0.1449, 0.0705, 0.1540]], grad_fn=<SubBackward0>))
<코드 끝>

[코드A]에서 보면 y1과 y2만을 가지고 r1, r2를 만들었는데, 그 값이 x1, x2와 일치한다. Reversible 트랜스포머에서는 f를 피드포워드 함수로 두고 g를 어탠션 함수로 바꿨다. 이로써 N개의 레이어를 갖는 리포머 구조를 학습할 때 활성화 함수의 값을 저장하지 않고 만들어서 사용할 수 있게 됐고, 활성화 함수를 저장하는데 필요한 메모리의 양을 없앨 수 있게 됐다.

그러나 여전히 메모리 사용량은 많다. 보통 트랜스포머 구조 안에 있는 피드포워드 네트워크의 히든 사이즈 크기가 크게는 4096까지도 커질 수 있기 때문이다. 다행히도 이 연산은 선형 연산이기 때문에 청크 단위로 나눠서 연산해도 완전히 동일한 결과를 얻을 수 있게 된다. 청크 단위로 나눠서 연산을 할 경우, 각 청크에서 피드포워드 연산을 할 때 메모리 사용량을 줄일 수 있다.

리포머 모델을 사용한 컴퓨팅 연산 성능을 간략하게 알아보기 위해서 이 책의 레포지토리에 있는 chapter4/reformer.ipynb에서 시퀀스 길이를 두 배씩 증가시키고 배치 사이즈를 절반씩 줄여가며 실행 속도 성능을 측정했다.

<코드 시작>
코드B: 리포머 모델 실행 속도 측정
for _ in range(6):
    inputs = make_random_inputs(batch_size, sequence_length)
    
    start = time.time()
    o = model(inputs)
    end = time.time()
    
    print(f'{end-start:.2f} seconds for input size of({batch_size},{sequence_length})')
    
    batch_size = batch_size // 2
    sequence_length = sequence_length * 2
<코드 끝>

[코드B]를 수행하면 [블록B]와 같은 결과를 얻는다.

<블록 시작>
블록B: [코드A] 실행 결과
1.04 seconds for input size of(32,64)
0.85 seconds for input size of(16,128)
0.96 seconds for input size of(8,256)
1.71 seconds for input size of(4,512)
1.83 seconds for input size of(2,1024)
1.85 seconds for input size of(1,2048)
<블록 끝>

기존의 O(L^2) 시간 복잡도를 가진 어탠션이라면 배치 사이즈가 1/2로 줄어든다고 하더라도 시퀀스 길이가 늘어나면 시간은 제곱으로 늘어야 하지만 [블록B]를 보면 시퀀스 길이가 증가할 때마다 상대적으로 조금씩 실행 속도가 증가한다는 것을 알 수 있다.

