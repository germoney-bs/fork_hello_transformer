4.7. DistilBERT
지금까지 설명한 BERT기반의 모델은 모두 기존의 CNN 또는 RNN 기반의 언어 모델보다 월등한 성능을 보여줬다. 하지만 모델의 사이즈도 굉장히 많아졌다. 이는 자연스럽게 연산량의 증가로 이어지며 실제 모델을 서비스에 적용할 경우 지연시간도 길어지게 된다. 따라서 가벼운 모델을 만들어보기 위한 경량화에 대한 연구도 활발하게 진행되고 있다. 크게 양자화(Quantization), 가지치기(Pruning), 지식 증류(Knowledge Distillation) 등이 있다. 이 절에서는 BERT를 지식 증류기법을 통해 경량화한 DistilBERT에 대해서 이야기해보자.

4.7.1. 지식 증류
지식 증류(Knowledge Distillation)는 모델 경량화 기법 중 하나이다. 대량의 데이터셋으로 학습된 큰 모델은 많은 수의 파라미터를 가지고 있다. 그렇기 때문에 이런 모델을 실제 서비스에 적용하려면 메모리를 비효율적으로 많이 사용해야하고, 또한 모델의 실행 속도도 느릴수 밖에 없다. 그렇다고 모델의 사이즈를 줄이거나 더 작은 모델을 찾아서 학습시킨다면 모델의 정확도가 감소하게 된다. 지식 증류는 모델의 사이즈를 비약적으로 줄여서 메모리 사용량과 모델의 실행속도를 크게 개선하면서 모델의 정확도는 거의 같거나 조금 낮은 정도로 유지시키는 기법이다. 즉, 대량의 데이터셋으로 학습된 큰 모델이 가지고 있는 지식을 그보다 작은 모델로 이전시키는 기법이다.

지식 증류에서 이미 학습된 지식을 가지고 있는 큰 모델을 티쳐(Teacher) 모델이라고 하고, 이 모델로부터 지식이 이전될 작은 모델을 스튜던트(Student) 모델이라고 부른다. 지식 증류로 스튜던트 모델을 학습하기 위해서는 기존에 학습된 티쳐 모델과 학습에 사용할 데이터셋이 필요하다. 이때 사용되는 데이터셋은 지식을 이전하는데 사용하는 모델이라는 의미로 트랜스퍼(Transfer) 데이터셋이라고도 한다. 티쳐 모델과 트랜스퍼 데이터셋을 이용해서 지식 증류할 때는 두 가지 목적 함수가 사용된다. 하나는 스튜던트 모델과 티쳐 모델 간의 손실(loss)을 구하는 함수(distillation loss function)이고 다른 하나는 스튜던트 모델과 실제 레이블 간의 손실을 구하는 함수(student loss function)이다. 두 손실 함수의 관계는 [그림13]을 통해서 확인할 수 있다.

<그림 시작>
그림13: distillation loss를 목적함수
https://intellabs.github.io/distiller/knowledge_distillation.html
<그림 끝>

[그림13]을 보면 트랜스퍼 데이터셋으로부터 입력 데이터(Input X)와 레이블(Hard label)을 이용한다. 우선 입력 데이터를 스튜던트 모델과 티쳐 모델에 넣어서 손실 함수를 이용해 손실을 계산(Distillation Loss Fn)한다. 그리고 스튜던트 모델의 출력과 실제 레이블로 손실을 계산(Student Loss Fn)한다. 이렇게 구한 손실 두 개를 각각 (alpha * Temperature * Temperature)와 (1 - alpha)를 곱해서 가중치를 적용한다. 여기에서 alpha는 0과 1 사이의 소수이고 Temperature는 소프트 레이블(soft label 또는 soft target)을 위해서 사용되며 양의 정수이다.

[그림13]을 다시 한번 보자. 티쳐 모델의 아웃풋이 소프트 레이블이 되고 스튜던트 모델의 아웃풋은 두 갈레로 나뉘어서 하나는 소프트 프리딕션 다른 하나는 하드 프리딕션으로 나뉜다. 지식 증류에서 스튜던트 모델은 티쳐 모델을 모방하도록 학습하게 된다. 즉 티쳐 모델의 아웃풋과 스튜던트 모델의 아웃풋이 거의 차이가 없어야 된다는 말이다. 이렇게 학습되도록 하기 위해서 티쳐 모델의 아웃풋과 스튜던트 모델의 아웃풋을 사용해야하는데 바로 사용하는 것이 아니라 Temperature 값으로 나눈 후 Softmax를 적용한다.

<수식 시작>
수식2
Soft Label = softmax((티쳐 모델의 출력) x 1/T)
Soft Prediction = softmax((스튜던트 모델의 출력) x 1/T)
<수식 끝>

[수식2]는 소프트 레이블과 소프트 프리딕션을 구하는 방법을 설명한다. 스튜던트 모델과 티쳐 모델의 아웃풋에 softmax를 적용함으로서 값의 분포가 조금 더 부드러워지고 softmax 함수를 적용하기 전에 Temperature로 나눈 후 적용하면 조금 더 부드러운 형태로 값이 분포될 수 있다. 강아지, 고양이, 사자, 호랑이를 구분하는 모델을 예로 들어보자. 이 모델을 학습시켜서 강아지, 고양이, 사자, 호랑이를 잘 구분하는 모델이 학습됐다고 하자. 이 때 이 모델에 고양이 이미지를 입력으로 넣어보자.

<그림 시작>
그림14: 하드레이블과 소프트 레이블
<그림 끝>

[그림14]를 보면 티쳐 모델의 출력에서 고양이가 가장 높게 나왔음을 알 수 있다. 지식 증류에서는 저 고양이 뿐만 아니라 나머지 강아지, 사자, 호랑이에도 집중을 한다. 정답이 아닌 강아지, 사자, 호랑이 중에서도 호랑이랑 가장 비슷하다는 정보를 무시하지 않게 하도록 하기 위해서 소프트 레이블을 만드는 것이다. 소프트 레이블을 만들기 위한 벙법이 [수식2]와 같이 아웃풋 값에 softmax를 적용해주는 것이고 더 소프트하게 만들기 위해서 T로 나눠주는 것이다. 

<코드 시작>
코드15: 소프트 레이블 예시
>>> import numpy as np
>>> def softmax(x):
...     f_x = np.exp(x) / np.sum(np.exp(x))
...     return f_x
...
>>>
>>> def soft_label(x, temperature):
...     return softmax(x/temperature)
...
>>> x = np.array([0.02, 0.90, 0.03, 0.05])
>>> soft_label(x, 1)
array([0.18343897, 0.44225295, 0.18528256, 0.18902552])
>>> soft_label(x, 3)
array([0.22965916, 0.3079476 , 0.23042597, 0.23196727])
>>> soft_label(x, 5)
array([0.23806927, 0.28388286, 0.23854589, 0.23950198])
<코드 끝>

## 코드 넘버링이 잘못 되어서 코드15가 두개입니다. 이전 코드15를 코드14로 바꿔야 합니다.

[코드15]를 보면 0.02, 0.90, 0.03, 0.05로 합이 1인 리스트가 x로 정의돼 있다. 이 리스트를 소프트하게 바꾸는 방법은 이 값에 softmax를 취하는 것이고 더 소프트하게 하려면 temperature에 1보다 더 큰 수를 넣어주면 된다. temperature의 값이 커질수록 리스트 내의 값이 서로 비슷해진다. 

지식 증류를 학습할 때 소프트 레이블과 소프트 프리딕션 간의 손실 뿐만 아니라 하드 레이블과 하드 프리딕션간의 손실도 고려하면 훨씬 학습을 잘 시킬 수 있다. 하드 레이블은 데이터셋이 가지고 있는 원래의 레이블이고 하드 프리딕션은 스튜던트 모델이 출력한 아웃풋이다. 하드 레이블과 하드 프리딕션간의 손실을 지식 증류 학습할 때 고려해주면 학습의 효과가 크게 증가한다. 논문 Distilling the knowledge in a neural network를 보면 트렌스퍼 데이터셋의 레이블이 주어질 경우 스튜던트 모델 학습이 훨씬 잘된다고 한다. [인용5]를 보자.

<인용 시작>
논문 Distilling the knowledge in a neural network 인용
When the correct labels are known for all or some of the transfer set, this method can be significantly improved by also training the distilled model to produce the correct labels. 
인용5
<인용 끝>
## 인용 부분 번호 두 개 밀렸네요.

이제 지식 증류 과정을 정리해보자. 지식 증류를 학습하기 위해서는 잘 학습된 티쳐 모델이 있어야 하고 실제로 학습시킬 스튜던트 모델이 있어야 한다. 스튜던트 모델과 티쳐 모델 간의 아웃풋을 소프트하게 만들어서 손실을 계산하고, 스튜던트 모델의 아웃풋과 실제 레이블간의 손실도 계산한다. 이 두 손실 값에 가중치를 적용해서 손실을 줄여나가는 방향으로 학습하면 티쳐 모델이 가지고 있는 지식이 스튜던트 모델의 지식으로 이전되는 효과가 발생한다.

4.7.2. DistilBERT의 구조와 성능 비교
DistilBERT는 BERT를 지식 증류 기법으로 경량화한 모델이다. 스튜던트 모델의 구조는 기본적으로 BERT와 동일하지만 토큰타입(token-type) 임베딩과 마지막 풀링 레이어를 없애고 Bert의 레이어 개수를 절반으로 줄인 형태이다. BERT의 토큰타입 임베딩은 BERT의 입력을 구성하는 토큰의 타입을 임베딩하는 레이어이다. BERT의 입력은 토큰과 그 토큰이 첫번째 문장에 속하는지 두번째 문장에 속하는지를 구분하는 토큰 타입이 있다. 토큰 타입을 임베딩하는 구조를 원래의 BERT는 가지고 있는데 DistilBERT에서는 그 레이어를 제거한 것이다. 이 장의 코드10을 보면 token_type_ids의 값이 어떻게 구성돼 있는지 알 수 있다. 마지막 풀링 레이어는 BERT의 레이어 이후의 레이어이다. 풀링 레이어는 아웃풋의 shape을 변형시키지 않는다. DistilBERT와 BERT의 파라미터 수를 비교해보자.

<코드 시작>
코드16: DistilBERT와 BERT의 파라미터 수 비교
>>> from thop import profile
>>> import torch
>>> import numpy as np
>>> from transformers import BertTokenizer, DistilBertTokenizer
>>> from transformers import BertModel, DistilBertModel
>>> tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')
>>> tokenizer_distilbert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
>>> bert = BertModel.from_pretrained('bert-base-uncased')
>>> distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')
>>> input = torch.from_numpy(np.random.randint(0, len(tokenizer_bert.vocab), (1, 512)))
>>> macs, params = profile(bert, inputs=(input,))
[INFO] Register count_ln() for <class 'torch.nn.modules.normalization.LayerNorm'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
>>> params
85646592.0
>>> macs
87013588992.0
>>> macs, params = profile(distilbert, inputs=(input,))
[INFO] Register count_ln() for <class 'torch.nn.modules.normalization.LayerNorm'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
>>> params
42528768.0
>>> macs
21753495552.0
<코드 끝>

파라미터 수는 BERT가 약 두 배 많다. 모델의 연산 횟수(MACS)는 BERT가 약 4배 느리다. MACS는 텐서의 연산 횟수를 카운트한 것이다. 보통 a*x + b를 연산 하나로 간주한다. FLOPS는 연산 횟수를 나타낸다. MACS의 연산 단위인 a*x + b는 덧셈 하나와 곱셈 하나로 이루어져 있기 때문에 보통 FLOPS는 MACS에 2를 곱한 값이다.

## 각주1 DistilBERT의 논문에서는 파라미터 수를 약 40% 줄였다고 한다. 코드16은 transformers에서 구현한 DistilBERT와 BERT를 사용해서 결과가 상이할 수 있다.

DistilBERT는 연산 속도와 사이즈를 비약적으로 증가시켰음에도 불구하고 성능은 약 97%정도 유지하고 있다고 한다. DistilBERT의 논문에서 사용된 성능 비교 표를 보자.

<그림 시작>
그림15: DistilBERT, BERT, ELMo의 GLUE 데이터셋 성능 비교
<그림 끝>

[그림15]는 논문 "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"에서 사용된 표이다. CoLA나 RTE의 경우에는 다소 성능이 떨어지는 것 같지만 대부분 BERT의 성능을 따라가는 것을 알 수 있다.

마지막으로 DistilBERT를 지식 증류하는데 드는 학습 소요시간을 보자. DistilBERT 논문에서 DistilBERT, BERT, RoBERTa의 학습 소요시간을 비교한 것을 [표4]에 정리했다. 

<표 시작>
표4: DistilBERT, BERT, RoBERTa 학습 시간 비교
table_4
<표 끝>

4.8. BigBird
이 책의 2장에서 어텐션에 대해서 자세하게 설명했다. 그리고 이 책의 3장에서 셀프 어텐션을 이용하는 트랜스포머의 핵심 구조를 이해해봤다. 그런데 셀프 어텐션에도 단점이 있다. 바로 연산량이다. 시간 복잡도를 따져보면 O(n^2)이 된다. 여기에서 n은 시퀀스의 길이인데 시퀀스의 길이가 512가 넘는 긴 문장의 경우 시간 복잡도가 기하급수적으로 늘어나서 시간과 공간 복잡도 등에 영향을 준다. BERT의 연산량에 큰 영향을 미치는 것이 어텐션이다. 따라서 이 어텐션의 연산량을 줄이기 위한 많은 연구가 있었다. 그 중에서 Sparse 어탠션에 대해서 이야기해보고 빅버드(Big Bird)라고 하는 긴 시퀀스를 위한 트랜스포머에 대해서도 알아보자. 빅버드에서의 어텐션 연산은 크게 글로벌 어텐션, 로컬 어텐션 그리고 랜덤 어텐션으로 구성된다. 

4.8.1. 전체 문장에 대한 어텐션, 글로벌 어텐션
빅버드에서는 첫번째부터 몇 개의 단어는 글로벌 어텐션을 적용한다. 첫번째부터 몇 개의 단어는 모든 단어에 대해서 어텐션을 계산하는 것이다.

<블록 시작>
문장: She is going to run soon
토큰: She, is, going, to, run, soon
어텐션(쿼리 → 키):
- She → is, going, to, run, soon
- is → She, going, to, run, soon
블록16: 글로벌 어텐션 예시
<블록 끝>

[블록16]과 같이 어텐션을 구성할 경우 첫번째 토큰 몇 개에서 나머지 토큰들로 정보가 전이된다. 토큰간의 어텐션을 계산하는 관계를 노드와 엣지의 그래프로 그리면 로컬 어텐션의 경우 [그림16]과 같이 그릴 수 있다.

<그림 시작>
그림16: 글로벌 어텐션에 대한 그래프 예시
<그림 끝>

4.8.2. 가까운 단어에만 집중하기, 로컬 어텐션
셀프 어텐션에서는 토큰 하나가 쿼리로 주어지면 그것에 대한 키가 나머지 다른 모든 토큰이 된다. 즉 하나의 토큰에 대해서 나머지 토큰과 모두 비교하는 연산을 거치게 된다. 그런데 언어에서 "가까움"은 무시할 수 없다. 가까이 있는 단어들에 더 많은 집중을 해야하는 것이 일반적이다. 그렇기 때문에 로컬 어텐션은 하나의 토큰에 대한 어텐션을 계산할 때 그 토큰과 가까이 있는 토큰들만 집중적으로 어탠션을 계산하는 것이다.

<블록 시작>
문장: She is going to run soon
토큰: She, is, going, to, run, soon
어텐션(쿼리 → 키):
- she → is
- is → She, is, going
- going → is, going, to
- to → going, to, run
- run → to, run, soon
- soon → run, soon
블록17: 로컬 어텐션 예시
<블록 끝>

[블록17]에서 사용한 문장 "She is going to run soon"에 대해서 로컬 어텐션을 적용할 경우, 토큰을 세 개씩 묶어서 그 세 개의 토큰에 대해서 어텐션을 계산한다. 물론 꼭 세 개가 아니여도 상관없다. 하나의 토큰에 대해서 앞 뒤로 몇 개의 토큰에 집중해서 어탠션을 계산하는 것이 로컬 어탠션이다. 토큰간의 어텐션을 계산하는 관계를 노드와 엣지의 그래프로 그리면 로컬 어텐션의 경우 [그림17]과 같이 그릴 수 있다.

<그림 시작>
그림17: 로컬 어텐션에 대한 그래프 예시
<그림 끝>

4.8.3. 임의의 토큰에 대한 어텐션, 랜덤 어텐션
랜덤 어텐션은 단어 뜻 그대로 하나의 토큰에 대한 랜덤 토큰 몇 개를 구해서 그 토큰들과 어텐션을 계산하는 것이다.

<블록 시작>
문장: She is going to run soon
토큰: She, is, going, to, run, soon
어텐션(쿼리 → 키):
- She → run
- going → She, soon
- run → is, to
블록18: 랜덤 어텐션 예시
<블록 끝>

[블록18]을 그래프로 나타내면 [그림18]과 같다.

<그림 시작>
그림18: 랜덤 어텐션에 대한 그래프 예시
<그림 끝>

[그림16]부터 [그림18]까지의 그림을 모두 합치면 [그림19]와 같다. 

<그림 시작>
그림19: 빅버드의 어텐션과 셀프 어텐션
<그림 끝>

BERT에서의 어텐션은 하나의 토큰이 모든 토큰과 연결돼 있는 구조이다. 빅버드의 어텐션을 [그림19]에서 보면 모든 토큰들이 연결돼 있지는 않지만 상당히 많이 연결돼 있는 것을 볼 수 있다. 그래프의 엣지를 하나의 토큰에서 다른 토큰으로 정보가 넘어가는 길이라고 생각해보자. 이렇게 생각면서 [그림19]를 보면 셀프 어텐션의 경우 하나의 토큰에서 모든 토큰으로 정보가 전달될 수 있는데 이는 언어의 특성상 불필요하게 많을 수 있다. 빅버드의 어텐션의 경우 토큰과 토큰 사이가 충분히 많이 연결돼 있되, 로컬 어텐션 등을 통해서 중요한 부분에만 집중할 수 있도록 했다.

4.8.4. 토큰 길이에 따른 연산량 비교
빅버드는 셀프 어텐션을 개선해서 시간복잡도를 줄였다. 절대적인 연산 시간을 줄인 것이 아니라 토큰 길이에 따른 연산 시간의 증가량을 개선시킨 것이다. 트랜스포머 버전 4.18에서 지원하고 있는 BigBird 모델을 이용해서 토큰 길이에 따른 연산량을 비교해보자.

<표 시작>
표5: 빅버드와 RoBERTa의 토큰 길이에 따른 지연 시간
table_5
<표 끝>

[표5]를 보면 토큰의 길이가 32에서 두 배씩 늘어나서 512까지 증가할 동안 RoBERTa의 지연 시간은 약 9.5배 늘어난 것이 비해 빅버드의 지연 시간은 5.5배 늘어났다. 

4.9. GLUE 데이터셋
NLP 모델을 평가하기 위한 데이터셋 중에서 가장 많이 사용되는 데이터셋으로 GLUE를 들수 있다. GLUE는 General Language Understanding Evaluation의 약자이다. 언어 능력을 평가할때 글의 핵심을 파악하는 능력, 문맥의 순서를 파악하는 능력 등등 여러가지 평가 기준이 있다. GLUE는 9개의 서브테스크로 언어 모델의 성능을 평가한다. 9개의 서브테스크는 CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI이며, 이 서브테스크들은 단일 문장 또는 문장 쌍들로 구성돼 있다. 이 장에서는 각각의 데이터셋이 어떤 문제들로 구성돼 있는지에 대해서 알아보자.

4.9.1. CoLA 데이터셋
Corpus of Linguistic Acceptability의 약자로, 영어 문장의 문법적인 수용성을 평가하는 데이터셋이다. 0은 문법적으로 수용되지 않는 문장을, 1은 문법적으로 수용되는 문장을 의미한다. CoLA 데이터셋의 레이블은 균형잡혀 있지 않는 unbalanced classification data이다. 평가할 때 사용되는 스코어는 Matthew correlation coefficient를 사용한다.

4.9.2. SST-2 데이터셋
Stanford Sentiment Treebank 데이터셋이다. 영화 리뷰에 대한 사람의 감정을 positive/negative로 나눈 데이터셋이다. SST-2는 accuracy를 통해 평가된다.

4.9.3. MRPC
MRPC(Microsoft Research Paraphrase Corpus)는 마이크로소프트에서 공개한 문장 쌍 데이터셋이고 온라인 뉴스로부터 자동으로 수집된 데이터셋이다. 이 데이터셋은 문장 쌍으로 되어 있고, 두 문장의 의미가 같은지 다른지를 0과 1로 판단하는 데이터셋이다. 이 데이터셋에 대한 평가는 accuracy와 F1 스코어로 측정한다.

4.9.4. QQP
QQP는 Quora로부터 확보한 질문을 쌍으로 연결해둔 데이터셋이다. Quora는 네이버 지식인과 비슷한 질의응답 웹사이트이다. Quora에는 중복된 질문이 많이 올라온다. 질문을 쌍으로 연결해서 중복된 질문인지 그렇지 않은지를 0과 1로 구분한 데이터셋이다. 이 데이터셋에 대한 평가는 accuracy와 F1 스코어로 측정한다.

4.9.5. STS-B
STS-B는 Semantic Textual Similarity Benchmark의 약자로 뉴스, 비디오, 이미지 캡션 등으로부터 추출한 문장 쌍 데이터셋이다. 각 데이터셋은 유사도 스코어를 1부터 5로 구분한 문장 쌍들로 구성돼 있다. 이 데이터셋에 대한 평가는 Pearson and Spearman correlation coefficient로 측정한다.

4.9.6. MNLI
MNLI 데이터셋은 Multi-Genre Natural Language inference의 약자이다. 이 데이터셋은 가설(hypothesis)이 전제(premise)를 수반(entailment)하는지, 모순(contradiction)하는지 또는 중립(neutral)적인지를 나타내는 데이터셋이다. 전제 데이터셋은 번역된 연설문이나 픽션 또는 정부의 리포트로부터 추출했다. 평가할 때는 accuracy로 측정한다. MNLI는 matched와 mismatched 데이터셋으로 나뉜다. matched 데이터셋은 in-domain 데이터셋으로 같은 도메인 상에서 생성된 가설/전제 데이터셋이고, mismatched 데이터셋은 cross-domain 데이터셋으로 서로 다른 도메인의 데이터로부터 가설/전제 데이터셋을 만든 것이다.

4.9.7. QNLI
QNLI는 스탠포드 질문/응답 데이터셋으로부터 생성한 데이터셋이다. 문장과 질문을 쌍으로 두고 질문에 대한 응답이 문장 내에 있는지 없는지를 판단하는 데이터셋이다. 이 데이터셋의 평가는 accuracy를 이용한다.

4.9.8. RTE
RTE는 Recognizaing Textual Entailment의 약자이다. 이 데이터셋은 RTE1, RTE2, RTE3, RTE5를 합친 데이터셋이고 뉴스와 위키피디아 텍스트로 이루어져 있다. 두 문장을 쌍으로 만들어서 그 두 문장이 서로 수반되는 문장인지 그렇지 않은지를 분류한 데이터셋이다. 레이블이 neutral이거나 또는 not_entailment일 경우에는 not_entailment로 통일했다. 데이터셋의 평가는 accuracy를 이용한다.

4.9.9. WNLI
Winograd Schema Challenge라는 독해능력 테스크로부터 생성한 데이터셋이다. Winograd Schema Challenge는 문장에 있는 특정 단어를 가르키는 레퍼런스를 선택지로부터 찾는 문제이다. 예를 들어서 [블록16]을 보자.

<블록 시작>
블록16: Winograd Schema Challenge 예제 데이터
문장: The city councilmen refused the demonstrators a permit because they [feared/advocated] violence.
단어: The city councilmen
선택지: [feared/advocated]
<블록 끝>

they가 the city councilmen을 가르키게 하려면 선택되야 하는 동사는 "feared"이다. WNLI 데이터셋은 선택지 중에서 feared가 선택된다면 entailment로 레이블링하고 advocated로 선택되면 not_entailment로 레이블링해서 만들어졌다. 이 데이터셋의 평가는 accuracy를 이용한다. 이 데이터셋은 레이블 상에 문제가 있어서 BERT에서는 평가시 제외했다.

GLUE 데이터셋은 뉴욕대학교(NYU)의 자연어처리 랩(The Machine Learning for Language Group at NYU CILVR)의 깃허브 레포지토리로부터 다운로드 받을 수 있다.

<코드 시작>
$ git clone https://github.com/nyu-mll/GLUE-baselines.git
$ cd GLUE-baselines
$ python download_glue_data.py --data_dir glue_data --tasks all
...
코드1: GLUE 데이터셋 다운로드
<코드 끝>
