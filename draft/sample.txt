4. 어탠션의 탄생
2장과 3장에 걸쳐서 언어 모델의 발전이 어떻게 발전해왔는지 알아봤다. 언어 모델은 N-Gram을 활용하는 확률적인 기법으로 발전하고 있었다. 그러다 2013년에 단어를 벡터로 표현하는 Word2Vec이 나오면서 언어 모델은 딥러닝의 범주로 들어오게 된다. Word2Vec을 계기로 기계가 마치 정말 사람처럼 문맥을 이해할 수 있다는 것을 알게 됐다. 그 이후 문맥을 조금 더 잘 이해하기 위해 단어의 연속적인 시퀀스를 이해하고자 RNN을 적용했고, 더 긴 시퀀스를 잘 기억할 수 있게 하기 위해 LSTM을 언어 모델에 적용했다.

그렇다면 LSTM 기반의 언어 모델에는 어떤 문제가 있을까? 3장에서 설명했듯이 LSTM의 가장 큰 특징은 입력값 w1을 고정된 길이의 벡터로 출력하고(v1), 그 출력된 벡터를 다시 그 다음 입력(w2)와 같이 넣어서 v2를 출력하는 사이클을 반복한다는 것이다. 이것을 Auto-Regressive 하다고 한다.
[LSTM의 Auto-Regressive한 특징]

4.1. 하나의 벡터로 모든 정보를 담는 LSTM
"6평짜리 방에서 살면 어떨까? 혼자 산다고 가정하면 그래도 어느 정도 괜찮을 것이다. 그런데 친구 5명이서 그 방에서 산다면 어떨까? 잠을 잘 때도 서로 엉겨붙어서 자야 할 것이다."
LSTM도 위와 유사한 단점을 갖는다. 입력 문장의 총 길이에 상관없이 LSTM은 그 입력을 고정된 길이의 벡터로 인코딩한다. 그렇다면 입력문장이 길어지면 길어질수록 많은 정보를 벡터에 넣어야 하기 때문에 인코딩이 힘들어질 수 밖에 없다. 실제로 문장의 길이가 길어질수록 LSTM의 성능이 떨어진다는 연구결과가 있다.
[그림1:https://arxiv.org/pdf/1409.1259.pdf Figure4]

LSTM으로 번역 시스템을(NMT) 만든다고 가정해보자. NMT는 기본적으로 입력 문장을(Sequence) 번역문(Sequence)으로 바꿔주는 Sequence-To-Sequence 구조를 갖게 된다. 입력 문장의 각 단어를 하나하나 LSTM으로 인코딩해서 나온 마지막 결과를 활용하여(h3) 번역문의 각 단어를 하나 하나 만드는 것이다.
[그림2:Seq2Seq 직관적: http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/]
(encoder/decoder)

직관적으로 생각해보자. 위의 Seq2Seq 구조의 모델이 훌륭한 번역기로 학습되려면 h3가 입력문장의 모든 단어를 표현하고 있어야 한다. 짧은 문장에서는 가능할지 모르겠지만 문장이 길수록 힘들어질 것이란 것을 직관적으로 이해할 수 있다.

4.2. 왜 어텐션(Attention)하지 않지?
앞 절에서 LSTM은 입력 데이터를 하나의 벡터로 표현하는 구조를 가지고 있고, 이러한 특성 때문에 입력 데이터가 길어질 경우 입력을 하나의 벡터로 표현하기가 힘들어진다는 것을 설명했다. 그렇다면 어떻게 이 문제를 해결할 수 있을까? 필요한 부분만 집중해서(Attention) 보면 된다. 번역기에서 입력 문장을 번역문으로 바꿀 때 각 입력 단어를 해석할 때 집중해야 되는 부분이 정해져 있다. 아래의 빈 칸 채우기 문제를 봐보자.

ENG: My name is Dooli.
KOR: 내 이름은 둘리__.

빈 칸에 들어갈 정답은 "입니다", "이다" 등이 적절하다. 저 빈 칸을 채우기 위해서 "My name" "Dooli" 등에 집중할 이유가 있을까? 물론 부분적으로 있지만, 가장 집중해야 되는 부분은 "is"이다. LSTM에서 집중을 해야하는 특정 부분에만 집중하도록 weight을 주는 것이 바로 어텐션 메커니즘이다. [그림2]에 어텐션 네트워크를 추가하면 아래와 같은 구조가 된다.
[그림3: Seq2Seq with Attention]

디코더에서 번역 단어를 하나 하나 출력할 때마다 어텐션 가중치를 추가해주는데, 이 어텐션 가중치는 인코더의 output 정보를 통해서 계산된다. [그림2]와 [그림3]을 비교해보자. [그림2]에서 디코더를 연산할 때 인코더의 히든스테이트 값만 들어간다. 하지만 [그림3]에서는 디코더에 인코더의 히든스테이트 값에 인코더의 아웃풋 값이 추가된다. 인코더의 아웃풋은 T개의 값으로 이루어져 있는데, 그 T개의 아웃풋이 가중치로 계산되어 디코더의 입력으로 사용된다. 어텐션 네트워크에 대한 자세한 구조는 다음 절에서 자세하게 설명할 예정이다.

4.3. 어떻게 어텐션(Attention)하지?
이전의 절에서 어텐션을 왜 해야하는지에 대한 직관적인 이해를 해봤다. Seq2Seq 구조는 인코더와 디코더의 구조로 이루어져 있는데, 시퀀스의 길이가 길어질수록 인코더에서 시퀀스를 효과적으로 표현할 수 없기 때문에 무언가 플러스 알파가 필요한 것이다. 그것이 어텐션이며, 그 어텐션은 인코더의 값을 참고해서 가중치를 만든 후 디코더의 값을 만들어가는 것이다.

이번 절에서는 어텐션에 대한 구체적인 이야기를 해보려고 한다. 어텐션을 공부할 때 이해해야 하는 쿼리, 키, 벨류(Query, Key, Value)가 무엇인지 그리고 그것을 어떻게 구현하는지 실제 코드를 통해 알아보자.

4.3.1. 묻고 참고하고 답하기
'''
배가 고픈 오후 판교에서 일하는 먹돌이는 퇴근 후에 뭘 사먹을지 녹생창에 검색을 해보려고 한다.

"판교 대박 맛집"

녹색창은 여러가지 맛집을 추천해주고 먹돌이는 그 중에 한 곳을 가서 맛있게 저녁을 먹었다고 한다.
'''

어제 있었다고 해도 이상할 것이 없는 아주 소소한 일상 이야기이다. 그런데 이것이 쿼리, 키, 벨류와 연관돼 있다. 먹돌이가 녹색창이라는 시스템으로부터 얻고 싶은 것은 판교에 있는 맛있는 식당 정보이다. 그것을 어떻게 물어볼까 생각하다가 몇 가지 키워드를 던진다. "판교" "대박" "맛집". 이것이 쿼리이다. 녹색창의 검색 시스템은 주어진 쿼리를 가지고 여러 가지 정보를 고려하게 된다. 블로그 제목, 블로그 컨텐츠, 맛집 별 개수, 과거 사람들이 클릭한 횟수 등등이다. 이것이 키이다. 키를 이용해서 쿼리를 해석한 후 만들어낸 최종 검색 결과가 밸류이다.


이 문제를 해결하기위해 고안된 것이 어탠션 메커니즘(Attention Mechanism)이다. (**주1) 그렇다면 어떻게
## 학자들이 이런 생각을 하기 시작했따. 왜 LSTM의 모든 셀에서의 히든을 사용하지 않지?
## 굳이 중간에 모든걸 기억할 필요는 없잖아?

4.1.2. 어탠션 이해하기
## query key value의 의미
## 공식
## shape 이해하기