4. 어탠션의 탄생
2장과 3장에 걸쳐서 언어 모델의 발전이 어떻게 발전해왔는지 알아봤다. 언어 모델은 N-Gram을 활용하는 확률적인 기법으로 발전하고 있었다. 그러다 2013년에 단어를 벡터로 표현하는 Word2Vec이 나오면서 언어 모델은 딥러닝의 범주로 들어오게 된다. Word2Vec을 계기로 기계가 마치 정말 사람처럼 문맥을 이해할 수 있다는 것을 알게 됐다. 그 이후 문맥을 조금 더 잘 이해하기 위해 단어의 연속적인 시퀀스를 이해하고자 RNN을 적용했고, 더 긴 시퀀스를 잘 기억할 수 있게 하기 위해 LSTM을 언어 모델에 적용했다.

그렇다면 LSTM 기반의 언어 모델에는 어떤 문제가 있을까? 3장에서 설명했듯이 LSTM의 가장 큰 특징은 입력값 w1을 고정된 길이의 벡터로 출력하고(v1), 그 출력된 벡터를 다시 그 다음 입력(w2)와 같이 넣어서 v2를 출력하는 사이클을 반복한다는 것이다. 이것을 Auto-Regressive 하다고 한다.
[LSTM의 Auto-Regressive한 특징]

4.1. 왜 어탠션!! 하지 않지?
"6평짜리 방에서 살면 어떨까? 혼자 산다고 가정하면 그래도 어느 정도 괜찮을 것이다. 그런데 친구 5명이서 그 방에서 산다면 어떨까? 잠을 잘 때도 서로 엉겨붙어서 자야 할 것이다."
LSTM도 위와 유사한 단점을 갖는다. 입력 문장의 총 길이에 상관없이 LSTM은 그 입력을 고정된 길이의 벡터로 인코딩한다. 그렇다면 입력문장이 길어지면 길어질수록 많은 정보를 벡터에 넣어야 하기 때문에 인코딩이 힘들어질 수 밖에 없다.



## 학자들이 이런 생각을 하기 시작했따. 왜 LSTM의 모든 셀에서의 히든을 사용하지 않지?
## 굳이 중간에 모든걸 기억할 필요는 없잖아?

4.1.2. 어탠션 이해하기
## query key value의 의미
## 공식
## shape 이해하기

