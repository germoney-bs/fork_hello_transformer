4. 어탠션의 탄생
2장과 3장에 걸쳐서 언어 모델의 발전이 어떻게 발전해왔는지 알아봤다. 언어 모델은 N-Gram을 활용하는 확률적인 기법으로 발전하고 있었다. 그러다 2013년에 단어를 벡터로 표현하는 Word2Vec이 나오면서 언어 모델은 딥러닝의 범주로 들어오게 된다. Word2Vec을 계기로 기계가 마치 정말 사람처럼 문맥을 이해할 수 있다는 것을 알게 됐다. 그 이후 문맥을 조금 더 잘 이해하기 위해 단어의 연속적인 시퀀스를 이해하고자 RNN을 적용했고, 더 긴 시퀀스를 잘 기억할 수 있게 하기 위해 LSTM을 언어 모델에 적용했다.

그렇다면 LSTM 기반의 언어 모델에는 어떤 문제가 있을까? 3장에서 설명했듯이 LSTM의 가장 큰 특징은 입력값 w1을 고정된 길이의 벡터로 출력하고(v1), 그 출력된 벡터를 다시 그 다음 입력(w2)와 같이 넣어서 v2를 출력하는 사이클을 반복한다는 것이다. 이것을 Auto-Regressive 하다고 한다.
[LSTM의 Auto-Regressive한 특징]

4.1. 하나의 벡터로 모든 정보를 담는 LSTM
"6평짜리 방에서 살면 어떨까? 혼자 산다고 가정하면 그래도 어느 정도 괜찮을 것이다. 그런데 친구 5명이서 그 방에서 산다면 어떨까? 잠을 잘 때도 서로 엉겨붙어서 자야 할 것이다."
LSTM도 위와 유사한 단점을 갖는다. 입력 문장의 총 길이에 상관없이 LSTM은 그 입력을 고정된 길이의 벡터로 인코딩한다. 그렇다면 입력문장이 길어지면 길어질수록 많은 정보를 벡터에 넣어야 하기 때문에 인코딩이 힘들어질 수 밖에 없다. 실제로 문장의 길이가 길어질수록 LSTM의 성능이 떨어진다는 연구결과가 있다.
[그림1:https://arxiv.org/pdf/1409.1259.pdf Figure4]

LSTM으로 번역 시스템을(NMT) 만든다고 가정해보자. NMT는 기본적으로 입력 문장을(Sequence) 번역문(Sequence)으로 바꿔주는 Sequence-To-Sequence 구조를 갖게 된다. 입력 문장의 각 단어를 하나하나 LSTM으로 인코딩해서 나온 마지막 결과를 활용하여(h3) 번역문의 각 단어를 하나 하나 만드는 것이다.
[그림2:Seq2Seq 직관적: http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/]
(encoder/decoder)

직관적으로 생각해보자. 위의 Seq2Seq 구조의 모델이 훌륭한 번역기로 학습되려면 h3가 입력문장의 모든 단어를 표현하고 있어야 한다. 짧은 문장에서는 가능할지 모르겠지만 문장이 길수록 힘들어질 것이란 것을 직관적으로 이해할 수 있다.

4.2. 왜 어텐션(Attention)하지 않지?
앞 절에서 LSTM은 입력 데이터를 하나의 벡터로 표현하는 구조를 가지고 있고, 이러한 특성 때문에 입력 데이터가 길어질 경우 입력을 하나의 벡터로 표현하기가 힘들어진다는 것을 설명했다. 그렇다면 어떻게 이 문제를 해결할 수 있을까? 필요한 부분만 집중해서(Attention) 보면 된다. 번역기에서 입력 문장을 번역문으로 바꿀 때 각 입력 단어를 해석할 때 집중해야 되는 부분이 정해져 있다. 아래의 빈 칸 채우기 문제를 봐보자.

ENG: My name is Dooli.
KOR: 내 이름은 둘리__.

빈 칸에 들어갈 정답은 "입니다", "이다" 등이 적절하다. 저 빈 칸을 채우기 위해서 "My name" "Dooli" 등에 집중할 이유가 있을까? 물론 부분적으로 있지만, 가장 집중해야 되는 부분은 "is"이다. LSTM에서 집중을 해야하는 특정 부분에만 집중하도록 weight을 주는 것이 바로 어텐션 메커니즘이다. [그림2]에 어텐션 네트워크를 추가하면 아래와 같은 구조가 된다.
[그림3: Seq2Seq with Attention]

디코더에서 번역 단어를 하나 하나 출력할 때마다 어텐션 가중치를 추가해주는데, 이 어텐션 가중치는 인코더의 output 정보를 통해서 계산된다. <여기에서 그림설명> 어텐션 네트워크에 대한 자세한 구조는 다음 절에서 자세하게 설명할 예정이다.

이 문제를 해결하기위해 고안된 것이 어탠션 메커니즘(Attention Mechanism)이다. (**주1) 그렇다면 어떻게 
## 학자들이 이런 생각을 하기 시작했따. 왜 LSTM의 모든 셀에서의 히든을 사용하지 않지?
## 굳이 중간에 모든걸 기억할 필요는 없잖아?

4.1.2. 어탠션 이해하기
## query key value의 의미
## 공식
## shape 이해하기



**주1: https://arxiv.org/pdf/1409.0473.pdf