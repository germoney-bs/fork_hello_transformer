1. 다음 단어는요? 언어 모델
1.1. 언어 모델은 확률 게임?
1.2. N-gram 언어 모델 만들기
1.2.1. 텍스트 전처리 함수
1.2.2. 제로 카운트 해결하기
1.2.3. N-gram 모델 학습하기
1.2.4. N-gram 언어 모델의 한계
1.3. 딥러닝 기반의 언어 모델
1.3.1. 입력 데이터의 벡터화
1.3.2. GRU 입출력 구조 이해해보기
1.3.3. GRU 언어 모델 구현하기
1.3.4. GRU 언어 모델로 문장 생성하기

2. 집중해보자! 어텐션
2.1. 하나의 벡터로 모든 정보를 담는 RNN
2.2. 왜 어텐션(Attention)하지 않지?
2.3. 어떻게 어텐션(Attention)하지?
2.3.1. 묻고 참고하고 답하기
2.3.2. 어텐션 이해하기
2.3.3. 어탠션 구현하기

3. 안녕, 트랜스포머
3.1. 트랜스포머의 구조
3.1.1. 인코더
3.1.2. 디코더
3.1.3. Transformer
3.2. Why Transformer
3.3. 트랜스포머 학습 결과
3.3.1. Perplexity(PPL)
3.3.2. BLEU 스코어

4. 중간부터 학습하자! 사전학습과 파인튜닝
4.1. 사전학습과 Fine-Tuning
4.2. BERT
4.2.1. BERT 모델 구조 이해하기
4.2.2. BERT의 입출력 정리
4.2.3. 사전학습 이해하기
4.2.4. Masked Language Model (MLM)
4.2.5. Next Sentence Prediction (NSP)
4.2.6. 사전학습을 위한 데이터셋 준비와 Self-supervised learning
4.2.7. 사전학습 파해치기
4.2.8. 사전학습 정리하기
4.2.9. Fine-Tuning 이해하기
4.2.10. 텍스트 분류 모델로 파인튜닝하기
4.2.11. 질의응답 모델로 파인튜닝하기
4.3. GPT
4.3.1. GPT의 사전학습
4.3.2. masked self-attention
4.4. RoBERTa
4.4.1. Static/Dynamic 마스킹 전략
4.4.2. NSP 전략
4.4.3. 배치 사이즈와 데이터셋 크기
4.5. ALBERT
4.5.1. Factorized embedding parameterization
4.5.2. Cross-layer parameter sharing
4.5.3. Sentence order prediction
4.5.4. ALBERT 정리
4.6. ELECTRA
4.6.1. 학습구조
4.6.2. RTD

5. 어떻게 배우지? 메타러닝
5.1. 학습을 위한 학습, 메타 러닝
5.2. 메타 러닝을 이용한 Amazon 리뷰 감정 분류 학습하기
5.2.1. 데이터셋/데이터로더 만들기
5.3. GPT2 그리고 GPT3
5.3.1. GPT2를 학습하기 위한 접근 방법
5.3.2. GPT2의 학습 데이터셋과 멀티태스크
5.3.3. GPT2 성능평가 결과
5.3.4. GP2를 통한 문장 생성
5.3.5. GPT2를 이용한 퓨샷 러닝

